Logging to ./logs_A2C_0.0
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 44.8     |
|    mean_reward        | -377     |
| time/                 |          |
|    total_timesteps    | 512      |
| train/                |          |
|    adaptive_beta      | 0.5      |
|    entropy_loss       | -10.3    |
|    explained_variance | 0.00012  |
|    learning_rate      | 0.0007   |
|    n_updates          | 3        |
|    policy_loss        | -749     |
|    value_loss         | 7.9e+03  |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 374       |
|    mean_reward        | -202      |
| time/                 |           |
|    total_timesteps    | 1024      |
| train/                |           |
|    adaptive_beta      | 0.5       |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000149 |
|    learning_rate      | 0.0007    |
|    n_updates          | 7         |
|    policy_loss        | -995      |
|    value_loss         | 1.29e+04  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 399      |
|    mean_reward        | -276     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    adaptive_beta      | 0.5      |
|    entropy_loss       | -10.3    |
|    explained_variance | 4.08e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 11       |
|    policy_loss        | -703     |
|    value_loss         | 7.92e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -581      |
| time/                 |           |
|    total_timesteps    | 2048      |
| train/                |           |
|    adaptive_beta      | 0.5       |
|    entropy_loss       | -10.2     |
|    explained_variance | -0.000136 |
|    learning_rate      | 0.0007    |
|    n_updates          | 15        |
|    policy_loss        | -733      |
|    value_loss         | 6.88e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -477      |
| time/                 |           |
|    total_timesteps    | 2560      |
| train/                |           |
|    adaptive_beta      | 0.5       |
|    entropy_loss       | -10.1     |
|    explained_variance | -3.05e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 19        |
|    policy_loss        | -890      |
|    value_loss         | 1.16e+04  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -538      |
| time/                 |           |
|    total_timesteps    | 3072      |
| train/                |           |
|    adaptive_beta      | 0.5       |
|    entropy_loss       | -10.1     |
|    explained_variance | -6.09e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 23        |
|    policy_loss        | -538      |
|    value_loss         | 4.72e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 59.2     |
|    mean_reward        | -632     |
| time/                 |          |
|    total_timesteps    | 3584     |
| train/                |          |
|    adaptive_beta      | 0.5      |
|    entropy_loss       | -10.2    |
|    explained_variance | -6.1e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 27       |
|    policy_loss        | -602     |
|    value_loss         | 5.88e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -638      |
| time/                 |           |
|    total_timesteps    | 4096      |
| train/                |           |
|    adaptive_beta      | 0.5       |
|    entropy_loss       | -10.1     |
|    explained_variance | -4.73e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 31        |
|    policy_loss        | -613      |
|    value_loss         | 6.02e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -538      |
| time/                 |           |
|    total_timesteps    | 4608      |
| train/                |           |
|    adaptive_beta      | 0.5       |
|    entropy_loss       | -10.1     |
|    explained_variance | -3.09e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 35        |
|    policy_loss        | -823      |
|    value_loss         | 9.28e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 5120      |
| train/                |           |
|    adaptive_beta      | 0.475     |
|    entropy_loss       | -9.97     |
|    explained_variance | -0.000118 |
|    learning_rate      | 0.0007    |
|    n_updates          | 39        |
|    policy_loss        | -440      |
|    value_loss         | 2.91e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 5632      |
| train/                |           |
|    adaptive_beta      | 0.475     |
|    entropy_loss       | -9.86     |
|    explained_variance | -4.51e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 43        |
|    policy_loss        | -708      |
|    value_loss         | 8.73e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 58       |
|    mean_reward        | -184     |
| time/                 |          |
|    total_timesteps    | 6144     |
| train/                |          |
|    adaptive_beta      | 0.475    |
|    entropy_loss       | -9.79    |
|    explained_variance | 1.79e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 47       |
|    policy_loss        | -663     |
|    value_loss         | 6.79e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 58       |
|    mean_reward        | -183     |
| time/                 |          |
|    total_timesteps    | 6656     |
| train/                |          |
|    adaptive_beta      | 0.475    |
|    entropy_loss       | -9.61    |
|    explained_variance | 9e-06    |
|    learning_rate      | 0.0007   |
|    n_updates          | 51       |
|    policy_loss        | -725     |
|    value_loss         | 7.87e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7168      |
| train/                |           |
|    adaptive_beta      | 0.475     |
|    entropy_loss       | -9.66     |
|    explained_variance | -1.85e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 55        |
|    policy_loss        | -451      |
|    value_loss         | 3.97e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7680      |
| train/                |           |
|    adaptive_beta      | 0.475     |
|    entropy_loss       | -9.64     |
|    explained_variance | -5.69e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 59        |
|    policy_loss        | -243      |
|    value_loss         | 1.24e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    adaptive_beta      | 0.475     |
|    entropy_loss       | -9.48     |
|    explained_variance | -6.87e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -268      |
|    value_loss         | 1.83e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    adaptive_beta      | 0.475     |
|    entropy_loss       | -9.54     |
|    explained_variance | -4.73e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | -331      |
|    value_loss         | 2.04e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 9216      |
| train/                |           |
|    adaptive_beta      | 0.475     |
|    entropy_loss       | -9.41     |
|    explained_variance | -6.79e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 71        |
|    policy_loss        | -334      |
|    value_loss         | 2.88e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -184      |
| time/                 |           |
|    total_timesteps    | 9728      |
| train/                |           |
|    adaptive_beta      | 0.475     |
|    entropy_loss       | -9.18     |
|    explained_variance | -4.53e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 75        |
|    policy_loss        | -259      |
|    value_loss         | 1.63e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -184      |
| time/                 |           |
|    total_timesteps    | 10240     |
| train/                |           |
|    adaptive_beta      | 0.451     |
|    entropy_loss       | -8.65     |
|    explained_variance | -7.55e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 79        |
|    policy_loss        | -164      |
|    value_loss         | 878       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -184      |
| time/                 |           |
|    total_timesteps    | 10752     |
| train/                |           |
|    adaptive_beta      | 0.451     |
|    entropy_loss       | -8.1      |
|    explained_variance | -0.000191 |
|    learning_rate      | 0.0007    |
|    n_updates          | 83        |
|    policy_loss        | -16.4     |
|    value_loss         | 141       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 100       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 11264     |
| train/                |           |
|    adaptive_beta      | 0.451     |
|    entropy_loss       | -6.5      |
|    explained_variance | -0.000167 |
|    learning_rate      | 0.0007    |
|    n_updates          | 87        |
|    policy_loss        | 5.33      |
|    value_loss         | 108       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11776    |
| train/                |          |
|    adaptive_beta      | 0.451    |
|    entropy_loss       | -6.24    |
|    explained_variance | 9.71e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 91       |
|    policy_loss        | 15.4     |
|    value_loss         | 115      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12288    |
| train/                |          |
|    adaptive_beta      | 0.451    |
|    entropy_loss       | -7.87    |
|    explained_variance | 0.000275 |
|    learning_rate      | 0.0007   |
|    n_updates          | 95       |
|    policy_loss        | 14.8     |
|    value_loss         | 107      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12800    |
| train/                |          |
|    adaptive_beta      | 0.451    |
|    entropy_loss       | -6.68    |
|    explained_variance | 0.000595 |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | 17.3     |
|    value_loss         | 91.9     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.57     |
|    ep_rew_mean     | -16.7    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 47100    |
|    total_timesteps | 12800    |
| train/             |          |
|    adaptive_beta   | 0.451    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13312    |
| train/                |          |
|    adaptive_beta      | 0.451    |
|    entropy_loss       | -6.45    |
|    explained_variance | 0.00114  |
|    learning_rate      | 0.0007   |
|    n_updates          | 103      |
|    policy_loss        | 18.8     |
|    value_loss         | 87.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13824    |
| train/                |          |
|    adaptive_beta      | 0.451    |
|    entropy_loss       | -6.73    |
|    explained_variance | 0.00137  |
|    learning_rate      | 0.0007   |
|    n_updates          | 107      |
|    policy_loss        | 8.78     |
|    value_loss         | 102      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14336    |
| train/                |          |
|    adaptive_beta      | 0.451    |
|    entropy_loss       | -6.24    |
|    explained_variance | 0.00442  |
|    learning_rate      | 0.0007   |
|    n_updates          | 111      |
|    policy_loss        | 11.6     |
|    value_loss         | 87       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14848    |
| train/                |          |
|    adaptive_beta      | 0.451    |
|    entropy_loss       | -6.3     |
|    explained_variance | 0.0379   |
|    learning_rate      | 0.0007   |
|    n_updates          | 115      |
|    policy_loss        | 23.5     |
|    value_loss         | 57.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15360    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.16    |
|    explained_variance | 0.0649   |
|    learning_rate      | 0.0007   |
|    n_updates          | 119      |
|    policy_loss        | 15.3     |
|    value_loss         | 70.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15872    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.56    |
|    explained_variance | 0.13     |
|    learning_rate      | 0.0007   |
|    n_updates          | 123      |
|    policy_loss        | 10.4     |
|    value_loss         | 79.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16384    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.74    |
|    explained_variance | 0.0721   |
|    learning_rate      | 0.0007   |
|    n_updates          | 127      |
|    policy_loss        | -16.2    |
|    value_loss         | 106      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16896    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.56    |
|    explained_variance | 0.0432   |
|    learning_rate      | 0.0007   |
|    n_updates          | 131      |
|    policy_loss        | -13.6    |
|    value_loss         | 44.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17408    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.48    |
|    explained_variance | 0.353    |
|    learning_rate      | 0.0007   |
|    n_updates          | 135      |
|    policy_loss        | 10.9     |
|    value_loss         | 30.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17920    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -7.03    |
|    explained_variance | 0.221    |
|    learning_rate      | 0.0007   |
|    n_updates          | 139      |
|    policy_loss        | -4.89    |
|    value_loss         | 28.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18432    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.7     |
|    explained_variance | 0.425    |
|    learning_rate      | 0.0007   |
|    n_updates          | 143      |
|    policy_loss        | 11       |
|    value_loss         | 21.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.59    |
|    explained_variance | 0.345    |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | -5.48    |
|    value_loss         | 48.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.45    |
|    explained_variance | 0.327    |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | 18.2     |
|    value_loss         | 38.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19968    |
| train/                |          |
|    adaptive_beta      | 0.429    |
|    entropy_loss       | -6.7     |
|    explained_variance | 0.212    |
|    learning_rate      | 0.0007   |
|    n_updates          | 155      |
|    policy_loss        | -9.28    |
|    value_loss         | 46.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 20480    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -6.42    |
|    explained_variance | 0.334    |
|    learning_rate      | 0.0007   |
|    n_updates          | 159      |
|    policy_loss        | -15.1    |
|    value_loss         | 39       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20992    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -6.64    |
|    explained_variance | 0.224    |
|    learning_rate      | 0.0007   |
|    n_updates          | 163      |
|    policy_loss        | -13.4    |
|    value_loss         | 58.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 106      |
|    mean_reward        | -54.4    |
| time/                 |          |
|    total_timesteps    | 21504    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -6.44    |
|    explained_variance | 0.568    |
|    learning_rate      | 0.0007   |
|    n_updates          | 167      |
|    policy_loss        | 3.57     |
|    value_loss         | 11.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 68.6     |
|    mean_reward        | -28.4    |
| time/                 |          |
|    total_timesteps    | 22016    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -7.08    |
|    explained_variance | 0.288    |
|    learning_rate      | 0.0007   |
|    n_updates          | 171      |
|    policy_loss        | -8.88    |
|    value_loss         | 54.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 49.6     |
|    mean_reward        | -18.9    |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -7.18    |
|    explained_variance | 0.296    |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | 10.6     |
|    value_loss         | 30.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23040    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -6.72    |
|    explained_variance | 0.197    |
|    learning_rate      | 0.0007   |
|    n_updates          | 179      |
|    policy_loss        | -0.0995  |
|    value_loss         | 46.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 99.8     |
|    mean_reward        | -29.2    |
| time/                 |          |
|    total_timesteps    | 23552    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -6.94    |
|    explained_variance | 0.549    |
|    learning_rate      | 0.0007   |
|    n_updates          | 183      |
|    policy_loss        | 16.2     |
|    value_loss         | 14       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 54.6     |
|    mean_reward        | -13      |
| time/                 |          |
|    total_timesteps    | 24064    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -7.56    |
|    explained_variance | 0.134    |
|    learning_rate      | 0.0007   |
|    n_updates          | 187      |
|    policy_loss        | -5.27    |
|    value_loss         | 50.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56       |
|    mean_reward        | -15.4    |
| time/                 |          |
|    total_timesteps    | 24576    |
| train/                |          |
|    adaptive_beta      | 0.407    |
|    entropy_loss       | -6.81    |
|    explained_variance | 0.345    |
|    learning_rate      | 0.0007   |
|    n_updates          | 191      |
|    policy_loss        | -5.47    |
|    value_loss         | 34.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 94.2     |
|    mean_reward        | -34.6    |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -6.98    |
|    explained_variance | 0.354    |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | -2.86    |
|    value_loss         | 25.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 53.8     |
|    mean_reward        | -13.2    |
| time/                 |          |
|    total_timesteps    | 25600    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -7.09    |
|    explained_variance | 0.342    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | -2.99    |
|    value_loss         | 22.7     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.53     |
|    ep_rew_mean     | -11.1    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 88140    |
|    total_timesteps | 25600    |
| train/             |          |
|    adaptive_beta   | 0.387    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 50.8     |
|    mean_reward        | -17.8    |
| time/                 |          |
|    total_timesteps    | 26112    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -6.73    |
|    explained_variance | 0.225    |
|    learning_rate      | 0.0007   |
|    n_updates          | 203      |
|    policy_loss        | -7.52    |
|    value_loss         | 35.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.2      |
|    mean_reward        | -5.72    |
| time/                 |          |
|    total_timesteps    | 26624    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -6.97    |
|    explained_variance | 0.279    |
|    learning_rate      | 0.0007   |
|    n_updates          | 207      |
|    policy_loss        | -3.42    |
|    value_loss         | 33       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.2     |
|    mean_reward        | -12.5    |
| time/                 |          |
|    total_timesteps    | 27136    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -7.8     |
|    explained_variance | 0.129    |
|    learning_rate      | 0.0007   |
|    n_updates          | 211      |
|    policy_loss        | -36.7    |
|    value_loss         | 91.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 55.2     |
|    mean_reward        | -13.3    |
| time/                 |          |
|    total_timesteps    | 27648    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -6.98    |
|    explained_variance | 0.323    |
|    learning_rate      | 0.0007   |
|    n_updates          | 215      |
|    policy_loss        | -7.17    |
|    value_loss         | 32.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.6      |
|    mean_reward        | -4.31    |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -6.7     |
|    explained_variance | 0.301    |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | -0.307   |
|    value_loss         | 34.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 29.8     |
|    mean_reward        | -7.12    |
| time/                 |          |
|    total_timesteps    | 28672    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -7.25    |
|    explained_variance | 0.262    |
|    learning_rate      | 0.0007   |
|    n_updates          | 223      |
|    policy_loss        | -28      |
|    value_loss         | 72.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 51.8     |
|    mean_reward        | -15.5    |
| time/                 |          |
|    total_timesteps    | 29184    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -7.59    |
|    explained_variance | 0.0945   |
|    learning_rate      | 0.0007   |
|    n_updates          | 227      |
|    policy_loss        | -33.4    |
|    value_loss         | 163      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.6     |
|    mean_reward        | -11.8    |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    adaptive_beta      | 0.387    |
|    entropy_loss       | -6.86    |
|    explained_variance | 0.579    |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | 7.64     |
|    value_loss         | 9.95     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 22.8     |
|    mean_reward        | -11.3    |
| time/                 |          |
|    total_timesteps    | 30208    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -6.8     |
|    explained_variance | 0.228    |
|    learning_rate      | 0.0007   |
|    n_updates          | 235      |
|    policy_loss        | -24.1    |
|    value_loss         | 69.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 23       |
|    mean_reward        | -11      |
| time/                 |          |
|    total_timesteps    | 30720    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -6.2     |
|    explained_variance | 0.39     |
|    learning_rate      | 0.0007   |
|    n_updates          | 239      |
|    policy_loss        | 10.8     |
|    value_loss         | 16       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 55.2     |
|    mean_reward        | -9.8     |
| time/                 |          |
|    total_timesteps    | 31232    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -6.8     |
|    explained_variance | 0.327    |
|    learning_rate      | 0.0007   |
|    n_updates          | 243      |
|    policy_loss        | -0.0551  |
|    value_loss         | 28.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 29       |
|    mean_reward        | -7.2     |
| time/                 |          |
|    total_timesteps    | 31744    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -7.14    |
|    explained_variance | 0.323    |
|    learning_rate      | 0.0007   |
|    n_updates          | 247      |
|    policy_loss        | -21.3    |
|    value_loss         | 61.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 54       |
|    mean_reward        | -12      |
| time/                 |          |
|    total_timesteps    | 32256    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -6.49    |
|    explained_variance | 0.565    |
|    learning_rate      | 0.0007   |
|    n_updates          | 251      |
|    policy_loss        | -0.364   |
|    value_loss         | 5.93     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3        |
|    mean_reward        | -6.12    |
| time/                 |          |
|    total_timesteps    | 32768    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -6.58    |
|    explained_variance | 0.584    |
|    learning_rate      | 0.0007   |
|    n_updates          | 255      |
|    policy_loss        | -3.04    |
|    value_loss         | 8.03     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 55.2     |
|    mean_reward        | -8.62    |
| time/                 |          |
|    total_timesteps    | 33280    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -7.13    |
|    explained_variance | 0.291    |
|    learning_rate      | 0.0007   |
|    n_updates          | 259      |
|    policy_loss        | 8.1      |
|    value_loss         | 29.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 29.2     |
|    mean_reward        | -6.81    |
| time/                 |          |
|    total_timesteps    | 33792    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -6.93    |
|    explained_variance | 0.0671   |
|    learning_rate      | 0.0007   |
|    n_updates          | 263      |
|    policy_loss        | 24.9     |
|    value_loss         | 27.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3        |
|    mean_reward        | -5.61    |
| time/                 |          |
|    total_timesteps    | 34304    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -6.54    |
|    explained_variance | 0.499    |
|    learning_rate      | 0.0007   |
|    n_updates          | 267      |
|    policy_loss        | -6.87    |
|    value_loss         | 10.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.4      |
|    mean_reward        | -5.77    |
| time/                 |          |
|    total_timesteps    | 34816    |
| train/                |          |
|    adaptive_beta      | 0.368    |
|    entropy_loss       | -6.53    |
|    explained_variance | 0.474    |
|    learning_rate      | 0.0007   |
|    n_updates          | 271      |
|    policy_loss        | 6.46     |
|    value_loss         | 14       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 67.8     |
|    mean_reward        | -17.3    |
| time/                 |          |
|    total_timesteps    | 35328    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -6.58    |
|    explained_variance | 0.516    |
|    learning_rate      | 0.0007   |
|    n_updates          | 275      |
|    policy_loss        | 0.874    |
|    value_loss         | 10.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.6      |
|    mean_reward        | -5.87    |
| time/                 |          |
|    total_timesteps    | 35840    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -6.89    |
|    explained_variance | 0.478    |
|    learning_rate      | 0.0007   |
|    n_updates          | 279      |
|    policy_loss        | -22      |
|    value_loss         | 32.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.2      |
|    mean_reward        | -6.29    |
| time/                 |          |
|    total_timesteps    | 36352    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -6.56    |
|    explained_variance | 0.651    |
|    learning_rate      | 0.0007   |
|    n_updates          | 283      |
|    policy_loss        | -4.89    |
|    value_loss         | 5.29     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 22.6     |
|    mean_reward        | -11      |
| time/                 |          |
|    total_timesteps    | 36864    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -7.69    |
|    explained_variance | 0.163    |
|    learning_rate      | 0.0007   |
|    n_updates          | 287      |
|    policy_loss        | 12.8     |
|    value_loss         | 27.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.4      |
|    mean_reward        | -6.31    |
| time/                 |          |
|    total_timesteps    | 37376    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -6.96    |
|    explained_variance | 0.46     |
|    learning_rate      | 0.0007   |
|    n_updates          | 291      |
|    policy_loss        | -6.28    |
|    value_loss         | 11.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 28.6     |
|    mean_reward        | -8.01    |
| time/                 |          |
|    total_timesteps    | 37888    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -7.26    |
|    explained_variance | 0.337    |
|    learning_rate      | 0.0007   |
|    n_updates          | 295      |
|    policy_loss        | -30.7    |
|    value_loss         | 44.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.4      |
|    mean_reward        | -4.72    |
| time/                 |          |
|    total_timesteps    | 38400    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -6.66    |
|    explained_variance | 0.478    |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | 8.84     |
|    value_loss         | 11.3     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.71     |
|    ep_rew_mean     | -9.73    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 300      |
|    time_elapsed    | 115547   |
|    total_timesteps | 38400    |
| train/             |          |
|    adaptive_beta   | 0.349    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.2      |
|    mean_reward        | -4.36    |
| time/                 |          |
|    total_timesteps    | 38912    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -7.01    |
|    explained_variance | 0.42     |
|    learning_rate      | 0.0007   |
|    n_updates          | 303      |
|    policy_loss        | -8.13    |
|    value_loss         | 33.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.4      |
|    mean_reward        | -6.03    |
| time/                 |          |
|    total_timesteps    | 39424    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -6.83    |
|    explained_variance | 0.307    |
|    learning_rate      | 0.0007   |
|    n_updates          | 307      |
|    policy_loss        | -8.84    |
|    value_loss         | 18       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3        |
|    mean_reward        | -4.81    |
| time/                 |          |
|    total_timesteps    | 39936    |
| train/                |          |
|    adaptive_beta      | 0.349    |
|    entropy_loss       | -6.7     |
|    explained_variance | 0.379    |
|    learning_rate      | 0.0007   |
|    n_updates          | 311      |
|    policy_loss        | -11.8    |
|    value_loss         | 22.2     |
------------------------------------
