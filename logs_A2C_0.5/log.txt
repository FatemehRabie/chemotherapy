Logging to ./logs_A2C_0.5/
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -584      |
| time/                 |           |
|    total_timesteps    | 512       |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -9.54e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 3         |
|    policy_loss        | -666      |
|    value_loss         | 6.19e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -415     |
| time/                 |          |
|    total_timesteps    | 1024     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 5.36e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 7        |
|    policy_loss        | -873     |
|    value_loss         | 1.09e+04 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 347      |
|    mean_reward        | -319     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 11       |
|    policy_loss        | -709     |
|    value_loss         | 7.61e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -243     |
| time/                 |          |
|    total_timesteps    | 2048     |
| train/                |          |
|    entropy_loss       | -9.82    |
|    explained_variance | 3.58e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 15       |
|    policy_loss        | -620     |
|    value_loss         | 5.38e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -478     |
| time/                 |          |
|    total_timesteps    | 2560     |
| train/                |          |
|    entropy_loss       | -9.85    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 19       |
|    policy_loss        | -640     |
|    value_loss         | 6.99e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -242     |
| time/                 |          |
|    total_timesteps    | 3072     |
| train/                |          |
|    entropy_loss       | -9.75    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 23       |
|    policy_loss        | -754     |
|    value_loss         | 8.27e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -242     |
| time/                 |          |
|    total_timesteps    | 3584     |
| train/                |          |
|    entropy_loss       | -9.76    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 27       |
|    policy_loss        | -598     |
|    value_loss         | 5.35e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -240     |
| time/                 |          |
|    total_timesteps    | 4096     |
| train/                |          |
|    entropy_loss       | -9.63    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 31       |
|    policy_loss        | -392     |
|    value_loss         | 2.83e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -385      |
| time/                 |           |
|    total_timesteps    | 4608      |
| train/                |           |
|    entropy_loss       | -9.56     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 35        |
|    policy_loss        | -448      |
|    value_loss         | 3.9e+03   |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 5120     |
| train/                |          |
|    entropy_loss       | -9.46    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 39       |
|    policy_loss        | -316     |
|    value_loss         | 2.05e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 5632      |
| train/                |           |
|    entropy_loss       | -9.57     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 43        |
|    policy_loss        | -218      |
|    value_loss         | 1.08e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 6144     |
| train/                |          |
|    entropy_loss       | -9.54    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 47       |
|    policy_loss        | -348     |
|    value_loss         | 2.07e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 6656     |
| train/                |          |
|    entropy_loss       | -9.32    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 51       |
|    policy_loss        | -518     |
|    value_loss         | 4.36e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 7168     |
| train/                |          |
|    entropy_loss       | -9.28    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 55       |
|    policy_loss        | -275     |
|    value_loss         | 2.03e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7680      |
| train/                |           |
|    entropy_loss       | -9.05     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 59        |
|    policy_loss        | -154      |
|    value_loss         | 683       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    entropy_loss       | -8.76     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -115      |
|    value_loss         | 606       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    entropy_loss       | -8.59     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | -103      |
|    value_loss         | 463       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 9216      |
| train/                |           |
|    entropy_loss       | -8.72     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 71        |
|    policy_loss        | -121      |
|    value_loss         | 508       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 9728      |
| train/                |           |
|    entropy_loss       | -8.56     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 75        |
|    policy_loss        | -81.1     |
|    value_loss         | 466       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 10240     |
| train/                |           |
|    entropy_loss       | -8.25     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 79        |
|    policy_loss        | -132      |
|    value_loss         | 829       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 10752     |
| train/                |           |
|    entropy_loss       | -8        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 83        |
|    policy_loss        | -101      |
|    value_loss         | 545       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 11264     |
| train/                |           |
|    entropy_loss       | -7.43     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 87        |
|    policy_loss        | -14.1     |
|    value_loss         | 242       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 11776     |
| train/                |           |
|    entropy_loss       | -7.16     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 91        |
|    policy_loss        | -60.5     |
|    value_loss         | 338       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 12288     |
| train/                |           |
|    entropy_loss       | -6.58     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 95        |
|    policy_loss        | 10.6      |
|    value_loss         | 110       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 12800     |
| train/                |           |
|    entropy_loss       | -6.28     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 99        |
|    policy_loss        | 6.42      |
|    value_loss         | 109       |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.64     |
|    ep_rew_mean     | -17      |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 42158    |
|    total_timesteps | 12800    |
---------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 13312     |
| train/                |           |
|    entropy_loss       | -5.76     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 103       |
|    policy_loss        | 9.79      |
|    value_loss         | 81.9      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 13824     |
| train/                |           |
|    entropy_loss       | -5.61     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 107       |
|    policy_loss        | 1.54      |
|    value_loss         | 114       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 14336     |
| train/                |           |
|    entropy_loss       | -5.77     |
|    explained_variance | -4.77e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 111       |
|    policy_loss        | 26.4      |
|    value_loss         | 86.9      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 14848     |
| train/                |           |
|    entropy_loss       | -5.82     |
|    explained_variance | -4.77e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 115       |
|    policy_loss        | 15.5      |
|    value_loss         | 95.3      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 15360     |
| train/                |           |
|    entropy_loss       | -5.83     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 119       |
|    policy_loss        | 10.9      |
|    value_loss         | 121       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15872    |
| train/                |          |
|    entropy_loss       | -5.73    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 123      |
|    policy_loss        | -2.3     |
|    value_loss         | 152      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 16384     |
| train/                |           |
|    entropy_loss       | -5.55     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 127       |
|    policy_loss        | 14.1      |
|    value_loss         | 94.2      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 100       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 16896     |
| train/                |           |
|    entropy_loss       | -5.65     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 131       |
|    policy_loss        | 17.8      |
|    value_loss         | 62.2      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17408    |
| train/                |          |
|    entropy_loss       | -5.87    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 135      |
|    policy_loss        | -19.4    |
|    value_loss         | 141      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 17920     |
| train/                |           |
|    entropy_loss       | -5.87     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 139       |
|    policy_loss        | 19.8      |
|    value_loss         | 81.3      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 18432     |
| train/                |           |
|    entropy_loss       | -5.53     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 143       |
|    policy_loss        | 19.6      |
|    value_loss         | 77.2      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    entropy_loss       | -5.61    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | -11.4    |
|    value_loss         | 160      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    entropy_loss       | -5.91    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | 13.6     |
|    value_loss         | 74.2     |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 19968     |
| train/                |           |
|    entropy_loss       | -6.73     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 155       |
|    policy_loss        | -19.8     |
|    value_loss         | 160       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20480    |
| train/                |          |
|    entropy_loss       | -6.85    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 159      |
|    policy_loss        | -29.5    |
|    value_loss         | 187      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20992    |
| train/                |          |
|    entropy_loss       | -7.11    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 163      |
|    policy_loss        | -51.3    |
|    value_loss         | 236      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 21504     |
| train/                |           |
|    entropy_loss       | -6.88     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 167       |
|    policy_loss        | 12        |
|    value_loss         | 81        |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 22016    |
| train/                |          |
|    entropy_loss       | -6.6     |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 171      |
|    policy_loss        | 19.2     |
|    value_loss         | 84.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    entropy_loss       | -6.71    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | -1.63    |
|    value_loss         | 105      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23040    |
| train/                |          |
|    entropy_loss       | -6.46    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 179      |
|    policy_loss        | -21.4    |
|    value_loss         | 193      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23552    |
| train/                |          |
|    entropy_loss       | -6.32    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 183      |
|    policy_loss        | 12.2     |
|    value_loss         | 55.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 24064    |
| train/                |          |
|    entropy_loss       | -6.33    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 187      |
|    policy_loss        | 20       |
|    value_loss         | 89.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 24576    |
| train/                |          |
|    entropy_loss       | -6.91    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 191      |
|    policy_loss        | -37.8    |
|    value_loss         | 276      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    entropy_loss       | -6.44    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | 1.3      |
|    value_loss         | 90.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -6.48    |
|    explained_variance | 2.38e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | 13.2     |
|    value_loss         | 114      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.1     |
|    ep_rew_mean     | -19.7    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 82108    |
|    total_timesteps | 25600    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 26112    |
| train/                |          |
|    entropy_loss       | -6.44    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 203      |
|    policy_loss        | -8.66    |
|    value_loss         | 119      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 26624    |
| train/                |          |
|    entropy_loss       | -6.24    |
|    explained_variance | 3.58e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 207      |
|    policy_loss        | -6.47    |
|    value_loss         | 122      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 27136    |
| train/                |          |
|    entropy_loss       | -6.44    |
|    explained_variance | 2.98e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 211      |
|    policy_loss        | -1.94    |
|    value_loss         | 105      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 27648    |
| train/                |          |
|    entropy_loss       | -7.12    |
|    explained_variance | 4.17e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 215      |
|    policy_loss        | -40.1    |
|    value_loss         | 271      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    entropy_loss       | -7.02    |
|    explained_variance | 1.97e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | -2.67    |
|    value_loss         | 86.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 28672    |
| train/                |          |
|    entropy_loss       | -7.28    |
|    explained_variance | 5.36e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 223      |
|    policy_loss        | -9.89    |
|    value_loss         | 118      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 29184    |
| train/                |          |
|    entropy_loss       | -7.84    |
|    explained_variance | 3.58e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 227      |
|    policy_loss        | -30.8    |
|    value_loss         | 186      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    entropy_loss       | -7.95    |
|    explained_variance | 2.38e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | -82.2    |
|    value_loss         | 359      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 30208    |
| train/                |          |
|    entropy_loss       | -7.62    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 235      |
|    policy_loss        | -69.6    |
|    value_loss         | 346      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 30720     |
| train/                |           |
|    entropy_loss       | -7.66     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 239       |
|    policy_loss        | -29.7     |
|    value_loss         | 146       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 31232     |
| train/                |           |
|    entropy_loss       | -7.06     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 243       |
|    policy_loss        | -14.2     |
|    value_loss         | 86.2      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 31744     |
| train/                |           |
|    entropy_loss       | -6.95     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 247       |
|    policy_loss        | -46.4     |
|    value_loss         | 340       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 32256     |
| train/                |           |
|    entropy_loss       | -6.99     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 251       |
|    policy_loss        | 4.99      |
|    value_loss         | 127       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 32768     |
| train/                |           |
|    entropy_loss       | -6.48     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 255       |
|    policy_loss        | -17.9     |
|    value_loss         | 229       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 33280    |
| train/                |          |
|    entropy_loss       | -6.79    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 259      |
|    policy_loss        | 11       |
|    value_loss         | 94.1     |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 33792     |
| train/                |           |
|    entropy_loss       | -6.58     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 263       |
|    policy_loss        | -1.13     |
|    value_loss         | 109       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 34304    |
| train/                |          |
|    entropy_loss       | -7.08    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 267      |
|    policy_loss        | -12.2    |
|    value_loss         | 155      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 34816    |
| train/                |          |
|    entropy_loss       | -7.1     |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 271      |
|    policy_loss        | 1.46     |
|    value_loss         | 178      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -180      |
| time/                 |           |
|    total_timesteps    | 35328     |
| train/                |           |
|    entropy_loss       | -7.27     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 275       |
|    policy_loss        | -13.8     |
|    value_loss         | 179       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 35840    |
| train/                |          |
|    entropy_loss       | -6.78    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 279      |
|    policy_loss        | -2.48    |
|    value_loss         | 114      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 36352     |
| train/                |           |
|    entropy_loss       | -6.65     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 283       |
|    policy_loss        | 13.9      |
|    value_loss         | 74.4      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 36864    |
| train/                |          |
|    entropy_loss       | -6.6     |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 287      |
|    policy_loss        | 2.65     |
|    value_loss         | 98.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 37376    |
| train/                |          |
|    entropy_loss       | -6.73    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 291      |
|    policy_loss        | -1.51    |
|    value_loss         | 103      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 100       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 37888     |
| train/                |           |
|    entropy_loss       | -6.85     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 295       |
|    policy_loss        | 4.3       |
|    value_loss         | 99.5      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -6.18    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | -32      |
|    value_loss         | 253      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.76     |
|    ep_rew_mean     | -20.6    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 300      |
|    time_elapsed    | 120820   |
|    total_timesteps | 38400    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 38912    |
| train/                |          |
|    entropy_loss       | -5.81    |
|    explained_variance | 2.38e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 303      |
|    policy_loss        | 22.1     |
|    value_loss         | 89.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 39424    |
| train/                |          |
|    entropy_loss       | -5.72    |
|    explained_variance | 1.13e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 307      |
|    policy_loss        | 21.5     |
|    value_loss         | 67.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 39936    |
| train/                |          |
|    entropy_loss       | -5.77    |
|    explained_variance | 1.37e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 311      |
|    policy_loss        | 9.56     |
|    value_loss         | 90.6     |
------------------------------------
