Logging to ./logs_A2C_0.5/
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 89.8     |
|    mean_reward        | -374     |
| time/                 |          |
|    total_timesteps    | 512      |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | -0.00144 |
|    learning_rate      | 0.0007   |
|    n_updates          | 3        |
|    policy_loss        | -645     |
|    value_loss         | 6.04e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 360       |
|    mean_reward        | -365      |
| time/                 |           |
|    total_timesteps    | 1024      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000434 |
|    learning_rate      | 0.0007    |
|    n_updates          | 7         |
|    policy_loss        | -925      |
|    value_loss         | 1.16e+04  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 122      |
|    mean_reward        | -263     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 8.58e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 11       |
|    policy_loss        | -837     |
|    value_loss         | 9.22e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 308       |
|    mean_reward        | -205      |
| time/                 |           |
|    total_timesteps    | 2048      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000419 |
|    learning_rate      | 0.0007    |
|    n_updates          | 15        |
|    policy_loss        | -786      |
|    value_loss         | 8e+03     |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58.8      |
|    mean_reward        | -269      |
| time/                 |           |
|    total_timesteps    | 2560      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -8.11e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 19        |
|    policy_loss        | -813      |
|    value_loss         | 9.47e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -195     |
| time/                 |          |
|    total_timesteps    | 3072     |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | -1.9e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 23       |
|    policy_loss        | -827     |
|    value_loss         | 9.41e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 59.6      |
|    mean_reward        | -195      |
| time/                 |           |
|    total_timesteps    | 3584      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -2.54e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 27        |
|    policy_loss        | -775      |
|    value_loss         | 8.2e+03   |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 65.4     |
|    mean_reward        | -208     |
| time/                 |          |
|    total_timesteps    | 4096     |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 1.67e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 31       |
|    policy_loss        | -893     |
|    value_loss         | 1.02e+04 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 62.6      |
|    mean_reward        | -200      |
| time/                 |           |
|    total_timesteps    | 4608      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -4.59e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 35        |
|    policy_loss        | -659      |
|    value_loss         | 6.3e+03   |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -214      |
| time/                 |           |
|    total_timesteps    | 5120      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -1.85e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 39        |
|    policy_loss        | -845      |
|    value_loss         | 1e+04     |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -201      |
| time/                 |           |
|    total_timesteps    | 5632      |
| train/                |           |
|    entropy_loss       | -9.9      |
|    explained_variance | -6.79e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 43        |
|    policy_loss        | -869      |
|    value_loss         | 1.09e+04  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -197      |
| time/                 |           |
|    total_timesteps    | 6144      |
| train/                |           |
|    entropy_loss       | -9.96     |
|    explained_variance | -5.91e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 47        |
|    policy_loss        | -734      |
|    value_loss         | 7.75e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -198     |
| time/                 |          |
|    total_timesteps    | 6656     |
| train/                |          |
|    entropy_loss       | -9.88    |
|    explained_variance | -8.7e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 51       |
|    policy_loss        | -668     |
|    value_loss         | 7.12e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -191      |
| time/                 |           |
|    total_timesteps    | 7168      |
| train/                |           |
|    entropy_loss       | -9.98     |
|    explained_variance | -4.48e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 55        |
|    policy_loss        | -651      |
|    value_loss         | 6.35e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -196      |
| time/                 |           |
|    total_timesteps    | 7680      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -3.09e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 59        |
|    policy_loss        | -712      |
|    value_loss         | 7.64e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 105       |
|    mean_reward        | -290      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -2.38e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -586      |
|    value_loss         | 5.43e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 113       |
|    mean_reward        | -257      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -2.21e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | -771      |
|    value_loss         | 8.59e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 81        |
|    mean_reward        | -413      |
| time/                 |           |
|    total_timesteps    | 9216      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -2.98e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 71        |
|    policy_loss        | -856      |
|    value_loss         | 9.55e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 111       |
|    mean_reward        | -338      |
| time/                 |           |
|    total_timesteps    | 9728      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -5.84e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 75        |
|    policy_loss        | -627      |
|    value_loss         | 6.21e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 107       |
|    mean_reward        | -358      |
| time/                 |           |
|    total_timesteps    | 10240     |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -2.38e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 79        |
|    policy_loss        | -861      |
|    value_loss         | 9.9e+03   |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 115       |
|    mean_reward        | -337      |
| time/                 |           |
|    total_timesteps    | 10752     |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -4.77e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 83        |
|    policy_loss        | -731      |
|    value_loss         | 8.01e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 88.6     |
|    mean_reward        | -414     |
| time/                 |          |
|    total_timesteps    | 11264    |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 8.94e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 87       |
|    policy_loss        | -704     |
|    value_loss         | 6.93e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 180       |
|    mean_reward        | -485      |
| time/                 |           |
|    total_timesteps    | 11776     |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -5.48e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 91        |
|    policy_loss        | -935      |
|    value_loss         | 1.15e+04  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 138      |
|    mean_reward        | -300     |
| time/                 |          |
|    total_timesteps    | 12288    |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 4.41e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 95       |
|    policy_loss        | -787     |
|    value_loss         | 8.64e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 169      |
|    mean_reward        | -218     |
| time/                 |          |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 5.3e-06  |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | -891     |
|    value_loss         | 1.09e+04 |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 77.6     |
|    ep_rew_mean     | -422     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 17197    |
|    total_timesteps | 12800    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -270     |
| time/                 |          |
|    total_timesteps    | 13312    |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | -5.4e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 103      |
|    policy_loss        | -862     |
|    value_loss         | 1.04e+04 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 124       |
|    mean_reward        | -146      |
| time/                 |           |
|    total_timesteps    | 13824     |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 107       |
|    policy_loss        | -763      |
|    value_loss         | 8.23e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -203     |
| time/                 |          |
|    total_timesteps    | 14336    |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 8.82e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 111      |
|    policy_loss        | -692     |
|    value_loss         | 6.68e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -209     |
| time/                 |          |
|    total_timesteps    | 14848    |
| train/                |          |
|    entropy_loss       | -9.98    |
|    explained_variance | 8.52e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 115      |
|    policy_loss        | -557     |
|    value_loss         | 4.89e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -210     |
| time/                 |          |
|    total_timesteps    | 15360    |
| train/                |          |
|    entropy_loss       | -9.99    |
|    explained_variance | -5.6e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 119      |
|    policy_loss        | -732     |
|    value_loss         | 7.26e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -187      |
| time/                 |           |
|    total_timesteps    | 15872     |
| train/                |           |
|    entropy_loss       | -9.83     |
|    explained_variance | -1.54e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 123       |
|    policy_loss        | -670      |
|    value_loss         | 6.72e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -197      |
| time/                 |           |
|    total_timesteps    | 16384     |
| train/                |           |
|    entropy_loss       | -9.82     |
|    explained_variance | -8.94e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 127       |
|    policy_loss        | -550      |
|    value_loss         | 5.03e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -202     |
| time/                 |          |
|    total_timesteps    | 16896    |
| train/                |          |
|    entropy_loss       | -9.87    |
|    explained_variance | 3.52e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 131      |
|    policy_loss        | -582     |
|    value_loss         | 4.73e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -204      |
| time/                 |           |
|    total_timesteps    | 17408     |
| train/                |           |
|    entropy_loss       | -9.89     |
|    explained_variance | -2.48e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 135       |
|    policy_loss        | -490      |
|    value_loss         | 3.82e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -198      |
| time/                 |           |
|    total_timesteps    | 17920     |
| train/                |           |
|    entropy_loss       | -10       |
|    explained_variance | -3.81e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 139       |
|    policy_loss        | -722      |
|    value_loss         | 7.09e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -204      |
| time/                 |           |
|    total_timesteps    | 18432     |
| train/                |           |
|    entropy_loss       | -9.97     |
|    explained_variance | -5.25e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 143       |
|    policy_loss        | -425      |
|    value_loss         | 3.23e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -260      |
| time/                 |           |
|    total_timesteps    | 18944     |
| train/                |           |
|    entropy_loss       | -10       |
|    explained_variance | -1.44e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 147       |
|    policy_loss        | -762      |
|    value_loss         | 7.76e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -385     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    entropy_loss       | -10      |
|    explained_variance | 8.34e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | -722     |
|    value_loss         | 7.24e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -233      |
| time/                 |           |
|    total_timesteps    | 19968     |
| train/                |           |
|    entropy_loss       | -10       |
|    explained_variance | -7.27e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 155       |
|    policy_loss        | -516      |
|    value_loss         | 4.04e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 20480    |
| train/                |          |
|    entropy_loss       | -10      |
|    explained_variance | 3.64e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 159      |
|    policy_loss        | -648     |
|    value_loss         | 6.08e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -242      |
| time/                 |           |
|    total_timesteps    | 20992     |
| train/                |           |
|    entropy_loss       | -10       |
|    explained_variance | -3.34e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 163       |
|    policy_loss        | -514      |
|    value_loss         | 4.06e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -245     |
| time/                 |          |
|    total_timesteps    | 21504    |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 2.38e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 167      |
|    policy_loss        | -717     |
|    value_loss         | 7.21e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -275      |
| time/                 |           |
|    total_timesteps    | 22016     |
| train/                |           |
|    entropy_loss       | -10       |
|    explained_variance | -1.25e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 171       |
|    policy_loss        | -638      |
|    value_loss         | 6.05e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -261     |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    entropy_loss       | -10      |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | -671     |
|    value_loss         | 6.01e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -260      |
| time/                 |           |
|    total_timesteps    | 23040     |
| train/                |           |
|    entropy_loss       | -9.96     |
|    explained_variance | -1.53e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 179       |
|    policy_loss        | -741      |
|    value_loss         | 8.05e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -274      |
| time/                 |           |
|    total_timesteps    | 23552     |
| train/                |           |
|    entropy_loss       | -9.92     |
|    explained_variance | -8.82e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 183       |
|    policy_loss        | -508      |
|    value_loss         | 3.61e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -202      |
| time/                 |           |
|    total_timesteps    | 24064     |
| train/                |           |
|    entropy_loss       | -9.93     |
|    explained_variance | -9.05e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 187       |
|    policy_loss        | -518      |
|    value_loss         | 4.11e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -209      |
| time/                 |           |
|    total_timesteps    | 24576     |
| train/                |           |
|    entropy_loss       | -9.86     |
|    explained_variance | -4.29e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 191       |
|    policy_loss        | -631      |
|    value_loss         | 5.56e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -210     |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    entropy_loss       | -9.85    |
|    explained_variance | -3.7e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | -440     |
|    value_loss         | 3.31e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -189      |
| time/                 |           |
|    total_timesteps    | 25600     |
| train/                |           |
|    entropy_loss       | -9.8      |
|    explained_variance | -2.09e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 199       |
|    policy_loss        | -487      |
|    value_loss         | 3.95e+03  |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 92.1     |
|    ep_rew_mean     | -400     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 33813    |
|    total_timesteps | 25600    |
---------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -202      |
| time/                 |           |
|    total_timesteps    | 26112     |
| train/                |           |
|    entropy_loss       | -9.74     |
|    explained_variance | -2.26e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 203       |
|    policy_loss        | -417      |
|    value_loss         | 2.72e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -191      |
| time/                 |           |
|    total_timesteps    | 26624     |
| train/                |           |
|    entropy_loss       | -9.71     |
|    explained_variance | -2.98e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 207       |
|    policy_loss        | -558      |
|    value_loss         | 4.36e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -211      |
| time/                 |           |
|    total_timesteps    | 27136     |
| train/                |           |
|    entropy_loss       | -9.76     |
|    explained_variance | -6.68e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 211       |
|    policy_loss        | -443      |
|    value_loss         | 3.17e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -203      |
| time/                 |           |
|    total_timesteps    | 27648     |
| train/                |           |
|    entropy_loss       | -9.66     |
|    explained_variance | -2.98e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 215       |
|    policy_loss        | -451      |
|    value_loss         | 3.33e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -219     |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    entropy_loss       | -9.61    |
|    explained_variance | 1.79e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | -399     |
|    value_loss         | 2.68e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -202      |
| time/                 |           |
|    total_timesteps    | 28672     |
| train/                |           |
|    entropy_loss       | -9.58     |
|    explained_variance | -6.68e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 223       |
|    policy_loss        | -581      |
|    value_loss         | 5.23e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 227       |
|    mean_reward        | -192      |
| time/                 |           |
|    total_timesteps    | 29184     |
| train/                |           |
|    entropy_loss       | -9.47     |
|    explained_variance | -8.34e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 227       |
|    policy_loss        | -415      |
|    value_loss         | 3.08e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -193      |
| time/                 |           |
|    total_timesteps    | 29696     |
| train/                |           |
|    entropy_loss       | -9.53     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 231       |
|    policy_loss        | -379      |
|    value_loss         | 2.55e+03  |
-------------------------------------
