Logging to ./logs_A2C_0.01_no-noise
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -189      |
| time/                 |           |
|    total_timesteps    | 512       |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0003    |
|    n_updates          | 7         |
|    policy_loss        | -486      |
|    value_loss         | 3.3e+03   |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -189     |
| time/                 |          |
|    total_timesteps    | 1024     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 0        |
|    learning_rate      | 0.0003   |
|    n_updates          | 15       |
|    policy_loss        | -375     |
|    value_loss         | 1.9e+03  |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -189     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.0003   |
|    n_updates          | 23       |
|    policy_loss        | -514     |
|    value_loss         | 3.16e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -189     |
| time/                 |          |
|    total_timesteps    | 2048     |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.0003   |
|    n_updates          | 31       |
|    policy_loss        | -417     |
|    value_loss         | 2.18e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -189     |
| time/                 |          |
|    total_timesteps    | 2560     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 0        |
|    learning_rate      | 0.0003   |
|    n_updates          | 39       |
|    policy_loss        | -296     |
|    value_loss         | 1.37e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -189     |
| time/                 |          |
|    total_timesteps    | 3072     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0003   |
|    n_updates          | 47       |
|    policy_loss        | -447     |
|    value_loss         | 2.65e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -189     |
| time/                 |          |
|    total_timesteps    | 3584     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 2.38e-07 |
|    learning_rate      | 0.0003   |
|    n_updates          | 55       |
|    policy_loss        | -386     |
|    value_loss         | 2.03e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -189     |
| time/                 |          |
|    total_timesteps    | 4096     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 0        |
|    learning_rate      | 0.0003   |
|    n_updates          | 63       |
|    policy_loss        | -261     |
|    value_loss         | 1.22e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -189     |
| time/                 |          |
|    total_timesteps    | 4608     |
| train/                |          |
|    entropy_loss       | -10      |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0003   |
|    n_updates          | 71       |
|    policy_loss        | -297     |
|    value_loss         | 1.17e+03 |
------------------------------------
