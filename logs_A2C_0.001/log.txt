Logging to ./logs_A2C_0.001
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 44.4     |
|    mean_reward        | -372     |
| time/                 |          |
|    total_timesteps    | 512      |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 0.00012  |
|    learning_rate      | 0.0007   |
|    n_updates          | 3        |
|    policy_loss        | -749     |
|    value_loss         | 7.9e+03  |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 43.2      |
|    mean_reward        | -203      |
| time/                 |           |
|    total_timesteps    | 1024      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000147 |
|    learning_rate      | 0.0007    |
|    n_updates          | 7         |
|    policy_loss        | -1.05e+03 |
|    value_loss         | 1.45e+04  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -216     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 3.93e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 11       |
|    policy_loss        | -752     |
|    value_loss         | 8.72e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 2048      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -0.000119 |
|    learning_rate      | 0.0007    |
|    n_updates          | 15        |
|    policy_loss        | -708      |
|    value_loss         | 6.49e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -243      |
| time/                 |           |
|    total_timesteps    | 2560      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -0.000105 |
|    learning_rate      | 0.0007    |
|    n_updates          | 19        |
|    policy_loss        | -779      |
|    value_loss         | 8.2e+03   |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 396       |
|    mean_reward        | -242      |
| time/                 |           |
|    total_timesteps    | 3072      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -5.07e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 23        |
|    policy_loss        | -666      |
|    value_loss         | 7.35e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 57        |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 3584      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -0.000103 |
|    learning_rate      | 0.0007    |
|    n_updates          | 27        |
|    policy_loss        | -575      |
|    value_loss         | 5.18e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 4096      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -5.19e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 31        |
|    policy_loss        | -697      |
|    value_loss         | 7.21e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 4608      |
| train/                |           |
|    entropy_loss       | -10       |
|    explained_variance | -4.09e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 35        |
|    policy_loss        | -698      |
|    value_loss         | 7.15e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 5120     |
| train/                |          |
|    entropy_loss       | -9.88    |
|    explained_variance | -7.7e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 39       |
|    policy_loss        | -614     |
|    value_loss         | 5.8e+03  |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 5632      |
| train/                |           |
|    entropy_loss       | -9.71     |
|    explained_variance | -3.04e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 43        |
|    policy_loss        | -582      |
|    value_loss         | 6.69e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 6144      |
| train/                |           |
|    entropy_loss       | -9.72     |
|    explained_variance | -2.16e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 47        |
|    policy_loss        | -592      |
|    value_loss         | 6.42e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 6656     |
| train/                |          |
|    entropy_loss       | -9.44    |
|    explained_variance | 3.8e-05  |
|    learning_rate      | 0.0007   |
|    n_updates          | 51       |
|    policy_loss        | -398     |
|    value_loss         | 2.62e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7168      |
| train/                |           |
|    entropy_loss       | -9.22     |
|    explained_variance | -0.000109 |
|    learning_rate      | 0.0007    |
|    n_updates          | 55        |
|    policy_loss        | -246      |
|    value_loss         | 1.73e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7680      |
| train/                |           |
|    entropy_loss       | -9.02     |
|    explained_variance | -0.000144 |
|    learning_rate      | 0.0007    |
|    n_updates          | 59        |
|    policy_loss        | -107      |
|    value_loss         | 465       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    entropy_loss       | -7.91     |
|    explained_variance | -0.000261 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -39.1     |
|    value_loss         | 264       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    entropy_loss       | -6.41     |
|    explained_variance | -0.000271 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | -25.5     |
|    value_loss         | 189       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 9216     |
| train/                |          |
|    entropy_loss       | -5.37    |
|    explained_variance | -1.3e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 71       |
|    policy_loss        | 13.2     |
|    value_loss         | 83.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 9728     |
| train/                |          |
|    entropy_loss       | -5.17    |
|    explained_variance | 0.000152 |
|    learning_rate      | 0.0007   |
|    n_updates          | 75       |
|    policy_loss        | 14.6     |
|    value_loss         | 93       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 10240    |
| train/                |          |
|    entropy_loss       | -4.87    |
|    explained_variance | 0.000547 |
|    learning_rate      | 0.0007   |
|    n_updates          | 79       |
|    policy_loss        | 12.4     |
|    value_loss         | 77.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 10752    |
| train/                |          |
|    entropy_loss       | -4.73    |
|    explained_variance | 0.00135  |
|    learning_rate      | 0.0007   |
|    n_updates          | 83       |
|    policy_loss        | 10.1     |
|    value_loss         | 66.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11264    |
| train/                |          |
|    entropy_loss       | -4.56    |
|    explained_variance | 0.00182  |
|    learning_rate      | 0.0007   |
|    n_updates          | 87       |
|    policy_loss        | 13.5     |
|    value_loss         | 60.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11776    |
| train/                |          |
|    entropy_loss       | -4.42    |
|    explained_variance | 0.01     |
|    learning_rate      | 0.0007   |
|    n_updates          | 91       |
|    policy_loss        | 9.96     |
|    value_loss         | 64.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12288    |
| train/                |          |
|    entropy_loss       | -4.33    |
|    explained_variance | 0.0761   |
|    learning_rate      | 0.0007   |
|    n_updates          | 95       |
|    policy_loss        | 4.06     |
|    value_loss         | 52.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -3.92    |
|    explained_variance | 0.154    |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | -2.54    |
|    value_loss         | 41.3     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.17     |
|    ep_rew_mean     | -13.3    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 46296    |
|    total_timesteps | 12800    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13312    |
| train/                |          |
|    entropy_loss       | -3.95    |
|    explained_variance | 0.257    |
|    learning_rate      | 0.0007   |
|    n_updates          | 103      |
|    policy_loss        | -7.51    |
|    value_loss         | 68.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13824    |
| train/                |          |
|    entropy_loss       | -3.94    |
|    explained_variance | 0.129    |
|    learning_rate      | 0.0007   |
|    n_updates          | 107      |
|    policy_loss        | -3.43    |
|    value_loss         | 45.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14336    |
| train/                |          |
|    entropy_loss       | -3.78    |
|    explained_variance | 0.422    |
|    learning_rate      | 0.0007   |
|    n_updates          | 111      |
|    policy_loss        | -1.03    |
|    value_loss         | 16.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14848    |
| train/                |          |
|    entropy_loss       | -3.39    |
|    explained_variance | 0.52     |
|    learning_rate      | 0.0007   |
|    n_updates          | 115      |
|    policy_loss        | -4.44    |
|    value_loss         | 23.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15360    |
| train/                |          |
|    entropy_loss       | -3.22    |
|    explained_variance | 0.47     |
|    learning_rate      | 0.0007   |
|    n_updates          | 119      |
|    policy_loss        | 0.287    |
|    value_loss         | 23.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15872    |
| train/                |          |
|    entropy_loss       | -2.99    |
|    explained_variance | 0.347    |
|    learning_rate      | 0.0007   |
|    n_updates          | 123      |
|    policy_loss        | -7.49    |
|    value_loss         | 44.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16384    |
| train/                |          |
|    entropy_loss       | -2.97    |
|    explained_variance | 0.496    |
|    learning_rate      | 0.0007   |
|    n_updates          | 127      |
|    policy_loss        | -3.04    |
|    value_loss         | 21.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16896    |
| train/                |          |
|    entropy_loss       | -3.02    |
|    explained_variance | 0.463    |
|    learning_rate      | 0.0007   |
|    n_updates          | 131      |
|    policy_loss        | 0.177    |
|    value_loss         | 15       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17408    |
| train/                |          |
|    entropy_loss       | -3.08    |
|    explained_variance | 0.353    |
|    learning_rate      | 0.0007   |
|    n_updates          | 135      |
|    policy_loss        | -2.83    |
|    value_loss         | 15.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17920    |
| train/                |          |
|    entropy_loss       | -3.05    |
|    explained_variance | 0.577    |
|    learning_rate      | 0.0007   |
|    n_updates          | 139      |
|    policy_loss        | -1.34    |
|    value_loss         | 11.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18432    |
| train/                |          |
|    entropy_loss       | -3.18    |
|    explained_variance | 0.431    |
|    learning_rate      | 0.0007   |
|    n_updates          | 143      |
|    policy_loss        | -4.57    |
|    value_loss         | 23.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    entropy_loss       | -3.11    |
|    explained_variance | 0.617    |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | -0.14    |
|    value_loss         | 8.58     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    entropy_loss       | -2.99    |
|    explained_variance | 0.345    |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | -2.69    |
|    value_loss         | 16.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19968    |
| train/                |          |
|    entropy_loss       | -2.72    |
|    explained_variance | 0.431    |
|    learning_rate      | 0.0007   |
|    n_updates          | 155      |
|    policy_loss        | -0.0221  |
|    value_loss         | 18.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20480    |
| train/                |          |
|    entropy_loss       | -2.58    |
|    explained_variance | 0.538    |
|    learning_rate      | 0.0007   |
|    n_updates          | 159      |
|    policy_loss        | 0.744    |
|    value_loss         | 8.87     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -194     |
| time/                 |          |
|    total_timesteps    | 20992    |
| train/                |          |
|    entropy_loss       | -2.65    |
|    explained_variance | 0.548    |
|    learning_rate      | 0.0007   |
|    n_updates          | 163      |
|    policy_loss        | -0.736   |
|    value_loss         | 12.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 61.2     |
|    mean_reward        | -36      |
| time/                 |          |
|    total_timesteps    | 21504    |
| train/                |          |
|    entropy_loss       | -2.67    |
|    explained_variance | 0.599    |
|    learning_rate      | 0.0007   |
|    n_updates          | 167      |
|    policy_loss        | -1.33    |
|    value_loss         | 6.29     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -78.1    |
| time/                 |          |
|    total_timesteps    | 22016    |
| train/                |          |
|    entropy_loss       | -2.55    |
|    explained_variance | 0.641    |
|    learning_rate      | 0.0007   |
|    n_updates          | 171      |
|    policy_loss        | 0.547    |
|    value_loss         | 7.54     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -65      |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    entropy_loss       | -2.63    |
|    explained_variance | 0.726    |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | 5.38     |
|    value_loss         | 8.5      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23040    |
| train/                |          |
|    entropy_loss       | -2.52    |
|    explained_variance | 0.487    |
|    learning_rate      | 0.0007   |
|    n_updates          | 179      |
|    policy_loss        | 0.0514   |
|    value_loss         | 17.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23552    |
| train/                |          |
|    entropy_loss       | -2.54    |
|    explained_variance | 0.543    |
|    learning_rate      | 0.0007   |
|    n_updates          | 183      |
|    policy_loss        | -0.301   |
|    value_loss         | 7.3      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80.8     |
|    mean_reward        | -60.1    |
| time/                 |          |
|    total_timesteps    | 24064    |
| train/                |          |
|    entropy_loss       | -2.52    |
|    explained_variance | 0.564    |
|    learning_rate      | 0.0007   |
|    n_updates          | 187      |
|    policy_loss        | -0.976   |
|    value_loss         | 7.69     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.4      |
|    mean_reward        | -5.95    |
| time/                 |          |
|    total_timesteps    | 24576    |
| train/                |          |
|    entropy_loss       | -2.58    |
|    explained_variance | 0.706    |
|    learning_rate      | 0.0007   |
|    n_updates          | 191      |
|    policy_loss        | -1.48    |
|    value_loss         | 4.92     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 44.8     |
|    mean_reward        | -48.4    |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    entropy_loss       | -2.49    |
|    explained_variance | 0.728    |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | 0.273    |
|    value_loss         | 4.17     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 25.2     |
|    mean_reward        | -8.52    |
| time/                 |          |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -2.48    |
|    explained_variance | 0.372    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | -3.14    |
|    value_loss         | 20       |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.51     |
|    ep_rew_mean     | -9.12    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 83835    |
|    total_timesteps | 25600    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 61.4     |
|    mean_reward        | -23.3    |
| time/                 |          |
|    total_timesteps    | 26112    |
| train/                |          |
|    entropy_loss       | -2.56    |
|    explained_variance | 0.515    |
|    learning_rate      | 0.0007   |
|    n_updates          | 203      |
|    policy_loss        | -2.68    |
|    value_loss         | 14.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 22.8     |
|    mean_reward        | -9.77    |
| time/                 |          |
|    total_timesteps    | 26624    |
| train/                |          |
|    entropy_loss       | -2.57    |
|    explained_variance | 0.705    |
|    learning_rate      | 0.0007   |
|    n_updates          | 207      |
|    policy_loss        | -0.664   |
|    value_loss         | 5.03     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -50.7    |
| time/                 |          |
|    total_timesteps    | 27136    |
| train/                |          |
|    entropy_loss       | -2.59    |
|    explained_variance | 0.835    |
|    learning_rate      | 0.0007   |
|    n_updates          | 211      |
|    policy_loss        | 1.66     |
|    value_loss         | 2.1      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 23.8     |
|    mean_reward        | -7.44    |
| time/                 |          |
|    total_timesteps    | 27648    |
| train/                |          |
|    entropy_loss       | -2.58    |
|    explained_variance | 0.743    |
|    learning_rate      | 0.0007   |
|    n_updates          | 215      |
|    policy_loss        | -1.6     |
|    value_loss         | 3.74     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -65.3    |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    entropy_loss       | -2.55    |
|    explained_variance | 0.769    |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | 2.49     |
|    value_loss         | 3.74     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -50.9    |
| time/                 |          |
|    total_timesteps    | 28672    |
| train/                |          |
|    entropy_loss       | -2.57    |
|    explained_variance | 0.716    |
|    learning_rate      | 0.0007   |
|    n_updates          | 223      |
|    policy_loss        | 0.792    |
|    value_loss         | 3.35     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 65.6     |
|    mean_reward        | -17.2    |
| time/                 |          |
|    total_timesteps    | 29184    |
| train/                |          |
|    entropy_loss       | -2.71    |
|    explained_variance | 0.483    |
|    learning_rate      | 0.0007   |
|    n_updates          | 227      |
|    policy_loss        | -1.27    |
|    value_loss         | 7.59     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 247      |
|    mean_reward        | -14.4    |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    entropy_loss       | -2.57    |
|    explained_variance | 0.737    |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | 2.39     |
|    value_loss         | 3.85     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 170      |
|    mean_reward        | -8.84    |
| time/                 |          |
|    total_timesteps    | 30208    |
| train/                |          |
|    entropy_loss       | -2.72    |
|    explained_variance | 0.605    |
|    learning_rate      | 0.0007   |
|    n_updates          | 235      |
|    policy_loss        | -2.68    |
|    value_loss         | 6.16     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 332      |
|    mean_reward        | -20.5    |
| time/                 |          |
|    total_timesteps    | 30720    |
| train/                |          |
|    entropy_loss       | -2.58    |
|    explained_variance | 0.62     |
|    learning_rate      | 0.0007   |
|    n_updates          | 239      |
|    policy_loss        | -1.49    |
|    value_loss         | 5.48     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 169      |
|    mean_reward        | -9.73    |
| time/                 |          |
|    total_timesteps    | 31232    |
| train/                |          |
|    entropy_loss       | -2.59    |
|    explained_variance | 0.717    |
|    learning_rate      | 0.0007   |
|    n_updates          | 243      |
|    policy_loss        | 2.98     |
|    value_loss         | 3.87     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 322      |
|    mean_reward        | -22.5    |
| time/                 |          |
|    total_timesteps    | 31744    |
| train/                |          |
|    entropy_loss       | -2.53    |
|    explained_variance | 0.717    |
|    learning_rate      | 0.0007   |
|    n_updates          | 247      |
|    policy_loss        | 1.3      |
|    value_loss         | 2.71     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 324      |
|    mean_reward        | -16.2    |
| time/                 |          |
|    total_timesteps    | 32256    |
| train/                |          |
|    entropy_loss       | -2.57    |
|    explained_variance | 0.737    |
|    learning_rate      | 0.0007   |
|    n_updates          | 251      |
|    policy_loss        | -0.925   |
|    value_loss         | 2.92     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 325      |
|    mean_reward        | -24.5    |
| time/                 |          |
|    total_timesteps    | 32768    |
| train/                |          |
|    entropy_loss       | -2.4     |
|    explained_variance | 0.707    |
|    learning_rate      | 0.0007   |
|    n_updates          | 255      |
|    policy_loss        | -0.279   |
|    value_loss         | 2.12     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -28.8    |
| time/                 |          |
|    total_timesteps    | 33280    |
| train/                |          |
|    entropy_loss       | -2.3     |
|    explained_variance | 0.704    |
|    learning_rate      | 0.0007   |
|    n_updates          | 259      |
|    policy_loss        | -0.595   |
|    value_loss         | 2.29     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 17.6     |
|    mean_reward        | -5.41    |
| time/                 |          |
|    total_timesteps    | 33792    |
| train/                |          |
|    entropy_loss       | -2.13    |
|    explained_variance | 0.762    |
|    learning_rate      | 0.0007   |
|    n_updates          | 263      |
|    policy_loss        | -0.597   |
|    value_loss         | 1.71     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -59.1    |
| time/                 |          |
|    total_timesteps    | 34304    |
| train/                |          |
|    entropy_loss       | -2.12    |
|    explained_variance | 0.85     |
|    learning_rate      | 0.0007   |
|    n_updates          | 267      |
|    policy_loss        | -0.15    |
|    value_loss         | 0.841    |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -48.8    |
| time/                 |          |
|    total_timesteps    | 34816    |
| train/                |          |
|    entropy_loss       | -2.12    |
|    explained_variance | 0.817    |
|    learning_rate      | 0.0007   |
|    n_updates          | 271      |
|    policy_loss        | -0.613   |
|    value_loss         | 1.55     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -11.9    |
| time/                 |          |
|    total_timesteps    | 35328    |
| train/                |          |
|    entropy_loss       | -1.96    |
|    explained_variance | 0.63     |
|    learning_rate      | 0.0007   |
|    n_updates          | 275      |
|    policy_loss        | -0.646   |
|    value_loss         | 2.78     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 175      |
|    mean_reward        | -8.99    |
| time/                 |          |
|    total_timesteps    | 35840    |
| train/                |          |
|    entropy_loss       | -1.97    |
|    explained_variance | 0.888    |
|    learning_rate      | 0.0007   |
|    n_updates          | 279      |
|    policy_loss        | -0.0513  |
|    value_loss         | 0.997    |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 13       |
|    mean_reward        | -5.22    |
| time/                 |          |
|    total_timesteps    | 36352    |
| train/                |          |
|    entropy_loss       | -1.91    |
|    explained_variance | 0.746    |
|    learning_rate      | 0.0007   |
|    n_updates          | 283      |
|    policy_loss        | 0.00241  |
|    value_loss         | 1.08     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 90.8     |
|    mean_reward        | -7.2     |
| time/                 |          |
|    total_timesteps    | 36864    |
| train/                |          |
|    entropy_loss       | -1.86    |
|    explained_variance | 0.76     |
|    learning_rate      | 0.0007   |
|    n_updates          | 287      |
|    policy_loss        | -1.73    |
|    value_loss         | 2.4      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 247      |
|    mean_reward        | -15.8    |
| time/                 |          |
|    total_timesteps    | 37376    |
| train/                |          |
|    entropy_loss       | -1.82    |
|    explained_variance | 0.844    |
|    learning_rate      | 0.0007   |
|    n_updates          | 291      |
|    policy_loss        | -0.353   |
|    value_loss         | 1.22     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 250      |
|    mean_reward        | -18.1    |
| time/                 |          |
|    total_timesteps    | 37888    |
| train/                |          |
|    entropy_loss       | -1.68    |
|    explained_variance | 0.876    |
|    learning_rate      | 0.0007   |
|    n_updates          | 295      |
|    policy_loss        | -0.6     |
|    value_loss         | 0.778    |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -52.2    |
| time/                 |          |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -1.6     |
|    explained_variance | 0.876    |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | 0.00934  |
|    value_loss         | 0.595    |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.4     |
|    ep_rew_mean     | -6.97    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 300      |
|    time_elapsed    | 110275   |
|    total_timesteps | 38400    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 324      |
|    mean_reward        | -20.4    |
| time/                 |          |
|    total_timesteps    | 38912    |
| train/                |          |
|    entropy_loss       | -1.59    |
|    explained_variance | 0.685    |
|    learning_rate      | 0.0007   |
|    n_updates          | 303      |
|    policy_loss        | -2.44    |
|    value_loss         | 3.2      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 170      |
|    mean_reward        | -8.75    |
| time/                 |          |
|    total_timesteps    | 39424    |
| train/                |          |
|    entropy_loss       | -1.54    |
|    explained_variance | 0.904    |
|    learning_rate      | 0.0007   |
|    n_updates          | 307      |
|    policy_loss        | -0.472   |
|    value_loss         | 0.728    |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 96.4     |
|    mean_reward        | -6.08    |
| time/                 |          |
|    total_timesteps    | 39936    |
| train/                |          |
|    entropy_loss       | -1.47    |
|    explained_variance | 0.896    |
|    learning_rate      | 0.0007   |
|    n_updates          | 311      |
|    policy_loss        | -0.271   |
|    value_loss         | 0.493    |
------------------------------------
