Logging to ./logs_A2C_0.01/
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -584      |
| time/                 |           |
|    total_timesteps    | 512       |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -9.54e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 3         |
|    policy_loss        | -666      |
|    value_loss         | 6.19e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -406     |
| time/                 |          |
|    total_timesteps    | 1024     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 5.36e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 7        |
|    policy_loss        | -875     |
|    value_loss         | 1.1e+04  |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 11       |
|    policy_loss        | -966     |
|    value_loss         | 1.23e+04 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -246     |
| time/                 |          |
|    total_timesteps    | 2048     |
| train/                |          |
|    entropy_loss       | -9.98    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 15       |
|    policy_loss        | -516     |
|    value_loss         | 4.1e+03  |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -474     |
| time/                 |          |
|    total_timesteps    | 2560     |
| train/                |          |
|    entropy_loss       | -10      |
|    explained_variance | 2.98e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 19       |
|    policy_loss        | -665     |
|    value_loss         | 7.28e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 399       |
|    mean_reward        | -241      |
| time/                 |           |
|    total_timesteps    | 3072      |
| train/                |           |
|    entropy_loss       | -9.87     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 23        |
|    policy_loss        | -781      |
|    value_loss         | 9.14e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 58       |
|    mean_reward        | -53.8    |
| time/                 |          |
|    total_timesteps    | 3584     |
| train/                |          |
|    entropy_loss       | -9.91    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 27       |
|    policy_loss        | -574     |
|    value_loss         | 6.59e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -250      |
| time/                 |           |
|    total_timesteps    | 4096      |
| train/                |           |
|    entropy_loss       | -9.79     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 31        |
|    policy_loss        | -591      |
|    value_loss         | 5.93e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 58       |
|    mean_reward        | -184     |
| time/                 |          |
|    total_timesteps    | 4608     |
| train/                |          |
|    entropy_loss       | -9.73    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 35       |
|    policy_loss        | -536     |
|    value_loss         | 5.17e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 5120     |
| train/                |          |
|    entropy_loss       | -9.6     |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 39       |
|    policy_loss        | -329     |
|    value_loss         | 2.05e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 5632     |
| train/                |          |
|    entropy_loss       | -9.42    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.0007   |
|    n_updates          | 43       |
|    policy_loss        | -343     |
|    value_loss         | 2.68e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 6144      |
| train/                |           |
|    entropy_loss       | -9.25     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 47        |
|    policy_loss        | -341      |
|    value_loss         | 2.25e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -180      |
| time/                 |           |
|    total_timesteps    | 6656      |
| train/                |           |
|    entropy_loss       | -8.97     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 51        |
|    policy_loss        | -240      |
|    value_loss         | 1.52e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 7168     |
| train/                |          |
|    entropy_loss       | -8.44    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 55       |
|    policy_loss        | -160     |
|    value_loss         | 957      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 7680     |
| train/                |          |
|    entropy_loss       | -7.36    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 59       |
|    policy_loss        | -57.4    |
|    value_loss         | 313      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    entropy_loss       | -6.21     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -8.93     |
|    value_loss         | 132       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -180      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    entropy_loss       | -4.98     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | 11.6      |
|    value_loss         | 62.3      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 9216      |
| train/                |           |
|    entropy_loss       | -4.56     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 71        |
|    policy_loss        | 20.3      |
|    value_loss         | 72.6      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 9728      |
| train/                |           |
|    entropy_loss       | -4.52     |
|    explained_variance | -4.77e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 75        |
|    policy_loss        | 9.77      |
|    value_loss         | 58.8      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 10240     |
| train/                |           |
|    entropy_loss       | -4.17     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 79        |
|    policy_loss        | 15.4      |
|    value_loss         | 72.3      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 10752     |
| train/                |           |
|    entropy_loss       | -4.14     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 83        |
|    policy_loss        | 14.7      |
|    value_loss         | 54        |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 11264     |
| train/                |           |
|    entropy_loss       | -4.01     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 87        |
|    policy_loss        | 15.6      |
|    value_loss         | 71.5      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 11776    |
| train/                |          |
|    entropy_loss       | -3.75    |
|    explained_variance | 3.58e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 91       |
|    policy_loss        | 5.82     |
|    value_loss         | 45.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12288    |
| train/                |          |
|    entropy_loss       | -3.78    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 95       |
|    policy_loss        | 12.9     |
|    value_loss         | 54.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -3.58    |
|    explained_variance | 4.77e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | 6.13     |
|    value_loss         | 56.9     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.66     |
|    ep_rew_mean     | -12.6    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 40276    |
|    total_timesteps | 12800    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 13312    |
| train/                |          |
|    entropy_loss       | -3.49    |
|    explained_variance | 1.67e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 103      |
|    policy_loss        | 8.12     |
|    value_loss         | 60.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13824    |
| train/                |          |
|    entropy_loss       | -3.48    |
|    explained_variance | 1.01e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 107      |
|    policy_loss        | 6.24     |
|    value_loss         | 61       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14336    |
| train/                |          |
|    entropy_loss       | -3.46    |
|    explained_variance | 2e-05    |
|    learning_rate      | 0.0007   |
|    n_updates          | 111      |
|    policy_loss        | 12.5     |
|    value_loss         | 41.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14848    |
| train/                |          |
|    entropy_loss       | -3.29    |
|    explained_variance | 7.66e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 115      |
|    policy_loss        | 6.5      |
|    value_loss         | 37.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15360    |
| train/                |          |
|    entropy_loss       | -3.38    |
|    explained_variance | 0.00254  |
|    learning_rate      | 0.0007   |
|    n_updates          | 119      |
|    policy_loss        | 1.24     |
|    value_loss         | 46.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15872    |
| train/                |          |
|    entropy_loss       | -3.39    |
|    explained_variance | 0.0144   |
|    learning_rate      | 0.0007   |
|    n_updates          | 123      |
|    policy_loss        | -13.4    |
|    value_loss         | 53.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16384    |
| train/                |          |
|    entropy_loss       | -3.25    |
|    explained_variance | 0.0628   |
|    learning_rate      | 0.0007   |
|    n_updates          | 127      |
|    policy_loss        | 6.27     |
|    value_loss         | 32.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16896    |
| train/                |          |
|    entropy_loss       | -3.08    |
|    explained_variance | 0.183    |
|    learning_rate      | 0.0007   |
|    n_updates          | 131      |
|    policy_loss        | -0.36    |
|    value_loss         | 13       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17408    |
| train/                |          |
|    entropy_loss       | -2.97    |
|    explained_variance | 0.238    |
|    learning_rate      | 0.0007   |
|    n_updates          | 135      |
|    policy_loss        | 1.06     |
|    value_loss         | 9.33     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17920    |
| train/                |          |
|    entropy_loss       | -2.93    |
|    explained_variance | 0.268    |
|    learning_rate      | 0.0007   |
|    n_updates          | 139      |
|    policy_loss        | -4.88    |
|    value_loss         | 16.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18432    |
| train/                |          |
|    entropy_loss       | -2.89    |
|    explained_variance | 0.195    |
|    learning_rate      | 0.0007   |
|    n_updates          | 143      |
|    policy_loss        | -4.55    |
|    value_loss         | 19.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    entropy_loss       | -2.82    |
|    explained_variance | 0.346    |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | 0.0255   |
|    value_loss         | 14.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    entropy_loss       | -2.95    |
|    explained_variance | 0.197    |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | -3.46    |
|    value_loss         | 30.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19968    |
| train/                |          |
|    entropy_loss       | -2.73    |
|    explained_variance | 0.456    |
|    learning_rate      | 0.0007   |
|    n_updates          | 155      |
|    policy_loss        | -0.0655  |
|    value_loss         | 8.03     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 20480    |
| train/                |          |
|    entropy_loss       | -2.47    |
|    explained_variance | 0.372    |
|    learning_rate      | 0.0007   |
|    n_updates          | 159      |
|    policy_loss        | -1.19    |
|    value_loss         | 12.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20992    |
| train/                |          |
|    entropy_loss       | -2.38    |
|    explained_variance | 0.481    |
|    learning_rate      | 0.0007   |
|    n_updates          | 163      |
|    policy_loss        | -1.49    |
|    value_loss         | 8.99     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 21504    |
| train/                |          |
|    entropy_loss       | -2.38    |
|    explained_variance | 0.471    |
|    learning_rate      | 0.0007   |
|    n_updates          | 167      |
|    policy_loss        | 4.19     |
|    value_loss         | 7.77     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 22016    |
| train/                |          |
|    entropy_loss       | -2.37    |
|    explained_variance | 0.552    |
|    learning_rate      | 0.0007   |
|    n_updates          | 171      |
|    policy_loss        | -0.844   |
|    value_loss         | 5.68     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    entropy_loss       | -2.35    |
|    explained_variance | 0.447    |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | -1.56    |
|    value_loss         | 6.54     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23040    |
| train/                |          |
|    entropy_loss       | -2.2     |
|    explained_variance | 0.398    |
|    learning_rate      | 0.0007   |
|    n_updates          | 179      |
|    policy_loss        | -3.18    |
|    value_loss         | 13       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23552    |
| train/                |          |
|    entropy_loss       | -2.18    |
|    explained_variance | 0.71     |
|    learning_rate      | 0.0007   |
|    n_updates          | 183      |
|    policy_loss        | 1.76     |
|    value_loss         | 3.66     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 24064    |
| train/                |          |
|    entropy_loss       | -2.19    |
|    explained_variance | 0.672    |
|    learning_rate      | 0.0007   |
|    n_updates          | 187      |
|    policy_loss        | 0.435    |
|    value_loss         | 3.32     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 24576    |
| train/                |          |
|    entropy_loss       | -2.22    |
|    explained_variance | 0.665    |
|    learning_rate      | 0.0007   |
|    n_updates          | 191      |
|    policy_loss        | -1.02    |
|    value_loss         | 2.8      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    entropy_loss       | -2.1     |
|    explained_variance | 0.533    |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | -0.505   |
|    value_loss         | 3.68     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -1.94    |
|    explained_variance | 0.612    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | -2.64    |
|    value_loss         | 3.87     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.1     |
|    ep_rew_mean     | -8.65    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 69611    |
|    total_timesteps | 25600    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 26112    |
| train/                |          |
|    entropy_loss       | -1.97    |
|    explained_variance | 0.72     |
|    learning_rate      | 0.0007   |
|    n_updates          | 203      |
|    policy_loss        | 0.421    |
|    value_loss         | 3.03     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 26624    |
| train/                |          |
|    entropy_loss       | -2.05    |
|    explained_variance | 0.744    |
|    learning_rate      | 0.0007   |
|    n_updates          | 207      |
|    policy_loss        | 1.53     |
|    value_loss         | 2.7      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 27136    |
| train/                |          |
|    entropy_loss       | -2.04    |
|    explained_variance | 0.749    |
|    learning_rate      | 0.0007   |
|    n_updates          | 211      |
|    policy_loss        | 1.66     |
|    value_loss         | 2.77     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 27648    |
| train/                |          |
|    entropy_loss       | -2.07    |
|    explained_variance | 0.657    |
|    learning_rate      | 0.0007   |
|    n_updates          | 215      |
|    policy_loss        | -0.691   |
|    value_loss         | 2.96     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    entropy_loss       | -2.06    |
|    explained_variance | 0.564    |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | 1.17     |
|    value_loss         | 4.9      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 28672    |
| train/                |          |
|    entropy_loss       | -2.08    |
|    explained_variance | 0.403    |
|    learning_rate      | 0.0007   |
|    n_updates          | 223      |
|    policy_loss        | -1.26    |
|    value_loss         | 7        |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 29184    |
| train/                |          |
|    entropy_loss       | -2.06    |
|    explained_variance | 0.633    |
|    learning_rate      | 0.0007   |
|    n_updates          | 227      |
|    policy_loss        | 0.514    |
|    value_loss         | 3.08     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    entropy_loss       | -1.99    |
|    explained_variance | 0.489    |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | -2.5     |
|    value_loss         | 5.36     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 30208    |
| train/                |          |
|    entropy_loss       | -1.96    |
|    explained_variance | 0.707    |
|    learning_rate      | 0.0007   |
|    n_updates          | 235      |
|    policy_loss        | 0.524    |
|    value_loss         | 2.52     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 30720    |
| train/                |          |
|    entropy_loss       | -2.02    |
|    explained_variance | 0.732    |
|    learning_rate      | 0.0007   |
|    n_updates          | 239      |
|    policy_loss        | -0.276   |
|    value_loss         | 2.06     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 31232    |
| train/                |          |
|    entropy_loss       | -1.99    |
|    explained_variance | 0.68     |
|    learning_rate      | 0.0007   |
|    n_updates          | 243      |
|    policy_loss        | -0.838   |
|    value_loss         | 2.88     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 31744    |
| train/                |          |
|    entropy_loss       | -1.97    |
|    explained_variance | 0.685    |
|    learning_rate      | 0.0007   |
|    n_updates          | 247      |
|    policy_loss        | 0.671    |
|    value_loss         | 2.14     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 32256    |
| train/                |          |
|    entropy_loss       | -1.95    |
|    explained_variance | 0.719    |
|    learning_rate      | 0.0007   |
|    n_updates          | 251      |
|    policy_loss        | -0.742   |
|    value_loss         | 2.25     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 32768    |
| train/                |          |
|    entropy_loss       | -1.94    |
|    explained_variance | 0.769    |
|    learning_rate      | 0.0007   |
|    n_updates          | 255      |
|    policy_loss        | 2.2      |
|    value_loss         | 2.84     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 33280    |
| train/                |          |
|    entropy_loss       | -2.02    |
|    explained_variance | 0.733    |
|    learning_rate      | 0.0007   |
|    n_updates          | 259      |
|    policy_loss        | -1.85    |
|    value_loss         | 3.07     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 33792    |
| train/                |          |
|    entropy_loss       | -1.98    |
|    explained_variance | 0.77     |
|    learning_rate      | 0.0007   |
|    n_updates          | 263      |
|    policy_loss        | 0.942    |
|    value_loss         | 2.59     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 34304    |
| train/                |          |
|    entropy_loss       | -1.96    |
|    explained_variance | 0.752    |
|    learning_rate      | 0.0007   |
|    n_updates          | 267      |
|    policy_loss        | -0.461   |
|    value_loss         | 1.8      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 34816    |
| train/                |          |
|    entropy_loss       | -1.95    |
|    explained_variance | 0.742    |
|    learning_rate      | 0.0007   |
|    n_updates          | 271      |
|    policy_loss        | -0.223   |
|    value_loss         | 2.06     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 35328    |
| train/                |          |
|    entropy_loss       | -2       |
|    explained_variance | 0.727    |
|    learning_rate      | 0.0007   |
|    n_updates          | 275      |
|    policy_loss        | 2.56     |
|    value_loss         | 3.43     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 35840    |
| train/                |          |
|    entropy_loss       | -1.86    |
|    explained_variance | 0.706    |
|    learning_rate      | 0.0007   |
|    n_updates          | 279      |
|    policy_loss        | 1.48     |
|    value_loss         | 2.69     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 36352    |
| train/                |          |
|    entropy_loss       | -1.77    |
|    explained_variance | 0.691    |
|    learning_rate      | 0.0007   |
|    n_updates          | 283      |
|    policy_loss        | -2.02    |
|    value_loss         | 3.87     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 36864    |
| train/                |          |
|    entropy_loss       | -1.77    |
|    explained_variance | 0.733    |
|    learning_rate      | 0.0007   |
|    n_updates          | 287      |
|    policy_loss        | -0.127   |
|    value_loss         | 2.76     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 37376    |
| train/                |          |
|    entropy_loss       | -1.75    |
|    explained_variance | 0.702    |
|    learning_rate      | 0.0007   |
|    n_updates          | 291      |
|    policy_loss        | 1.13     |
|    value_loss         | 3.86     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 37888    |
| train/                |          |
|    entropy_loss       | -1.74    |
|    explained_variance | 0.787    |
|    learning_rate      | 0.0007   |
|    n_updates          | 295      |
|    policy_loss        | 0.942    |
|    value_loss         | 1.55     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -1.75    |
|    explained_variance | 0.82     |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | -0.41    |
|    value_loss         | 1.57     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | -8.69    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 300      |
|    time_elapsed    | 96571    |
|    total_timesteps | 38400    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 38912    |
| train/                |          |
|    entropy_loss       | -1.75    |
|    explained_variance | 0.634    |
|    learning_rate      | 0.0007   |
|    n_updates          | 303      |
|    policy_loss        | 0.195    |
|    value_loss         | 2.74     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 39424    |
| train/                |          |
|    entropy_loss       | -1.73    |
|    explained_variance | 0.776    |
|    learning_rate      | 0.0007   |
|    n_updates          | 307      |
|    policy_loss        | -0.911   |
|    value_loss         | 2.64     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 39936    |
| train/                |          |
|    entropy_loss       | -1.78    |
|    explained_variance | 0.199    |
|    learning_rate      | 0.0007   |
|    n_updates          | 311      |
|    policy_loss        | -2.67    |
|    value_loss         | 6.61     |
------------------------------------
