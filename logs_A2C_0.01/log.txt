Logging to ./logs_A2C_0.01
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 44.4     |
|    mean_reward        | -372     |
| time/                 |          |
|    total_timesteps    | 512      |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 0.00012  |
|    learning_rate      | 0.0007   |
|    n_updates          | 3        |
|    policy_loss        | -749     |
|    value_loss         | 7.9e+03  |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 43.2      |
|    mean_reward        | -203      |
| time/                 |           |
|    total_timesteps    | 1024      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000147 |
|    learning_rate      | 0.0007    |
|    n_updates          | 7         |
|    policy_loss        | -1.05e+03 |
|    value_loss         | 1.45e+04  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -216     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 3.94e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 11       |
|    policy_loss        | -752     |
|    value_loss         | 8.72e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 2048      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -0.000118 |
|    learning_rate      | 0.0007    |
|    n_updates          | 15        |
|    policy_loss        | -708      |
|    value_loss         | 6.49e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -244      |
| time/                 |           |
|    total_timesteps    | 2560      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -0.000105 |
|    learning_rate      | 0.0007    |
|    n_updates          | 19        |
|    policy_loss        | -779      |
|    value_loss         | 8.2e+03   |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -241      |
| time/                 |           |
|    total_timesteps    | 3072      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -6.43e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 23        |
|    policy_loss        | -650      |
|    value_loss         | 7.1e+03   |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.4       |
|    mean_reward        | -9.14     |
| time/                 |           |
|    total_timesteps    | 3584      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -7.51e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 27        |
|    policy_loss        | -699      |
|    value_loss         | 7.46e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 323       |
|    mean_reward        | -38.2     |
| time/                 |           |
|    total_timesteps    | 4096      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -7.75e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 31        |
|    policy_loss        | -755      |
|    value_loss         | 8.47e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 4608      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -4.79e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 35        |
|    policy_loss        | -653      |
|    value_loss         | 6.76e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 5120      |
| train/                |           |
|    entropy_loss       | -9.97     |
|    explained_variance | -5.85e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 39        |
|    policy_loss        | -545      |
|    value_loss         | 4.8e+03   |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 5632      |
| train/                |           |
|    entropy_loss       | -9.88     |
|    explained_variance | -5.98e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 43        |
|    policy_loss        | -638      |
|    value_loss         | 6.58e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 6144      |
| train/                |           |
|    entropy_loss       | -9.9      |
|    explained_variance | -5.96e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 47        |
|    policy_loss        | -680      |
|    value_loss         | 6.5e+03   |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 6656      |
| train/                |           |
|    entropy_loss       | -9.73     |
|    explained_variance | -4.29e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 51        |
|    policy_loss        | -476      |
|    value_loss         | 3.98e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7168      |
| train/                |           |
|    entropy_loss       | -9.71     |
|    explained_variance | -3.81e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 55        |
|    policy_loss        | -513      |
|    value_loss         | 4.95e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7680      |
| train/                |           |
|    entropy_loss       | -9.59     |
|    explained_variance | -5.95e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 59        |
|    policy_loss        | -270      |
|    value_loss         | 1.79e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 8192     |
| train/                |          |
|    entropy_loss       | -9.27    |
|    explained_variance | -6.4e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 63       |
|    policy_loss        | -220     |
|    value_loss         | 1.6e+03  |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    entropy_loss       | -8.67     |
|    explained_variance | -0.000102 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | -171      |
|    value_loss         | 1.04e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 9216      |
| train/                |           |
|    entropy_loss       | -7.53     |
|    explained_variance | -0.000277 |
|    learning_rate      | 0.0007    |
|    n_updates          | 71        |
|    policy_loss        | 11.6      |
|    value_loss         | 90.5      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -183      |
| time/                 |           |
|    total_timesteps    | 9728      |
| train/                |           |
|    entropy_loss       | -6.12     |
|    explained_variance | -0.000243 |
|    learning_rate      | 0.0007    |
|    n_updates          | 75        |
|    policy_loss        | -13.1     |
|    value_loss         | 146       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -184      |
| time/                 |           |
|    total_timesteps    | 10240     |
| train/                |           |
|    entropy_loss       | -5.59     |
|    explained_variance | -0.000133 |
|    learning_rate      | 0.0007    |
|    n_updates          | 79        |
|    policy_loss        | 4.28      |
|    value_loss         | 111       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 10752    |
| train/                |          |
|    entropy_loss       | -5.5     |
|    explained_variance | 8.29e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 83       |
|    policy_loss        | 5.05     |
|    value_loss         | 106      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11264    |
| train/                |          |
|    entropy_loss       | -5       |
|    explained_variance | 0.000231 |
|    learning_rate      | 0.0007   |
|    n_updates          | 87       |
|    policy_loss        | -2.65    |
|    value_loss         | 110      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11776    |
| train/                |          |
|    entropy_loss       | -4.47    |
|    explained_variance | 0.00161  |
|    learning_rate      | 0.0007   |
|    n_updates          | 91       |
|    policy_loss        | 23.7     |
|    value_loss         | 67.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 12288    |
| train/                |          |
|    entropy_loss       | -4.35    |
|    explained_variance | 0.00162  |
|    learning_rate      | 0.0007   |
|    n_updates          | 95       |
|    policy_loss        | 14.4     |
|    value_loss         | 69       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -4.2     |
|    explained_variance | 0.00291  |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | 5.35     |
|    value_loss         | 60.7     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.12     |
|    ep_rew_mean     | -14.3    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 46173    |
|    total_timesteps | 12800    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13312    |
| train/                |          |
|    entropy_loss       | -4.17    |
|    explained_variance | 0.0163   |
|    learning_rate      | 0.0007   |
|    n_updates          | 103      |
|    policy_loss        | 2.26     |
|    value_loss         | 96.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13824    |
| train/                |          |
|    entropy_loss       | -4.24    |
|    explained_variance | 0.0615   |
|    learning_rate      | 0.0007   |
|    n_updates          | 107      |
|    policy_loss        | 9.41     |
|    value_loss         | 48.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14336    |
| train/                |          |
|    entropy_loss       | -4.34    |
|    explained_variance | 0.258    |
|    learning_rate      | 0.0007   |
|    n_updates          | 111      |
|    policy_loss        | 8.29     |
|    value_loss         | 27       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14848    |
| train/                |          |
|    entropy_loss       | -4.19    |
|    explained_variance | 0.313    |
|    learning_rate      | 0.0007   |
|    n_updates          | 115      |
|    policy_loss        | -0.708   |
|    value_loss         | 34       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15360    |
| train/                |          |
|    entropy_loss       | -4.14    |
|    explained_variance | 0.427    |
|    learning_rate      | 0.0007   |
|    n_updates          | 119      |
|    policy_loss        | 5.81     |
|    value_loss         | 22.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15872    |
| train/                |          |
|    entropy_loss       | -4.11    |
|    explained_variance | 0.38     |
|    learning_rate      | 0.0007   |
|    n_updates          | 123      |
|    policy_loss        | -8.01    |
|    value_loss         | 42.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16384    |
| train/                |          |
|    entropy_loss       | -4.15    |
|    explained_variance | 0.474    |
|    learning_rate      | 0.0007   |
|    n_updates          | 127      |
|    policy_loss        | -2.81    |
|    value_loss         | 18       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16896    |
| train/                |          |
|    entropy_loss       | -3.99    |
|    explained_variance | 0.418    |
|    learning_rate      | 0.0007   |
|    n_updates          | 131      |
|    policy_loss        | 0.597    |
|    value_loss         | 17.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17408    |
| train/                |          |
|    entropy_loss       | -3.99    |
|    explained_variance | 0.488    |
|    learning_rate      | 0.0007   |
|    n_updates          | 135      |
|    policy_loss        | 3.09     |
|    value_loss         | 14.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17920    |
| train/                |          |
|    entropy_loss       | -4.15    |
|    explained_variance | 0.638    |
|    learning_rate      | 0.0007   |
|    n_updates          | 139      |
|    policy_loss        | 0.75     |
|    value_loss         | 6.66     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18432    |
| train/                |          |
|    entropy_loss       | -4.16    |
|    explained_variance | 0.489    |
|    learning_rate      | 0.0007   |
|    n_updates          | 143      |
|    policy_loss        | -4.97    |
|    value_loss         | 15.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    entropy_loss       | -4.08    |
|    explained_variance | 0.423    |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | -1.1     |
|    value_loss         | 15.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    entropy_loss       | -4       |
|    explained_variance | 0.41     |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | -2.51    |
|    value_loss         | 15.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19968    |
| train/                |          |
|    entropy_loss       | -3.78    |
|    explained_variance | 0.669    |
|    learning_rate      | 0.0007   |
|    n_updates          | 155      |
|    policy_loss        | 0.448    |
|    value_loss         | 5.18     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20480    |
| train/                |          |
|    entropy_loss       | -3.79    |
|    explained_variance | 0.668    |
|    learning_rate      | 0.0007   |
|    n_updates          | 159      |
|    policy_loss        | -0.783   |
|    value_loss         | 3.36     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 83.6     |
|    mean_reward        | -12.1    |
| time/                 |          |
|    total_timesteps    | 20992    |
| train/                |          |
|    entropy_loss       | -3.77    |
|    explained_variance | 0.498    |
|    learning_rate      | 0.0007   |
|    n_updates          | 163      |
|    policy_loss        | -3.53    |
|    value_loss         | 11.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -47.8    |
| time/                 |          |
|    total_timesteps    | 21504    |
| train/                |          |
|    entropy_loss       | -3.55    |
|    explained_variance | 0.711    |
|    learning_rate      | 0.0007   |
|    n_updates          | 167      |
|    policy_loss        | -0.263   |
|    value_loss         | 4.39     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -34.2    |
| time/                 |          |
|    total_timesteps    | 22016    |
| train/                |          |
|    entropy_loss       | -3.48    |
|    explained_variance | 0.543    |
|    learning_rate      | 0.0007   |
|    n_updates          | 171      |
|    policy_loss        | 0.751    |
|    value_loss         | 6.66     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 46.6     |
|    mean_reward        | -10.4    |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    entropy_loss       | -3.38    |
|    explained_variance | 0.798    |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | 0.0175   |
|    value_loss         | 1.76     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 84.8     |
|    mean_reward        | -12      |
| time/                 |          |
|    total_timesteps    | 23040    |
| train/                |          |
|    entropy_loss       | -3.13    |
|    explained_variance | 0.571    |
|    learning_rate      | 0.0007   |
|    n_updates          | 179      |
|    policy_loss        | 1.58     |
|    value_loss         | 4.95     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -44.9    |
| time/                 |          |
|    total_timesteps    | 23552    |
| train/                |          |
|    entropy_loss       | -3.09    |
|    explained_variance | 0.722    |
|    learning_rate      | 0.0007   |
|    n_updates          | 183      |
|    policy_loss        | 2.38     |
|    value_loss         | 2.88     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 123      |
|    mean_reward        | -26.6    |
| time/                 |          |
|    total_timesteps    | 24064    |
| train/                |          |
|    entropy_loss       | -3.03    |
|    explained_variance | 0.6      |
|    learning_rate      | 0.0007   |
|    n_updates          | 187      |
|    policy_loss        | -1.89    |
|    value_loss         | 5.38     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 123      |
|    mean_reward        | -18.4    |
| time/                 |          |
|    total_timesteps    | 24576    |
| train/                |          |
|    entropy_loss       | -3.12    |
|    explained_variance | 0.557    |
|    learning_rate      | 0.0007   |
|    n_updates          | 191      |
|    policy_loss        | -1.75    |
|    value_loss         | 6.9      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -34.5    |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    entropy_loss       | -2.72    |
|    explained_variance | 0.705    |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | -0.904   |
|    value_loss         | 2.84     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 122      |
|    mean_reward        | -11.9    |
| time/                 |          |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -2.91    |
|    explained_variance | 0.741    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | -0.769   |
|    value_loss         | 2.85     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.86     |
|    ep_rew_mean     | -7.37    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 83071    |
|    total_timesteps | 25600    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.6      |
|    mean_reward        | -4.61    |
| time/                 |          |
|    total_timesteps    | 26112    |
| train/                |          |
|    entropy_loss       | -2.84    |
|    explained_variance | 0.688    |
|    learning_rate      | 0.0007   |
|    n_updates          | 203      |
|    policy_loss        | -0.561   |
|    value_loss         | 2.72     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 83.6     |
|    mean_reward        | -9.06    |
| time/                 |          |
|    total_timesteps    | 26624    |
| train/                |          |
|    entropy_loss       | -2.79    |
|    explained_variance | 0.822    |
|    learning_rate      | 0.0007   |
|    n_updates          | 207      |
|    policy_loss        | -0.239   |
|    value_loss         | 1.47     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 122      |
|    mean_reward        | -14.5    |
| time/                 |          |
|    total_timesteps    | 27136    |
| train/                |          |
|    entropy_loss       | -2.57    |
|    explained_variance | 0.818    |
|    learning_rate      | 0.0007   |
|    n_updates          | 211      |
|    policy_loss        | 0.206    |
|    value_loss         | 1.34     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 46       |
|    mean_reward        | -10.1    |
| time/                 |          |
|    total_timesteps    | 27648    |
| train/                |          |
|    entropy_loss       | -2.47    |
|    explained_variance | 0.738    |
|    learning_rate      | 0.0007   |
|    n_updates          | 215      |
|    policy_loss        | 0.763    |
|    value_loss         | 1.55     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 45       |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    entropy_loss       | -2.41    |
|    explained_variance | 0.639    |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | -1.54    |
|    value_loss         | 4.16     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6.6      |
|    mean_reward        | -4.27    |
| time/                 |          |
|    total_timesteps    | 28672    |
| train/                |          |
|    entropy_loss       | -2.57    |
|    explained_variance | 0.788    |
|    learning_rate      | 0.0007   |
|    n_updates          | 223      |
|    policy_loss        | 0.373    |
|    value_loss         | 2.25     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6.6      |
|    mean_reward        | -4.83    |
| time/                 |          |
|    total_timesteps    | 29184    |
| train/                |          |
|    entropy_loss       | -2.55    |
|    explained_variance | 0.809    |
|    learning_rate      | 0.0007   |
|    n_updates          | 227      |
|    policy_loss        | 0.139    |
|    value_loss         | 1.66     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 125      |
|    mean_reward        | -21.1    |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    entropy_loss       | -2.5     |
|    explained_variance | 0.843    |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | -1.18    |
|    value_loss         | 1.25     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 83.6     |
|    mean_reward        | -8.78    |
| time/                 |          |
|    total_timesteps    | 30208    |
| train/                |          |
|    entropy_loss       | -2.43    |
|    explained_variance | 0.827    |
|    learning_rate      | 0.0007   |
|    n_updates          | 235      |
|    policy_loss        | 1.96     |
|    value_loss         | 1.63     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 44.6     |
|    mean_reward        | -6.93    |
| time/                 |          |
|    total_timesteps    | 30720    |
| train/                |          |
|    entropy_loss       | -2.54    |
|    explained_variance | 0.828    |
|    learning_rate      | 0.0007   |
|    n_updates          | 239      |
|    policy_loss        | 0.354    |
|    value_loss         | 1.58     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 46       |
|    mean_reward        | -7.6     |
| time/                 |          |
|    total_timesteps    | 31232    |
| train/                |          |
|    entropy_loss       | -2.49    |
|    explained_variance | 0.652    |
|    learning_rate      | 0.0007   |
|    n_updates          | 243      |
|    policy_loss        | -2.25    |
|    value_loss         | 3.01     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 47       |
|    mean_reward        | -6.31    |
| time/                 |          |
|    total_timesteps    | 31744    |
| train/                |          |
|    entropy_loss       | -2.46    |
|    explained_variance | 0.909    |
|    learning_rate      | 0.0007   |
|    n_updates          | 247      |
|    policy_loss        | -0.116   |
|    value_loss         | 0.549    |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 44.6     |
|    mean_reward        | -7.3     |
| time/                 |          |
|    total_timesteps    | 32256    |
| train/                |          |
|    entropy_loss       | -2.34    |
|    explained_variance | 0.602    |
|    learning_rate      | 0.0007   |
|    n_updates          | 251      |
|    policy_loss        | -1.6     |
|    value_loss         | 3.15     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 161      |
|    mean_reward        | -16.4    |
| time/                 |          |
|    total_timesteps    | 32768    |
| train/                |          |
|    entropy_loss       | -2.28    |
|    explained_variance | 0.811    |
|    learning_rate      | 0.0007   |
|    n_updates          | 255      |
|    policy_loss        | -0.986   |
|    value_loss         | 1.25     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 45.6     |
|    mean_reward        | -7.48    |
| time/                 |          |
|    total_timesteps    | 33280    |
| train/                |          |
|    entropy_loss       | -2.17    |
|    explained_variance | 0.861    |
|    learning_rate      | 0.0007   |
|    n_updates          | 259      |
|    policy_loss        | 2.79     |
|    value_loss         | 2.56     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.4      |
|    mean_reward        | -4.96    |
| time/                 |          |
|    total_timesteps    | 33792    |
| train/                |          |
|    entropy_loss       | -2.19    |
|    explained_variance | 0.725    |
|    learning_rate      | 0.0007   |
|    n_updates          | 263      |
|    policy_loss        | -1.21    |
|    value_loss         | 1.9      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 125      |
|    mean_reward        | -37      |
| time/                 |          |
|    total_timesteps    | 34304    |
| train/                |          |
|    entropy_loss       | -2.12    |
|    explained_variance | 0.863    |
|    learning_rate      | 0.0007   |
|    n_updates          | 267      |
|    policy_loss        | -0.528   |
|    value_loss         | 0.902    |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 123      |
|    mean_reward        | -14.1    |
| time/                 |          |
|    total_timesteps    | 34816    |
| train/                |          |
|    entropy_loss       | -1.94    |
|    explained_variance | 0.669    |
|    learning_rate      | 0.0007   |
|    n_updates          | 271      |
|    policy_loss        | -0.654   |
|    value_loss         | 1.85     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 123      |
|    mean_reward        | -17.4    |
| time/                 |          |
|    total_timesteps    | 35328    |
| train/                |          |
|    entropy_loss       | -2       |
|    explained_variance | 0.814    |
|    learning_rate      | 0.0007   |
|    n_updates          | 275      |
|    policy_loss        | 1.09     |
|    value_loss         | 1.25     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.4      |
|    mean_reward        | -4.32    |
| time/                 |          |
|    total_timesteps    | 35840    |
| train/                |          |
|    entropy_loss       | -2.03    |
|    explained_variance | 0.874    |
|    learning_rate      | 0.0007   |
|    n_updates          | 279      |
|    policy_loss        | -1.25    |
|    value_loss         | 0.793    |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 47.8     |
|    mean_reward        | -7.68    |
| time/                 |          |
|    total_timesteps    | 36352    |
| train/                |          |
|    entropy_loss       | -1.94    |
|    explained_variance | 0.621    |
|    learning_rate      | 0.0007   |
|    n_updates          | 283      |
|    policy_loss        | -1.12    |
|    value_loss         | 2.41     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 83       |
|    mean_reward        | -8.52    |
| time/                 |          |
|    total_timesteps    | 36864    |
| train/                |          |
|    entropy_loss       | -1.95    |
|    explained_variance | 0.806    |
|    learning_rate      | 0.0007   |
|    n_updates          | 287      |
|    policy_loss        | 0.0878   |
|    value_loss         | 1.48     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6        |
|    mean_reward        | -4.5     |
| time/                 |          |
|    total_timesteps    | 37376    |
| train/                |          |
|    entropy_loss       | -1.86    |
|    explained_variance | 0.918    |
|    learning_rate      | 0.0007   |
|    n_updates          | 291      |
|    policy_loss        | -0.46    |
|    value_loss         | 0.385    |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 123      |
|    mean_reward        | -18.6    |
| time/                 |          |
|    total_timesteps    | 37888    |
| train/                |          |
|    entropy_loss       | -1.66    |
|    explained_variance | 0.395    |
|    learning_rate      | 0.0007   |
|    n_updates          | 295      |
|    policy_loss        | -0.81    |
|    value_loss         | 3.55     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6        |
|    mean_reward        | -4.48    |
| time/                 |          |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -1.72    |
|    explained_variance | 0.755    |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | -0.398   |
|    value_loss         | 1.16     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.6     |
|    ep_rew_mean     | -6.61    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 300      |
|    time_elapsed    | 102430   |
|    total_timesteps | 38400    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.6      |
|    mean_reward        | -5.24    |
| time/                 |          |
|    total_timesteps    | 38912    |
| train/                |          |
|    entropy_loss       | -1.86    |
|    explained_variance | 0.792    |
|    learning_rate      | 0.0007   |
|    n_updates          | 303      |
|    policy_loss        | -1.97    |
|    value_loss         | 2        |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 45       |
|    mean_reward        | -8.3     |
| time/                 |          |
|    total_timesteps    | 39424    |
| train/                |          |
|    entropy_loss       | -1.83    |
|    explained_variance | 0.796    |
|    learning_rate      | 0.0007   |
|    n_updates          | 307      |
|    policy_loss        | -2.82    |
|    value_loss         | 2.52     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 12.2     |
|    mean_reward        | -5.16    |
| time/                 |          |
|    total_timesteps    | 39936    |
| train/                |          |
|    entropy_loss       | -1.6     |
|    explained_variance | 0.641    |
|    learning_rate      | 0.0007   |
|    n_updates          | 311      |
|    policy_loss        | -1.06    |
|    value_loss         | 1.76     |
------------------------------------
