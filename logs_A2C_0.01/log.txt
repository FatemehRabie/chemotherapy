Logging to ./logs_A2C_0.01/
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 87.8     |
|    mean_reward        | -369     |
| time/                 |          |
|    total_timesteps    | 512      |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | -0.00144 |
|    learning_rate      | 0.0007   |
|    n_updates          | 3        |
|    policy_loss        | -645     |
|    value_loss         | 6.04e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -471      |
| time/                 |           |
|    total_timesteps    | 1024      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000177 |
|    learning_rate      | 0.0007    |
|    n_updates          | 7         |
|    policy_loss        | -960      |
|    value_loss         | 1.27e+04  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 103       |
|    mean_reward        | -242      |
| time/                 |           |
|    total_timesteps    | 1536      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -9.66e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 11        |
|    policy_loss        | -996      |
|    value_loss         | 1.27e+04  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 114       |
|    mean_reward        | -331      |
| time/                 |           |
|    total_timesteps    | 2048      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000195 |
|    learning_rate      | 0.0007    |
|    n_updates          | 15        |
|    policy_loss        | -844      |
|    value_loss         | 9.36e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -452      |
| time/                 |           |
|    total_timesteps    | 2560      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000151 |
|    learning_rate      | 0.0007    |
|    n_updates          | 19        |
|    policy_loss        | -816      |
|    value_loss         | 9.12e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -489     |
| time/                 |          |
|    total_timesteps    | 3072     |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 0.000104 |
|    learning_rate      | 0.0007   |
|    n_updates          | 23       |
|    policy_loss        | -844     |
|    value_loss         | 9.73e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -462      |
| time/                 |           |
|    total_timesteps    | 3584      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -2.71e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 27        |
|    policy_loss        | -991      |
|    value_loss         | 1.24e+04  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -487      |
| time/                 |           |
|    total_timesteps    | 4096      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -1.23e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 31        |
|    policy_loss        | -757      |
|    value_loss         | 8.55e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 131      |
|    mean_reward        | -351     |
| time/                 |          |
|    total_timesteps    | 4608     |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 1.7e-05  |
|    learning_rate      | 0.0007   |
|    n_updates          | 35       |
|    policy_loss        | -869     |
|    value_loss         | 1.02e+04 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 86.6      |
|    mean_reward        | -253      |
| time/                 |           |
|    total_timesteps    | 5120      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -3.22e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 39        |
|    policy_loss        | -866      |
|    value_loss         | 9.98e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -414     |
| time/                 |          |
|    total_timesteps    | 5632     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 8.29e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 43       |
|    policy_loss        | -802     |
|    value_loss         | 9.57e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 79.2      |
|    mean_reward        | -332      |
| time/                 |           |
|    total_timesteps    | 6144      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -0.000116 |
|    learning_rate      | 0.0007    |
|    n_updates          | 47        |
|    policy_loss        | -682      |
|    value_loss         | 6.64e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -203     |
| time/                 |          |
|    total_timesteps    | 6656     |
| train/                |          |
|    entropy_loss       | -9.96    |
|    explained_variance | 6.22e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 51       |
|    policy_loss        | -832     |
|    value_loss         | 9.67e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 66        |
|    mean_reward        | -277      |
| time/                 |           |
|    total_timesteps    | 7168      |
| train/                |           |
|    entropy_loss       | -9.94     |
|    explained_variance | -6.32e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 55        |
|    policy_loss        | -520      |
|    value_loss         | 4.64e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 55.8     |
|    mean_reward        | -215     |
| time/                 |          |
|    total_timesteps    | 7680     |
| train/                |          |
|    entropy_loss       | -10      |
|    explained_variance | 2.03e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 59       |
|    policy_loss        | -852     |
|    value_loss         | 9.72e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -222      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    entropy_loss       | -9.99     |
|    explained_variance | -4.15e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -582      |
|    value_loss         | 5.08e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -218      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    entropy_loss       | -9.92     |
|    explained_variance | -6.32e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | -730      |
|    value_loss         | 7.87e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 71.6      |
|    mean_reward        | -298      |
| time/                 |           |
|    total_timesteps    | 9216      |
| train/                |           |
|    entropy_loss       | -9.9      |
|    explained_variance | -2.15e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 71        |
|    policy_loss        | -753      |
|    value_loss         | 8.34e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 64.4      |
|    mean_reward        | -273      |
| time/                 |           |
|    total_timesteps    | 9728      |
| train/                |           |
|    entropy_loss       | -9.97     |
|    explained_variance | -1.63e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 75        |
|    policy_loss        | -750      |
|    value_loss         | 8.01e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 97        |
|    mean_reward        | -204      |
| time/                 |           |
|    total_timesteps    | 10240     |
| train/                |           |
|    entropy_loss       | -9.97     |
|    explained_variance | -5.17e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 79        |
|    policy_loss        | -635      |
|    value_loss         | 6.12e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 100       |
|    mean_reward        | -192      |
| time/                 |           |
|    total_timesteps    | 10752     |
| train/                |           |
|    entropy_loss       | -9.99     |
|    explained_variance | -7.87e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 83        |
|    policy_loss        | -745      |
|    value_loss         | 8.56e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 110      |
|    mean_reward        | -298     |
| time/                 |          |
|    total_timesteps    | 11264    |
| train/                |          |
|    entropy_loss       | -9.9     |
|    explained_variance | 9.24e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 87       |
|    policy_loss        | -789     |
|    value_loss         | 8.44e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -191      |
| time/                 |           |
|    total_timesteps    | 11776     |
| train/                |           |
|    entropy_loss       | -9.84     |
|    explained_variance | -3.34e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 91        |
|    policy_loss        | -557      |
|    value_loss         | 4.74e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -195     |
| time/                 |          |
|    total_timesteps    | 12288    |
| train/                |          |
|    entropy_loss       | -9.88    |
|    explained_variance | 1.82e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 95       |
|    policy_loss        | -702     |
|    value_loss         | 6.71e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -223      |
| time/                 |           |
|    total_timesteps    | 12800     |
| train/                |           |
|    entropy_loss       | -9.88     |
|    explained_variance | -4.29e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 99        |
|    policy_loss        | -685      |
|    value_loss         | 6.49e+03  |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 82.3     |
|    ep_rew_mean     | -425     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 16555    |
|    total_timesteps | 12800    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 202      |
|    mean_reward        | -194     |
| time/                 |          |
|    total_timesteps    | 13312    |
| train/                |          |
|    entropy_loss       | -9.85    |
|    explained_variance | 5.6e-06  |
|    learning_rate      | 0.0007   |
|    n_updates          | 103      |
|    policy_loss        | -646     |
|    value_loss         | 6.73e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -205      |
| time/                 |           |
|    total_timesteps    | 13824     |
| train/                |           |
|    entropy_loss       | -9.83     |
|    explained_variance | -1.03e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 107       |
|    policy_loss        | -560      |
|    value_loss         | 5.25e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -206     |
| time/                 |          |
|    total_timesteps    | 14336    |
| train/                |          |
|    entropy_loss       | -9.71    |
|    explained_variance | 2.15e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 111      |
|    policy_loss        | -467     |
|    value_loss         | 3.36e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -228     |
| time/                 |          |
|    total_timesteps    | 14848    |
| train/                |          |
|    entropy_loss       | -9.64    |
|    explained_variance | 2.98e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 115      |
|    policy_loss        | -549     |
|    value_loss         | 4.97e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -221      |
| time/                 |           |
|    total_timesteps    | 15360     |
| train/                |           |
|    entropy_loss       | -9.63     |
|    explained_variance | -1.19e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 119       |
|    policy_loss        | -426      |
|    value_loss         | 3.03e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -190     |
| time/                 |          |
|    total_timesteps    | 15872    |
| train/                |          |
|    entropy_loss       | -9.42    |
|    explained_variance | -2e-05   |
|    learning_rate      | 0.0007   |
|    n_updates          | 123      |
|    policy_loss        | -550     |
|    value_loss         | 4.59e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -213     |
| time/                 |          |
|    total_timesteps    | 16384    |
| train/                |          |
|    entropy_loss       | -9.36    |
|    explained_variance | 4.77e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 127      |
|    policy_loss        | -479     |
|    value_loss         | 3.56e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 200       |
|    mean_reward        | -193      |
| time/                 |           |
|    total_timesteps    | 16896     |
| train/                |           |
|    entropy_loss       | -9.34     |
|    explained_variance | -5.25e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 131       |
|    policy_loss        | -360      |
|    value_loss         | 2.04e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 201       |
|    mean_reward        | -193      |
| time/                 |           |
|    total_timesteps    | 17408     |
| train/                |           |
|    entropy_loss       | -9.25     |
|    explained_variance | -1.19e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 135       |
|    policy_loss        | -315      |
|    value_loss         | 1.97e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -210      |
| time/                 |           |
|    total_timesteps    | 17920     |
| train/                |           |
|    entropy_loss       | -9.28     |
|    explained_variance | -8.58e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 139       |
|    policy_loss        | -286      |
|    value_loss         | 1.52e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -223     |
| time/                 |          |
|    total_timesteps    | 18432    |
| train/                |          |
|    entropy_loss       | -9.25    |
|    explained_variance | 1.96e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 143      |
|    policy_loss        | -258     |
|    value_loss         | 1.31e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -201     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    entropy_loss       | -9.05    |
|    explained_variance | 6.79e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | -275     |
|    value_loss         | 1.41e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -203     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    entropy_loss       | -9.15    |
|    explained_variance | 1.53e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | -314     |
|    value_loss         | 1.74e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -193     |
| time/                 |          |
|    total_timesteps    | 19968    |
| train/                |          |
|    entropy_loss       | -9.11    |
|    explained_variance | 1.37e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 155      |
|    policy_loss        | -347     |
|    value_loss         | 2.01e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -211      |
| time/                 |           |
|    total_timesteps    | 20480     |
| train/                |           |
|    entropy_loss       | -9.06     |
|    explained_variance | -2.74e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 159       |
|    policy_loss        | -351      |
|    value_loss         | 2.02e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -191     |
| time/                 |          |
|    total_timesteps    | 20992    |
| train/                |          |
|    entropy_loss       | -9.01    |
|    explained_variance | 1.14e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 163      |
|    policy_loss        | -263     |
|    value_loss         | 1.26e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -197      |
| time/                 |           |
|    total_timesteps    | 21504     |
| train/                |           |
|    entropy_loss       | -8.88     |
|    explained_variance | -5.01e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 167       |
|    policy_loss        | -366      |
|    value_loss         | 2.26e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -232      |
| time/                 |           |
|    total_timesteps    | 22016     |
| train/                |           |
|    entropy_loss       | -8.82     |
|    explained_variance | -2.77e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 171       |
|    policy_loss        | -313      |
|    value_loss         | 1.79e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -205      |
| time/                 |           |
|    total_timesteps    | 22528     |
| train/                |           |
|    entropy_loss       | -8.67     |
|    explained_variance | -7.15e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 175       |
|    policy_loss        | -188      |
|    value_loss         | 837       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -207      |
| time/                 |           |
|    total_timesteps    | 23040     |
| train/                |           |
|    entropy_loss       | -8.54     |
|    explained_variance | -1.78e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 179       |
|    policy_loss        | -217      |
|    value_loss         | 934       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -219      |
| time/                 |           |
|    total_timesteps    | 23552     |
| train/                |           |
|    entropy_loss       | -8.46     |
|    explained_variance | -1.47e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 183       |
|    policy_loss        | -176      |
|    value_loss         | 868       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -217      |
| time/                 |           |
|    total_timesteps    | 24064     |
| train/                |           |
|    entropy_loss       | -8.39     |
|    explained_variance | -9.78e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 187       |
|    policy_loss        | -201      |
|    value_loss         | 928       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -206      |
| time/                 |           |
|    total_timesteps    | 24576     |
| train/                |           |
|    entropy_loss       | -8.35     |
|    explained_variance | -9.42e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 191       |
|    policy_loss        | -264      |
|    value_loss         | 1.57e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -193      |
| time/                 |           |
|    total_timesteps    | 25088     |
| train/                |           |
|    entropy_loss       | -8.04     |
|    explained_variance | -3.28e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 195       |
|    policy_loss        | -177      |
|    value_loss         | 879       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -192      |
| time/                 |           |
|    total_timesteps    | 25600     |
| train/                |           |
|    entropy_loss       | -7.9      |
|    explained_variance | -2.26e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 199       |
|    policy_loss        | -174      |
|    value_loss         | 631       |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 130      |
|    ep_rew_mean     | -365     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 31503    |
|    total_timesteps | 25600    |
---------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -188      |
| time/                 |           |
|    total_timesteps    | 26112     |
| train/                |           |
|    entropy_loss       | -7.85     |
|    explained_variance | -1.55e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 203       |
|    policy_loss        | -165      |
|    value_loss         | 614       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -200      |
| time/                 |           |
|    total_timesteps    | 26624     |
| train/                |           |
|    entropy_loss       | -7.92     |
|    explained_variance | -1.59e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 207       |
|    policy_loss        | -128      |
|    value_loss         | 364       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -203      |
| time/                 |           |
|    total_timesteps    | 27136     |
| train/                |           |
|    entropy_loss       | -7.87     |
|    explained_variance | -3.76e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 211       |
|    policy_loss        | -152      |
|    value_loss         | 562       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -186      |
| time/                 |           |
|    total_timesteps    | 27648     |
| train/                |           |
|    entropy_loss       | -7.84     |
|    explained_variance | -2.29e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 215       |
|    policy_loss        | -146      |
|    value_loss         | 470       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -203      |
| time/                 |           |
|    total_timesteps    | 28160     |
| train/                |           |
|    entropy_loss       | -7.77     |
|    explained_variance | -7.51e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 219       |
|    policy_loss        | -160      |
|    value_loss         | 573       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -188      |
| time/                 |           |
|    total_timesteps    | 28672     |
| train/                |           |
|    entropy_loss       | -7.79     |
|    explained_variance | -1.34e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 223       |
|    policy_loss        | -179      |
|    value_loss         | 688       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -202      |
| time/                 |           |
|    total_timesteps    | 29184     |
| train/                |           |
|    entropy_loss       | -7.76     |
|    explained_variance | -9.18e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 227       |
|    policy_loss        | -121      |
|    value_loss         | 565       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -198     |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    entropy_loss       | -7.57    |
|    explained_variance | 9.66e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | -162     |
|    value_loss         | 561      |
------------------------------------
