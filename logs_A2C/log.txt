Logging to ./logs_A2C/
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -601     |
| time/                 |          |
|    total_timesteps    | 512      |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 9.66e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 3        |
|    policy_loss        | -711     |
|    value_loss         | 7.26e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -367      |
| time/                 |           |
|    total_timesteps    | 1024      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 7         |
|    policy_loss        | -797      |
|    value_loss         | 9.65e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -319      |
| time/                 |           |
|    total_timesteps    | 1536      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 11        |
|    policy_loss        | -489      |
|    value_loss         | 4.16e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -743      |
| time/                 |           |
|    total_timesteps    | 2048      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -1.07e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 15        |
|    policy_loss        | -784      |
|    value_loss         | 8.43e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -694     |
| time/                 |          |
|    total_timesteps    | 2560     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 19       |
|    policy_loss        | -635     |
|    value_loss         | 6.55e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 3072      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 23        |
|    policy_loss        | -886      |
|    value_loss         | 1.05e+04  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 3584      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -9.54e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 27        |
|    policy_loss        | -450      |
|    value_loss         | 3.08e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 4096      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 31        |
|    policy_loss        | -821      |
|    value_loss         | 9.27e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 4608      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 35        |
|    policy_loss        | -647      |
|    value_loss         | 6.71e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 5120     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 39       |
|    policy_loss        | -765     |
|    value_loss         | 8.42e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 5632      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 43        |
|    policy_loss        | -504      |
|    value_loss         | 4.41e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 58        |
|    mean_reward        | -184      |
| time/                 |           |
|    total_timesteps    | 6144      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 47        |
|    policy_loss        | -672      |
|    value_loss         | 6.92e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 58       |
|    mean_reward        | -184     |
| time/                 |          |
|    total_timesteps    | 6656     |
| train/                |          |
|    entropy_loss       | -10.2    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 51       |
|    policy_loss        | -651     |
|    value_loss         | 6.53e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 46        |
|    mean_reward        | -184      |
| time/                 |           |
|    total_timesteps    | 7168      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 55        |
|    policy_loss        | -582      |
|    value_loss         | 6.5e+03   |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 7680     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 59       |
|    policy_loss        | -663     |
|    value_loss         | 6.8e+03  |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -712      |
|    value_loss         | 6.85e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | -576      |
|    value_loss         | 5.39e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 9216      |
| train/                |           |
|    entropy_loss       | -10       |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 71        |
|    policy_loss        | -687      |
|    value_loss         | 7.01e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 5.6       |
|    mean_reward        | -10.4     |
| time/                 |           |
|    total_timesteps    | 9728      |
| train/                |           |
|    entropy_loss       | -9.89     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 75        |
|    policy_loss        | -648      |
|    value_loss         | 6.62e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 10240     |
| train/                |           |
|    entropy_loss       | -9.83     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 79        |
|    policy_loss        | -501      |
|    value_loss         | 4.46e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 10752    |
| train/                |          |
|    entropy_loss       | -9.67    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 83       |
|    policy_loss        | -383     |
|    value_loss         | 3.55e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 11264     |
| train/                |           |
|    entropy_loss       | -9.38     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 87        |
|    policy_loss        | -210      |
|    value_loss         | 1.12e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -146      |
| time/                 |           |
|    total_timesteps    | 11776     |
| train/                |           |
|    entropy_loss       | -9.02     |
|    explained_variance | -4.77e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 91        |
|    policy_loss        | -94       |
|    value_loss         | 505       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 12288     |
| train/                |           |
|    entropy_loss       | -8.41     |
|    explained_variance | -7.15e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 95        |
|    policy_loss        | -38.1     |
|    value_loss         | 260       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 12800     |
| train/                |           |
|    entropy_loss       | -8.29     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 99        |
|    policy_loss        | -0.7      |
|    value_loss         | 124       |
-------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 13.6     |
|    ep_rew_mean     | -41.3    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 45924    |
|    total_timesteps | 12800    |
---------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 13312     |
| train/                |           |
|    entropy_loss       | -6.47     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 103       |
|    policy_loss        | 2.97      |
|    value_loss         | 229       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 13824     |
| train/                |           |
|    entropy_loss       | -5.99     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 107       |
|    policy_loss        | 29.5      |
|    value_loss         | 102       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 14336     |
| train/                |           |
|    entropy_loss       | -5.28     |
|    explained_variance | -8.34e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 111       |
|    policy_loss        | 30.9      |
|    value_loss         | 98.7      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 14848     |
| train/                |           |
|    entropy_loss       | -5.19     |
|    explained_variance | -4.77e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 115       |
|    policy_loss        | 17.7      |
|    value_loss         | 101       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 15360     |
| train/                |           |
|    entropy_loss       | -5.08     |
|    explained_variance | -4.77e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 119       |
|    policy_loss        | 12.9      |
|    value_loss         | 95.2      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 15872     |
| train/                |           |
|    entropy_loss       | -4.94     |
|    explained_variance | -1.43e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 123       |
|    policy_loss        | 26.6      |
|    value_loss         | 89        |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 16384     |
| train/                |           |
|    entropy_loss       | -4.73     |
|    explained_variance | -9.54e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 127       |
|    policy_loss        | 32.7      |
|    value_loss         | 93.7      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 16896     |
| train/                |           |
|    entropy_loss       | -4.55     |
|    explained_variance | -9.54e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 131       |
|    policy_loss        | 25.2      |
|    value_loss         | 94.5      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 17408     |
| train/                |           |
|    entropy_loss       | -4.44     |
|    explained_variance | -1.07e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 135       |
|    policy_loss        | 17.5      |
|    value_loss         | 88.5      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 17920     |
| train/                |           |
|    entropy_loss       | -4.35     |
|    explained_variance | -1.19e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 139       |
|    policy_loss        | 14.4      |
|    value_loss         | 90.7      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 18432     |
| train/                |           |
|    entropy_loss       | -4.1      |
|    explained_variance | -9.54e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 143       |
|    policy_loss        | 22.8      |
|    value_loss         | 78.2      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    entropy_loss       | -4.02    |
|    explained_variance | -2.5e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | 18.1     |
|    value_loss         | 70.1     |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 19456     |
| train/                |           |
|    entropy_loss       | -4.09     |
|    explained_variance | -1.67e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 151       |
|    policy_loss        | 22.5      |
|    value_loss         | 65.1      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 19968     |
| train/                |           |
|    entropy_loss       | -4.14     |
|    explained_variance | -2.98e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 155       |
|    policy_loss        | 22.9      |
|    value_loss         | 66.3      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 20480     |
| train/                |           |
|    entropy_loss       | -4.07     |
|    explained_variance | -2.38e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 159       |
|    policy_loss        | 15.7      |
|    value_loss         | 67.8      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 134       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 20992     |
| train/                |           |
|    entropy_loss       | -4.04     |
|    explained_variance | -1.67e-06 |
|    learning_rate      | 0.0007    |
|    n_updates          | 163       |
|    policy_loss        | 14.5      |
|    value_loss         | 83.5      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 21504    |
| train/                |          |
|    entropy_loss       | -3.96    |
|    explained_variance | 1.49e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 167      |
|    policy_loss        | -1.59    |
|    value_loss         | 104      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 22016    |
| train/                |          |
|    entropy_loss       | -3.65    |
|    explained_variance | 8.98e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 171      |
|    policy_loss        | 4.55     |
|    value_loss         | 44.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    entropy_loss       | -3.22    |
|    explained_variance | 2.1e-05  |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | -0.162   |
|    value_loss         | 32       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 23040    |
| train/                |          |
|    entropy_loss       | -3.05    |
|    explained_variance | 5.05e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 179      |
|    policy_loss        | -0.404   |
|    value_loss         | 32.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23552    |
| train/                |          |
|    entropy_loss       | -2.98    |
|    explained_variance | 0.022    |
|    learning_rate      | 0.0007   |
|    n_updates          | 183      |
|    policy_loss        | 7.71     |
|    value_loss         | 20.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 24064    |
| train/                |          |
|    entropy_loss       | -2.92    |
|    explained_variance | 0.154    |
|    learning_rate      | 0.0007   |
|    n_updates          | 187      |
|    policy_loss        | -1.87    |
|    value_loss         | 19.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 24576    |
| train/                |          |
|    entropy_loss       | -2.78    |
|    explained_variance | 0.212    |
|    learning_rate      | 0.0007   |
|    n_updates          | 191      |
|    policy_loss        | -1.35    |
|    value_loss         | 19.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    entropy_loss       | -2.69    |
|    explained_variance | 0.252    |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | 2.6      |
|    value_loss         | 13.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -39.2    |
| time/                 |          |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -2.7     |
|    explained_variance | 0.131    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | -2.08    |
|    value_loss         | 27.4     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.49     |
|    ep_rew_mean     | -10.3    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 86143    |
|    total_timesteps | 25600    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 26112    |
| train/                |          |
|    entropy_loss       | -2.68    |
|    explained_variance | 0.183    |
|    learning_rate      | 0.0007   |
|    n_updates          | 203      |
|    policy_loss        | -4.48    |
|    value_loss         | 28.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 26624    |
| train/                |          |
|    entropy_loss       | -2.52    |
|    explained_variance | 0.252    |
|    learning_rate      | 0.0007   |
|    n_updates          | 207      |
|    policy_loss        | -0.474   |
|    value_loss         | 16.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 27136    |
| train/                |          |
|    entropy_loss       | -2.5     |
|    explained_variance | 0.346    |
|    learning_rate      | 0.0007   |
|    n_updates          | 211      |
|    policy_loss        | 1.39     |
|    value_loss         | 13.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 27648    |
| train/                |          |
|    entropy_loss       | -2.36    |
|    explained_variance | 0.36     |
|    learning_rate      | 0.0007   |
|    n_updates          | 215      |
|    policy_loss        | -1.51    |
|    value_loss         | 10.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    entropy_loss       | -2.26    |
|    explained_variance | 0.393    |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | -1.9     |
|    value_loss         | 9.71     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -159     |
| time/                 |          |
|    total_timesteps    | 28672    |
| train/                |          |
|    entropy_loss       | -2.19    |
|    explained_variance | 0.378    |
|    learning_rate      | 0.0007   |
|    n_updates          | 223      |
|    policy_loss        | -1.8     |
|    value_loss         | 11       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -78.5    |
| time/                 |          |
|    total_timesteps    | 29184    |
| train/                |          |
|    entropy_loss       | -2.27    |
|    explained_variance | 0.365    |
|    learning_rate      | 0.0007   |
|    n_updates          | 227      |
|    policy_loss        | 0.254    |
|    value_loss         | 9.71     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 108      |
|    mean_reward        | -27      |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    entropy_loss       | -2.2     |
|    explained_variance | 0.517    |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | 0.813    |
|    value_loss         | 6.38     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -28.3    |
| time/                 |          |
|    total_timesteps    | 30208    |
| train/                |          |
|    entropy_loss       | -2.14    |
|    explained_variance | 0.383    |
|    learning_rate      | 0.0007   |
|    n_updates          | 235      |
|    policy_loss        | 2.11     |
|    value_loss         | 9.59     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 109      |
|    mean_reward        | -42.1    |
| time/                 |          |
|    total_timesteps    | 30720    |
| train/                |          |
|    entropy_loss       | -2.08    |
|    explained_variance | 0.485    |
|    learning_rate      | 0.0007   |
|    n_updates          | 239      |
|    policy_loss        | 1.13     |
|    value_loss         | 5.37     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 83.2     |
|    mean_reward        | -24.7    |
| time/                 |          |
|    total_timesteps    | 31232    |
| train/                |          |
|    entropy_loss       | -2.15    |
|    explained_variance | 0.325    |
|    learning_rate      | 0.0007   |
|    n_updates          | 243      |
|    policy_loss        | -0.157   |
|    value_loss         | 8.45     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 31744    |
| train/                |          |
|    entropy_loss       | -1.98    |
|    explained_variance | 0.41     |
|    learning_rate      | 0.0007   |
|    n_updates          | 247      |
|    policy_loss        | -0.498   |
|    value_loss         | 9.04     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.4      |
|    mean_reward        | -5.3     |
| time/                 |          |
|    total_timesteps    | 32256    |
| train/                |          |
|    entropy_loss       | -2.02    |
|    explained_variance | 0.416    |
|    learning_rate      | 0.0007   |
|    n_updates          | 251      |
|    policy_loss        | -0.834   |
|    value_loss         | 12.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 108      |
|    mean_reward        | -29.6    |
| time/                 |          |
|    total_timesteps    | 32768    |
| train/                |          |
|    entropy_loss       | -1.96    |
|    explained_variance | 0.549    |
|    learning_rate      | 0.0007   |
|    n_updates          | 255      |
|    policy_loss        | 0.175    |
|    value_loss         | 6        |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -43.4    |
| time/                 |          |
|    total_timesteps    | 33280    |
| train/                |          |
|    entropy_loss       | -2.03    |
|    explained_variance | 0.489    |
|    learning_rate      | 0.0007   |
|    n_updates          | 259      |
|    policy_loss        | -1.67    |
|    value_loss         | 6.74     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 57.2     |
|    mean_reward        | -12.2    |
| time/                 |          |
|    total_timesteps    | 33792    |
| train/                |          |
|    entropy_loss       | -2       |
|    explained_variance | 0.472    |
|    learning_rate      | 0.0007   |
|    n_updates          | 263      |
|    policy_loss        | -0.651   |
|    value_loss         | 6.94     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 34304    |
| train/                |          |
|    entropy_loss       | -1.95    |
|    explained_variance | 0.538    |
|    learning_rate      | 0.0007   |
|    n_updates          | 267      |
|    policy_loss        | -0.251   |
|    value_loss         | 4.66     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -40      |
| time/                 |          |
|    total_timesteps    | 34816    |
| train/                |          |
|    entropy_loss       | -2.07    |
|    explained_variance | 0.548    |
|    learning_rate      | 0.0007   |
|    n_updates          | 271      |
|    policy_loss        | 0.3      |
|    value_loss         | 5.19     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -147     |
| time/                 |          |
|    total_timesteps    | 35328    |
| train/                |          |
|    entropy_loss       | -2.31    |
|    explained_variance | 0.231    |
|    learning_rate      | 0.0007   |
|    n_updates          | 275      |
|    policy_loss        | -5.86    |
|    value_loss         | 27.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 35840    |
| train/                |          |
|    entropy_loss       | -2.01    |
|    explained_variance | 0.428    |
|    learning_rate      | 0.0007   |
|    n_updates          | 279      |
|    policy_loss        | 3.4      |
|    value_loss         | 10.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 36352    |
| train/                |          |
|    entropy_loss       | -2.05    |
|    explained_variance | 0.54     |
|    learning_rate      | 0.0007   |
|    n_updates          | 283      |
|    policy_loss        | -0.917   |
|    value_loss         | 6.27     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 36864    |
| train/                |          |
|    entropy_loss       | -2.03    |
|    explained_variance | 0.329    |
|    learning_rate      | 0.0007   |
|    n_updates          | 287      |
|    policy_loss        | -1.82    |
|    value_loss         | 10.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 37376    |
| train/                |          |
|    entropy_loss       | -1.86    |
|    explained_variance | 0.507    |
|    learning_rate      | 0.0007   |
|    n_updates          | 291      |
|    policy_loss        | 0.139    |
|    value_loss         | 8.7      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 37888    |
| train/                |          |
|    entropy_loss       | -1.91    |
|    explained_variance | 0.503    |
|    learning_rate      | 0.0007   |
|    n_updates          | 295      |
|    policy_loss        | 1.07     |
|    value_loss         | 6.71     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -1.97    |
|    explained_variance | 0.554    |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | 0.105    |
|    value_loss         | 5.37     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.79     |
|    ep_rew_mean     | -8.5     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 300      |
|    time_elapsed    | 119743   |
|    total_timesteps | 38400    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 38912    |
| train/                |          |
|    entropy_loss       | -1.91    |
|    explained_variance | 0.405    |
|    learning_rate      | 0.0007   |
|    n_updates          | 303      |
|    policy_loss        | -5.09    |
|    value_loss         | 20.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 39424    |
| train/                |          |
|    entropy_loss       | -1.87    |
|    explained_variance | 0.569    |
|    learning_rate      | 0.0007   |
|    n_updates          | 307      |
|    policy_loss        | -1.12    |
|    value_loss         | 4.61     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -38.8    |
| time/                 |          |
|    total_timesteps    | 39936    |
| train/                |          |
|    entropy_loss       | -2       |
|    explained_variance | 0.536    |
|    learning_rate      | 0.0007   |
|    n_updates          | 311      |
|    policy_loss        | -1.01    |
|    value_loss         | 6.07     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 83       |
|    mean_reward        | -18.9    |
| time/                 |          |
|    total_timesteps    | 40448    |
| train/                |          |
|    entropy_loss       | -1.86    |
|    explained_variance | 0.494    |
|    learning_rate      | 0.0007   |
|    n_updates          | 315      |
|    policy_loss        | -0.532   |
|    value_loss         | 8.96     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.8     |
|    mean_reward        | -8.93    |
| time/                 |          |
|    total_timesteps    | 40960    |
| train/                |          |
|    entropy_loss       | -1.92    |
|    explained_variance | 0.423    |
|    learning_rate      | 0.0007   |
|    n_updates          | 319      |
|    policy_loss        | 0.234    |
|    value_loss         | 5.69     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -93.4    |
| time/                 |          |
|    total_timesteps    | 41472    |
| train/                |          |
|    entropy_loss       | -2.01    |
|    explained_variance | 0.491    |
|    learning_rate      | 0.0007   |
|    n_updates          | 323      |
|    policy_loss        | -0.743   |
|    value_loss         | 6.99     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31       |
|    mean_reward        | -8.83    |
| time/                 |          |
|    total_timesteps    | 41984    |
| train/                |          |
|    entropy_loss       | -2.01    |
|    explained_variance | 0.546    |
|    learning_rate      | 0.0007   |
|    n_updates          | 327      |
|    policy_loss        | 2.69     |
|    value_loss         | 5.6      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 134      |
|    mean_reward        | -38.3    |
| time/                 |          |
|    total_timesteps    | 42496    |
| train/                |          |
|    entropy_loss       | -1.99    |
|    explained_variance | 0.491    |
|    learning_rate      | 0.0007   |
|    n_updates          | 331      |
|    policy_loss        | -0.299   |
|    value_loss         | 4.27     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31       |
|    mean_reward        | -6.87    |
| time/                 |          |
|    total_timesteps    | 43008    |
| train/                |          |
|    entropy_loss       | -2.05    |
|    explained_variance | 0.64     |
|    learning_rate      | 0.0007   |
|    n_updates          | 335      |
|    policy_loss        | 0.391    |
|    value_loss         | 3.39     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.4     |
|    mean_reward        | -7.3     |
| time/                 |          |
|    total_timesteps    | 43520    |
| train/                |          |
|    entropy_loss       | -2.22    |
|    explained_variance | 0.626    |
|    learning_rate      | 0.0007   |
|    n_updates          | 339      |
|    policy_loss        | -1.46    |
|    value_loss         | 2.91     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.8     |
|    mean_reward        | -10.4    |
| time/                 |          |
|    total_timesteps    | 44032    |
| train/                |          |
|    entropy_loss       | -1.94    |
|    explained_variance | 0.489    |
|    learning_rate      | 0.0007   |
|    n_updates          | 343      |
|    policy_loss        | -0.282   |
|    value_loss         | 3.98     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 109      |
|    mean_reward        | -44.8    |
| time/                 |          |
|    total_timesteps    | 44544    |
| train/                |          |
|    entropy_loss       | -1.79    |
|    explained_variance | 0.603    |
|    learning_rate      | 0.0007   |
|    n_updates          | 347      |
|    policy_loss        | -1.1     |
|    value_loss         | 4.6      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 8.2      |
|    mean_reward        | -4.74    |
| time/                 |          |
|    total_timesteps    | 45056    |
| train/                |          |
|    entropy_loss       | -1.72    |
|    explained_variance | 0.551    |
|    learning_rate      | 0.0007   |
|    n_updates          | 351      |
|    policy_loss        | 1.78     |
|    value_loss         | 4.01     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.4      |
|    mean_reward        | -5.45    |
| time/                 |          |
|    total_timesteps    | 45568    |
| train/                |          |
|    entropy_loss       | -1.89    |
|    explained_variance | 0.59     |
|    learning_rate      | 0.0007   |
|    n_updates          | 355      |
|    policy_loss        | 0.0761   |
|    value_loss         | 4.35     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.2     |
|    mean_reward        | -5.95    |
| time/                 |          |
|    total_timesteps    | 46080    |
| train/                |          |
|    entropy_loss       | -1.79    |
|    explained_variance | 0.628    |
|    learning_rate      | 0.0007   |
|    n_updates          | 359      |
|    policy_loss        | 0.0746   |
|    value_loss         | 2.51     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.6     |
|    mean_reward        | -7.41    |
| time/                 |          |
|    total_timesteps    | 46592    |
| train/                |          |
|    entropy_loss       | -1.78    |
|    explained_variance | 0.559    |
|    learning_rate      | 0.0007   |
|    n_updates          | 363      |
|    policy_loss        | 0.351    |
|    value_loss         | 3.59     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 82.8     |
|    mean_reward        | -16.1    |
| time/                 |          |
|    total_timesteps    | 47104    |
| train/                |          |
|    entropy_loss       | -1.71    |
|    explained_variance | 0.687    |
|    learning_rate      | 0.0007   |
|    n_updates          | 367      |
|    policy_loss        | -1.15    |
|    value_loss         | 2.36     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 32.6     |
|    mean_reward        | -7.1     |
| time/                 |          |
|    total_timesteps    | 47616    |
| train/                |          |
|    entropy_loss       | -1.62    |
|    explained_variance | 0.646    |
|    learning_rate      | 0.0007   |
|    n_updates          | 371      |
|    policy_loss        | -1.11    |
|    value_loss         | 3.3      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.4     |
|    mean_reward        | -10.5    |
| time/                 |          |
|    total_timesteps    | 48128    |
| train/                |          |
|    entropy_loss       | -1.57    |
|    explained_variance | 0.699    |
|    learning_rate      | 0.0007   |
|    n_updates          | 375      |
|    policy_loss        | 1.39     |
|    value_loss         | 3.6      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.4      |
|    mean_reward        | -6.13    |
| time/                 |          |
|    total_timesteps    | 48640    |
| train/                |          |
|    entropy_loss       | -1.53    |
|    explained_variance | 0.5      |
|    learning_rate      | 0.0007   |
|    n_updates          | 379      |
|    policy_loss        | -0.396   |
|    value_loss         | 4.81     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.8      |
|    mean_reward        | -5.13    |
| time/                 |          |
|    total_timesteps    | 49152    |
| train/                |          |
|    entropy_loss       | -1.69    |
|    explained_variance | 0.514    |
|    learning_rate      | 0.0007   |
|    n_updates          | 383      |
|    policy_loss        | -0.932   |
|    value_loss         | 4.05     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 57       |
|    mean_reward        | -10.7    |
| time/                 |          |
|    total_timesteps    | 49664    |
| train/                |          |
|    entropy_loss       | -1.61    |
|    explained_variance | 0.643    |
|    learning_rate      | 0.0007   |
|    n_updates          | 387      |
|    policy_loss        | -0.602   |
|    value_loss         | 2.88     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.6     |
|    mean_reward        | -6.63    |
| time/                 |          |
|    total_timesteps    | 50176    |
| train/                |          |
|    entropy_loss       | -1.54    |
|    explained_variance | 0.668    |
|    learning_rate      | 0.0007   |
|    n_updates          | 391      |
|    policy_loss        | -0.219   |
|    value_loss         | 2.82     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.6      |
|    mean_reward        | -5.74    |
| time/                 |          |
|    total_timesteps    | 50688    |
| train/                |          |
|    entropy_loss       | -1.74    |
|    explained_variance | 0.494    |
|    learning_rate      | 0.0007   |
|    n_updates          | 395      |
|    policy_loss        | -2.39    |
|    value_loss         | 4.79     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.8      |
|    mean_reward        | -5.71    |
| time/                 |          |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -1.63    |
|    explained_variance | 0.646    |
|    learning_rate      | 0.0007   |
|    n_updates          | 399      |
|    policy_loss        | -0.966   |
|    value_loss         | 3.42     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.69     |
|    ep_rew_mean     | -7.03    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 400      |
|    time_elapsed    | 144307   |
|    total_timesteps | 51200    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 82.6     |
|    mean_reward        | -18.5    |
| time/                 |          |
|    total_timesteps    | 51712    |
| train/                |          |
|    entropy_loss       | -1.41    |
|    explained_variance | 0.664    |
|    learning_rate      | 0.0007   |
|    n_updates          | 403      |
|    policy_loss        | 0.0237   |
|    value_loss         | 2.04     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.4      |
|    mean_reward        | -5.76    |
| time/                 |          |
|    total_timesteps    | 52224    |
| train/                |          |
|    entropy_loss       | -1.62    |
|    explained_variance | 0.705    |
|    learning_rate      | 0.0007   |
|    n_updates          | 407      |
|    policy_loss        | -0.258   |
|    value_loss         | 2.52     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 82.8     |
|    mean_reward        | -15.5    |
| time/                 |          |
|    total_timesteps    | 52736    |
| train/                |          |
|    entropy_loss       | -1.41    |
|    explained_variance | 0.315    |
|    learning_rate      | 0.0007   |
|    n_updates          | 411      |
|    policy_loss        | -1.24    |
|    value_loss         | 4.04     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5        |
|    mean_reward        | -5.6     |
| time/                 |          |
|    total_timesteps    | 53248    |
| train/                |          |
|    entropy_loss       | -1.54    |
|    explained_variance | 0.0443   |
|    learning_rate      | 0.0007   |
|    n_updates          | 415      |
|    policy_loss        | -1.17    |
|    value_loss         | 8.03     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.4     |
|    mean_reward        | -8.9     |
| time/                 |          |
|    total_timesteps    | 53760    |
| train/                |          |
|    entropy_loss       | -1.54    |
|    explained_variance | 0.554    |
|    learning_rate      | 0.0007   |
|    n_updates          | 419      |
|    policy_loss        | 0.248    |
|    value_loss         | 2.3      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 82.8     |
|    mean_reward        | -11.2    |
| time/                 |          |
|    total_timesteps    | 54272    |
| train/                |          |
|    entropy_loss       | -1.45    |
|    explained_variance | 0.488    |
|    learning_rate      | 0.0007   |
|    n_updates          | 423      |
|    policy_loss        | -2.07    |
|    value_loss         | 4.16     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 32.6     |
|    mean_reward        | -8.89    |
| time/                 |          |
|    total_timesteps    | 54784    |
| train/                |          |
|    entropy_loss       | -1.47    |
|    explained_variance | 0.661    |
|    learning_rate      | 0.0007   |
|    n_updates          | 427      |
|    policy_loss        | 0.334    |
|    value_loss         | 2.69     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 108      |
|    mean_reward        | -19.9    |
| time/                 |          |
|    total_timesteps    | 55296    |
| train/                |          |
|    entropy_loss       | -1.49    |
|    explained_variance | 0.564    |
|    learning_rate      | 0.0007   |
|    n_updates          | 431      |
|    policy_loss        | -0.288   |
|    value_loss         | 2.88     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 82.6     |
|    mean_reward        | -11.4    |
| time/                 |          |
|    total_timesteps    | 55808    |
| train/                |          |
|    entropy_loss       | -1.21    |
|    explained_variance | 0.321    |
|    learning_rate      | 0.0007   |
|    n_updates          | 435      |
|    policy_loss        | -1.52    |
|    value_loss         | 6.39     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5        |
|    mean_reward        | -6.02    |
| time/                 |          |
|    total_timesteps    | 56320    |
| train/                |          |
|    entropy_loss       | -1.46    |
|    explained_variance | 0.645    |
|    learning_rate      | 0.0007   |
|    n_updates          | 439      |
|    policy_loss        | -1.46    |
|    value_loss         | 3.29     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.2      |
|    mean_reward        | -6.17    |
| time/                 |          |
|    total_timesteps    | 56832    |
| train/                |          |
|    entropy_loss       | -1.42    |
|    explained_variance | 0.716    |
|    learning_rate      | 0.0007   |
|    n_updates          | 443      |
|    policy_loss        | 0.832    |
|    value_loss         | 2.61     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31       |
|    mean_reward        | -8.28    |
| time/                 |          |
|    total_timesteps    | 57344    |
| train/                |          |
|    entropy_loss       | -1.42    |
|    explained_variance | 0.729    |
|    learning_rate      | 0.0007   |
|    n_updates          | 447      |
|    policy_loss        | -1.3     |
|    value_loss         | 2.11     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6        |
|    mean_reward        | -5.22    |
| time/                 |          |
|    total_timesteps    | 57856    |
| train/                |          |
|    entropy_loss       | -1.38    |
|    explained_variance | 0.709    |
|    learning_rate      | 0.0007   |
|    n_updates          | 451      |
|    policy_loss        | -1.06    |
|    value_loss         | 1.73     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.6      |
|    mean_reward        | -4.79    |
| time/                 |          |
|    total_timesteps    | 58368    |
| train/                |          |
|    entropy_loss       | -0.987   |
|    explained_variance | 0.12     |
|    learning_rate      | 0.0007   |
|    n_updates          | 455      |
|    policy_loss        | -1.23    |
|    value_loss         | 8.22     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 57       |
|    mean_reward        | -15.9    |
| time/                 |          |
|    total_timesteps    | 58880    |
| train/                |          |
|    entropy_loss       | -1.32    |
|    explained_variance | 0.717    |
|    learning_rate      | 0.0007   |
|    n_updates          | 459      |
|    policy_loss        | -1.51    |
|    value_loss         | 2.17     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.2      |
|    mean_reward        | -4.85    |
| time/                 |          |
|    total_timesteps    | 59392    |
| train/                |          |
|    entropy_loss       | -1.14    |
|    explained_variance | 0.391    |
|    learning_rate      | 0.0007   |
|    n_updates          | 463      |
|    policy_loss        | -1.22    |
|    value_loss         | 5.18     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31       |
|    mean_reward        | -5.55    |
| time/                 |          |
|    total_timesteps    | 59904    |
| train/                |          |
|    entropy_loss       | -1.24    |
|    explained_variance | 0.643    |
|    learning_rate      | 0.0007   |
|    n_updates          | 467      |
|    policy_loss        | -0.501   |
|    value_loss         | 2.02     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.4     |
|    mean_reward        | -7.67    |
| time/                 |          |
|    total_timesteps    | 60416    |
| train/                |          |
|    entropy_loss       | -1.15    |
|    explained_variance | 0.72     |
|    learning_rate      | 0.0007   |
|    n_updates          | 471      |
|    policy_loss        | -0.584   |
|    value_loss         | 1.71     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 85.2     |
|    mean_reward        | -19.8    |
| time/                 |          |
|    total_timesteps    | 60928    |
| train/                |          |
|    entropy_loss       | -0.832   |
|    explained_variance | 0.112    |
|    learning_rate      | 0.0007   |
|    n_updates          | 475      |
|    policy_loss        | -0.403   |
|    value_loss         | 7.55     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31       |
|    mean_reward        | -6.46    |
| time/                 |          |
|    total_timesteps    | 61440    |
| train/                |          |
|    entropy_loss       | -1.01    |
|    explained_variance | 0.578    |
|    learning_rate      | 0.0007   |
|    n_updates          | 479      |
|    policy_loss        | 0.61     |
|    value_loss         | 2.26     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.6     |
|    mean_reward        | -6.42    |
| time/                 |          |
|    total_timesteps    | 61952    |
| train/                |          |
|    entropy_loss       | -1.08    |
|    explained_variance | 0.717    |
|    learning_rate      | 0.0007   |
|    n_updates          | 483      |
|    policy_loss        | -0.759   |
|    value_loss         | 1.79     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.8     |
|    mean_reward        | -6.66    |
| time/                 |          |
|    total_timesteps    | 62464    |
| train/                |          |
|    entropy_loss       | -1.1     |
|    explained_variance | 0.644    |
|    learning_rate      | 0.0007   |
|    n_updates          | 487      |
|    policy_loss        | 0.859    |
|    value_loss         | 3.74     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.8      |
|    mean_reward        | -5.64    |
| time/                 |          |
|    total_timesteps    | 62976    |
| train/                |          |
|    entropy_loss       | -1.13    |
|    explained_variance | 0.686    |
|    learning_rate      | 0.0007   |
|    n_updates          | 491      |
|    policy_loss        | 0.021    |
|    value_loss         | 2.28     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.6     |
|    mean_reward        | -8.97    |
| time/                 |          |
|    total_timesteps    | 63488    |
| train/                |          |
|    entropy_loss       | -0.904   |
|    explained_variance | 0.546    |
|    learning_rate      | 0.0007   |
|    n_updates          | 495      |
|    policy_loss        | -0.778   |
|    value_loss         | 2.71     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5        |
|    mean_reward        | -4.67    |
| time/                 |          |
|    total_timesteps    | 64000    |
| train/                |          |
|    entropy_loss       | -1.12    |
|    explained_variance | 0.708    |
|    learning_rate      | 0.0007   |
|    n_updates          | 499      |
|    policy_loss        | 0.0483   |
|    value_loss         | 2.02     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.33     |
|    ep_rew_mean     | -6.32    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 500      |
|    time_elapsed    | 165273   |
|    total_timesteps | 64000    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6        |
|    mean_reward        | -4.95    |
| time/                 |          |
|    total_timesteps    | 64512    |
| train/                |          |
|    entropy_loss       | -1.12    |
|    explained_variance | 0.764    |
|    learning_rate      | 0.0007   |
|    n_updates          | 503      |
|    policy_loss        | 0.518    |
|    value_loss         | 1.54     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.6      |
|    mean_reward        | -4.36    |
| time/                 |          |
|    total_timesteps    | 65024    |
| train/                |          |
|    entropy_loss       | -0.884   |
|    explained_variance | 0.335    |
|    learning_rate      | 0.0007   |
|    n_updates          | 507      |
|    policy_loss        | -1.19    |
|    value_loss         | 5.93     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.6     |
|    mean_reward        | -9.25    |
| time/                 |          |
|    total_timesteps    | 65536    |
| train/                |          |
|    entropy_loss       | -1.11    |
|    explained_variance | 0.71     |
|    learning_rate      | 0.0007   |
|    n_updates          | 511      |
|    policy_loss        | -0.18    |
|    value_loss         | 2.09     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31       |
|    mean_reward        | -7.89    |
| time/                 |          |
|    total_timesteps    | 66048    |
| train/                |          |
|    entropy_loss       | -1.18    |
|    explained_variance | 0.66     |
|    learning_rate      | 0.0007   |
|    n_updates          | 515      |
|    policy_loss        | -0.551   |
|    value_loss         | 2.22     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6        |
|    mean_reward        | -5.34    |
| time/                 |          |
|    total_timesteps    | 66560    |
| train/                |          |
|    entropy_loss       | -1.1     |
|    explained_variance | 0.757    |
|    learning_rate      | 0.0007   |
|    n_updates          | 519      |
|    policy_loss        | 0.184    |
|    value_loss         | 1.33     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6.4      |
|    mean_reward        | -5.55    |
| time/                 |          |
|    total_timesteps    | 67072    |
| train/                |          |
|    entropy_loss       | -1.34    |
|    explained_variance | 0.72     |
|    learning_rate      | 0.0007   |
|    n_updates          | 523      |
|    policy_loss        | 0.798    |
|    value_loss         | 2.45     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.8     |
|    mean_reward        | -10.6    |
| time/                 |          |
|    total_timesteps    | 67584    |
| train/                |          |
|    entropy_loss       | -0.726   |
|    explained_variance | 0.0421   |
|    learning_rate      | 0.0007   |
|    n_updates          | 527      |
|    policy_loss        | -0.585   |
|    value_loss         | 16.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.6      |
|    mean_reward        | -4.58    |
| time/                 |          |
|    total_timesteps    | 68096    |
| train/                |          |
|    entropy_loss       | -1.02    |
|    explained_variance | 0.561    |
|    learning_rate      | 0.0007   |
|    n_updates          | 531      |
|    policy_loss        | 1.19     |
|    value_loss         | 4.64     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.2      |
|    mean_reward        | -4.71    |
| time/                 |          |
|    total_timesteps    | 68608    |
| train/                |          |
|    entropy_loss       | -0.941   |
|    explained_variance | 0.657    |
|    learning_rate      | 0.0007   |
|    n_updates          | 535      |
|    policy_loss        | -1.55    |
|    value_loss         | 2.26     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5        |
|    mean_reward        | -5.84    |
| time/                 |          |
|    total_timesteps    | 69120    |
| train/                |          |
|    entropy_loss       | -1.33    |
|    explained_variance | 0.591    |
|    learning_rate      | 0.0007   |
|    n_updates          | 539      |
|    policy_loss        | -1.53    |
|    value_loss         | 2.75     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.8      |
|    mean_reward        | -5.17    |
| time/                 |          |
|    total_timesteps    | 69632    |
| train/                |          |
|    entropy_loss       | -1.13    |
|    explained_variance | 0.683    |
|    learning_rate      | 0.0007   |
|    n_updates          | 543      |
|    policy_loss        | -0.886   |
|    value_loss         | 1.83     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.8      |
|    mean_reward        | -5.19    |
| time/                 |          |
|    total_timesteps    | 70144    |
| train/                |          |
|    entropy_loss       | -1.04    |
|    explained_variance | 0.655    |
|    learning_rate      | 0.0007   |
|    n_updates          | 547      |
|    policy_loss        | -1.83    |
|    value_loss         | 2.13     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.2      |
|    mean_reward        | -6.5     |
| time/                 |          |
|    total_timesteps    | 70656    |
| train/                |          |
|    entropy_loss       | -1.12    |
|    explained_variance | 0.652    |
|    learning_rate      | 0.0007   |
|    n_updates          | 551      |
|    policy_loss        | -0.488   |
|    value_loss         | 1.94     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.6     |
|    mean_reward        | -6.74    |
| time/                 |          |
|    total_timesteps    | 71168    |
| train/                |          |
|    entropy_loss       | -1.08    |
|    explained_variance | 0.71     |
|    learning_rate      | 0.0007   |
|    n_updates          | 555      |
|    policy_loss        | -0.832   |
|    value_loss         | 1.89     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.2     |
|    mean_reward        | -9.09    |
| time/                 |          |
|    total_timesteps    | 71680    |
| train/                |          |
|    entropy_loss       | -1.16    |
|    explained_variance | 0.803    |
|    learning_rate      | 0.0007   |
|    n_updates          | 559      |
|    policy_loss        | 0.498    |
|    value_loss         | 1.54     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.6     |
|    mean_reward        | -7.46    |
| time/                 |          |
|    total_timesteps    | 72192    |
| train/                |          |
|    entropy_loss       | -0.874   |
|    explained_variance | 0.493    |
|    learning_rate      | 0.0007   |
|    n_updates          | 563      |
|    policy_loss        | 0.249    |
|    value_loss         | 1.87     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.4     |
|    mean_reward        | -8.08    |
| time/                 |          |
|    total_timesteps    | 72704    |
| train/                |          |
|    entropy_loss       | -0.914   |
|    explained_variance | 0.667    |
|    learning_rate      | 0.0007   |
|    n_updates          | 567      |
|    policy_loss        | -1.11    |
|    value_loss         | 2.15     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31       |
|    mean_reward        | -8.18    |
| time/                 |          |
|    total_timesteps    | 73216    |
| train/                |          |
|    entropy_loss       | -0.942   |
|    explained_variance | 0.524    |
|    learning_rate      | 0.0007   |
|    n_updates          | 571      |
|    policy_loss        | 0.594    |
|    value_loss         | 4.74     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.8     |
|    mean_reward        | -7.08    |
| time/                 |          |
|    total_timesteps    | 73728    |
| train/                |          |
|    entropy_loss       | -1.05    |
|    explained_variance | 0.686    |
|    learning_rate      | 0.0007   |
|    n_updates          | 575      |
|    policy_loss        | 0.609    |
|    value_loss         | 3.18     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 32.2     |
|    mean_reward        | -6.16    |
| time/                 |          |
|    total_timesteps    | 74240    |
| train/                |          |
|    entropy_loss       | -1.14    |
|    explained_variance | 0.372    |
|    learning_rate      | 0.0007   |
|    n_updates          | 579      |
|    policy_loss        | 0.384    |
|    value_loss         | 7.77     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.8     |
|    mean_reward        | -7.17    |
| time/                 |          |
|    total_timesteps    | 74752    |
| train/                |          |
|    entropy_loss       | -0.84    |
|    explained_variance | 0.512    |
|    learning_rate      | 0.0007   |
|    n_updates          | 583      |
|    policy_loss        | 0.484    |
|    value_loss         | 4.26     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 82.6     |
|    mean_reward        | -10.7    |
| time/                 |          |
|    total_timesteps    | 75264    |
| train/                |          |
|    entropy_loss       | -1.06    |
|    explained_variance | 0.758    |
|    learning_rate      | 0.0007   |
|    n_updates          | 587      |
|    policy_loss        | -0.381   |
|    value_loss         | 1.27     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5        |
|    mean_reward        | -4.94    |
| time/                 |          |
|    total_timesteps    | 75776    |
| train/                |          |
|    entropy_loss       | -0.955   |
|    explained_variance | 0.504    |
|    learning_rate      | 0.0007   |
|    n_updates          | 591      |
|    policy_loss        | 0.709    |
|    value_loss         | 4.23     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.6      |
|    mean_reward        | -5.42    |
| time/                 |          |
|    total_timesteps    | 76288    |
| train/                |          |
|    entropy_loss       | -0.963   |
|    explained_variance | 0.588    |
|    learning_rate      | 0.0007   |
|    n_updates          | 595      |
|    policy_loss        | 0.0128   |
|    value_loss         | 1.97     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31       |
|    mean_reward        | -8.4     |
| time/                 |          |
|    total_timesteps    | 76800    |
| train/                |          |
|    entropy_loss       | -0.775   |
|    explained_variance | 0.644    |
|    learning_rate      | 0.0007   |
|    n_updates          | 599      |
|    policy_loss        | -0.0772  |
|    value_loss         | 1.59     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.78     |
|    ep_rew_mean     | -6.2     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 600      |
|    time_elapsed    | 183963   |
|    total_timesteps | 76800    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.8      |
|    mean_reward        | -4.64    |
| time/                 |          |
|    total_timesteps    | 77312    |
| train/                |          |
|    entropy_loss       | -0.983   |
|    explained_variance | 0.552    |
|    learning_rate      | 0.0007   |
|    n_updates          | 603      |
|    policy_loss        | -0.88    |
|    value_loss         | 3.18     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.4     |
|    mean_reward        | -7.13    |
| time/                 |          |
|    total_timesteps    | 77824    |
| train/                |          |
|    entropy_loss       | -0.742   |
|    explained_variance | 0.589    |
|    learning_rate      | 0.0007   |
|    n_updates          | 607      |
|    policy_loss        | -1.02    |
|    value_loss         | 2.28     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 31.2     |
|    mean_reward        | -6.93    |
| time/                 |          |
|    total_timesteps    | 78336    |
| train/                |          |
|    entropy_loss       | -1.12    |
|    explained_variance | 0.777    |
|    learning_rate      | 0.0007   |
|    n_updates          | 611      |
|    policy_loss        | -0.445   |
|    value_loss         | 1.31     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.2      |
|    mean_reward        | -5.54    |
| time/                 |          |
|    total_timesteps    | 78848    |
| train/                |          |
|    entropy_loss       | -0.941   |
|    explained_variance | 0.688    |
|    learning_rate      | 0.0007   |
|    n_updates          | 615      |
|    policy_loss        | -1.12    |
|    value_loss         | 2.21     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.4      |
|    mean_reward        | -5.36    |
| time/                 |          |
|    total_timesteps    | 79360    |
| train/                |          |
|    entropy_loss       | -0.843   |
|    explained_variance | 0.521    |
|    learning_rate      | 0.0007   |
|    n_updates          | 619      |
|    policy_loss        | -0.24    |
|    value_loss         | 3.19     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.8      |
|    mean_reward        | -6.13    |
| time/                 |          |
|    total_timesteps    | 79872    |
| train/                |          |
|    entropy_loss       | -0.914   |
|    explained_variance | 0.722    |
|    learning_rate      | 0.0007   |
|    n_updates          | 623      |
|    policy_loss        | 0.0596   |
|    value_loss         | 1.77     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 32       |
|    mean_reward        | -8.34    |
| time/                 |          |
|    total_timesteps    | 80384    |
| train/                |          |
|    entropy_loss       | -0.835   |
|    explained_variance | 0.632    |
|    learning_rate      | 0.0007   |
|    n_updates          | 627      |
|    policy_loss        | -0.171   |
|    value_loss         | 2.01     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 6        |
|    mean_reward        | -6.01    |
| time/                 |          |
|    total_timesteps    | 80896    |
| train/                |          |
|    entropy_loss       | -1.01    |
|    explained_variance | 0.519    |
|    learning_rate      | 0.0007   |
|    n_updates          | 631      |
|    policy_loss        | 0.472    |
|    value_loss         | 3.61     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 8.4      |
|    mean_reward        | -5.81    |
| time/                 |          |
|    total_timesteps    | 81408    |
| train/                |          |
|    entropy_loss       | -1.11    |
|    explained_variance | 0.793    |
|    learning_rate      | 0.0007   |
|    n_updates          | 635      |
|    policy_loss        | -1.09    |
|    value_loss         | 1.67     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 82.2     |
|    mean_reward        | -9.53    |
| time/                 |          |
|    total_timesteps    | 81920    |
| train/                |          |
|    entropy_loss       | -0.678   |
|    explained_variance | 0.628    |
|    learning_rate      | 0.0007   |
|    n_updates          | 639      |
|    policy_loss        | -0.0952  |
|    value_loss         | 2.55     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 108      |
|    mean_reward        | -22.2    |
| time/                 |          |
|    total_timesteps    | 82432    |
| train/                |          |
|    entropy_loss       | -0.585   |
|    explained_variance | 0.142    |
|    learning_rate      | 0.0007   |
|    n_updates          | 643      |
|    policy_loss        | 0.4      |
|    value_loss         | 3.16     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 57.2     |
|    mean_reward        | -7.45    |
| time/                 |          |
|    total_timesteps    | 82944    |
| train/                |          |
|    entropy_loss       | -0.713   |
|    explained_variance | 0.642    |
|    learning_rate      | 0.0007   |
|    n_updates          | 647      |
|    policy_loss        | -0.293   |
|    value_loss         | 1.93     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5        |
|    mean_reward        | -5.36    |
| time/                 |          |
|    total_timesteps    | 83456    |
| train/                |          |
|    entropy_loss       | -0.956   |
|    explained_variance | 0.701    |
|    learning_rate      | 0.0007   |
|    n_updates          | 651      |
|    policy_loss        | 0.239    |
|    value_loss         | 1.98     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.6      |
|    mean_reward        | -5.09    |
| time/                 |          |
|    total_timesteps    | 83968    |
| train/                |          |
|    entropy_loss       | -1.01    |
|    explained_variance | 0.752    |
|    learning_rate      | 0.0007   |
|    n_updates          | 655      |
|    policy_loss        | 0.212    |
|    value_loss         | 1.51     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 29.8     |
|    mean_reward        | -5.37    |
| time/                 |          |
|    total_timesteps    | 84480    |
| train/                |          |
|    entropy_loss       | -1.08    |
|    explained_variance | 0.776    |
|    learning_rate      | 0.0007   |
|    n_updates          | 659      |
|    policy_loss        | 0.217    |
|    value_loss         | 1.77     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.6      |
|    mean_reward        | -5.8     |
| time/                 |          |
|    total_timesteps    | 84992    |
| train/                |          |
|    entropy_loss       | -0.797   |
|    explained_variance | 0.699    |
|    learning_rate      | 0.0007   |
|    n_updates          | 663      |
|    policy_loss        | -0.0308  |
|    value_loss         | 1.61     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.4      |
|    mean_reward        | -6       |
| time/                 |          |
|    total_timesteps    | 85504    |
| train/                |          |
|    entropy_loss       | -1.15    |
|    explained_variance | 0.781    |
|    learning_rate      | 0.0007   |
|    n_updates          | 667      |
|    policy_loss        | 0.0119   |
|    value_loss         | 1.27     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56       |
|    mean_reward        | -7.82    |
| time/                 |          |
|    total_timesteps    | 86016    |
| train/                |          |
|    entropy_loss       | -0.99    |
|    explained_variance | 0.78     |
|    learning_rate      | 0.0007   |
|    n_updates          | 671      |
|    policy_loss        | 0.7      |
|    value_loss         | 2.03     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.6      |
|    mean_reward        | -4.85    |
| time/                 |          |
|    total_timesteps    | 86528    |
| train/                |          |
|    entropy_loss       | -0.688   |
|    explained_variance | 0.603    |
|    learning_rate      | 0.0007   |
|    n_updates          | 675      |
|    policy_loss        | -0.156   |
|    value_loss         | 2.91     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.8      |
|    mean_reward        | -4.08    |
| time/                 |          |
|    total_timesteps    | 87040    |
| train/                |          |
|    entropy_loss       | -0.724   |
|    explained_variance | 0.625    |
|    learning_rate      | 0.0007   |
|    n_updates          | 679      |
|    policy_loss        | 0.358    |
|    value_loss         | 1.86     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56       |
|    mean_reward        | -10.1    |
| time/                 |          |
|    total_timesteps    | 87552    |
| train/                |          |
|    entropy_loss       | -0.533   |
|    explained_variance | 0.00622  |
|    learning_rate      | 0.0007   |
|    n_updates          | 683      |
|    policy_loss        | 0.599    |
|    value_loss         | 5.67     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.8     |
|    mean_reward        | -13.6    |
| time/                 |          |
|    total_timesteps    | 88064    |
| train/                |          |
|    entropy_loss       | -0.844   |
|    explained_variance | 0.649    |
|    learning_rate      | 0.0007   |
|    n_updates          | 687      |
|    policy_loss        | 0.223    |
|    value_loss         | 2.89     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56.6     |
|    mean_reward        | -11.5    |
| time/                 |          |
|    total_timesteps    | 88576    |
| train/                |          |
|    entropy_loss       | -0.924   |
|    explained_variance | 0.708    |
|    learning_rate      | 0.0007   |
|    n_updates          | 691      |
|    policy_loss        | 0.158    |
|    value_loss         | 1.87     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4        |
|    mean_reward        | -4.96    |
| time/                 |          |
|    total_timesteps    | 89088    |
| train/                |          |
|    entropy_loss       | -1.03    |
|    explained_variance | 0.675    |
|    learning_rate      | 0.0007   |
|    n_updates          | 695      |
|    policy_loss        | 0.57     |
|    value_loss         | 2.48     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 81.8     |
|    mean_reward        | -12.4    |
| time/                 |          |
|    total_timesteps    | 89600    |
| train/                |          |
|    entropy_loss       | -0.866   |
|    explained_variance | 0.712    |
|    learning_rate      | 0.0007   |
|    n_updates          | 699      |
|    policy_loss        | -0.57    |
|    value_loss         | 1.25     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.73     |
|    ep_rew_mean     | -6.14    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 700      |
|    time_elapsed    | 203851   |
|    total_timesteps | 89600    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 5.2      |
|    mean_reward        | -5.76    |
| time/                 |          |
|    total_timesteps    | 90112    |
| train/                |          |
|    entropy_loss       | -1.04    |
|    explained_variance | 0.768    |
|    learning_rate      | 0.0007   |
|    n_updates          | 703      |
|    policy_loss        | -0.337   |
|    value_loss         | 1.64     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.6      |
|    mean_reward        | -5.58    |
| time/                 |          |
|    total_timesteps    | 90624    |
| train/                |          |
|    entropy_loss       | -0.841   |
|    explained_variance | 0.776    |
|    learning_rate      | 0.0007   |
|    n_updates          | 707      |
|    policy_loss        | 0.192    |
|    value_loss         | 1.58     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56       |
|    mean_reward        | -12.3    |
| time/                 |          |
|    total_timesteps    | 91136    |
| train/                |          |
|    entropy_loss       | -0.821   |
|    explained_variance | 0.531    |
|    learning_rate      | 0.0007   |
|    n_updates          | 711      |
|    policy_loss        | -0.208   |
|    value_loss         | 4.65     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 55.8     |
|    mean_reward        | -12.7    |
| time/                 |          |
|    total_timesteps    | 91648    |
| train/                |          |
|    entropy_loss       | -0.969   |
|    explained_variance | 0.79     |
|    learning_rate      | 0.0007   |
|    n_updates          | 715      |
|    policy_loss        | -0.188   |
|    value_loss         | 1.25     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.6     |
|    mean_reward        | -8.45    |
| time/                 |          |
|    total_timesteps    | 92160    |
| train/                |          |
|    entropy_loss       | -0.921   |
|    explained_variance | 0.751    |
|    learning_rate      | 0.0007   |
|    n_updates          | 719      |
|    policy_loss        | -0.59    |
|    value_loss         | 1.16     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4        |
|    mean_reward        | -6.26    |
| time/                 |          |
|    total_timesteps    | 92672    |
| train/                |          |
|    entropy_loss       | -0.735   |
|    explained_variance | 0.559    |
|    learning_rate      | 0.0007   |
|    n_updates          | 723      |
|    policy_loss        | -0.551   |
|    value_loss         | 2        |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30       |
|    mean_reward        | -6.36    |
| time/                 |          |
|    total_timesteps    | 93184    |
| train/                |          |
|    entropy_loss       | -0.779   |
|    explained_variance | 0.657    |
|    learning_rate      | 0.0007   |
|    n_updates          | 727      |
|    policy_loss        | -1.32    |
|    value_loss         | 2.45     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4        |
|    mean_reward        | -5.88    |
| time/                 |          |
|    total_timesteps    | 93696    |
| train/                |          |
|    entropy_loss       | -0.875   |
|    explained_variance | 0.677    |
|    learning_rate      | 0.0007   |
|    n_updates          | 731      |
|    policy_loss        | -1.13    |
|    value_loss         | 2.02     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.2     |
|    mean_reward        | -8.4     |
| time/                 |          |
|    total_timesteps    | 94208    |
| train/                |          |
|    entropy_loss       | -0.929   |
|    explained_variance | 0.773    |
|    learning_rate      | 0.0007   |
|    n_updates          | 735      |
|    policy_loss        | -0.736   |
|    value_loss         | 1.32     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.8      |
|    mean_reward        | -5.36    |
| time/                 |          |
|    total_timesteps    | 94720    |
| train/                |          |
|    entropy_loss       | -0.747   |
|    explained_variance | 0.622    |
|    learning_rate      | 0.0007   |
|    n_updates          | 739      |
|    policy_loss        | -0.596   |
|    value_loss         | 2.01     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4        |
|    mean_reward        | -6.35    |
| time/                 |          |
|    total_timesteps    | 95232    |
| train/                |          |
|    entropy_loss       | -0.846   |
|    explained_variance | 0.813    |
|    learning_rate      | 0.0007   |
|    n_updates          | 743      |
|    policy_loss        | -0.398   |
|    value_loss         | 1.06     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 81.8     |
|    mean_reward        | -15.2    |
| time/                 |          |
|    total_timesteps    | 95744    |
| train/                |          |
|    entropy_loss       | -0.607   |
|    explained_variance | 0.673    |
|    learning_rate      | 0.0007   |
|    n_updates          | 747      |
|    policy_loss        | -0.114   |
|    value_loss         | 1.22     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 29.6     |
|    mean_reward        | -6.97    |
| time/                 |          |
|    total_timesteps    | 96256    |
| train/                |          |
|    entropy_loss       | -0.608   |
|    explained_variance | 0.7      |
|    learning_rate      | 0.0007   |
|    n_updates          | 751      |
|    policy_loss        | 0.15     |
|    value_loss         | 3.09     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.4      |
|    mean_reward        | -5.5     |
| time/                 |          |
|    total_timesteps    | 96768    |
| train/                |          |
|    entropy_loss       | -0.752   |
|    explained_variance | 0.726    |
|    learning_rate      | 0.0007   |
|    n_updates          | 755      |
|    policy_loss        | -0.573   |
|    value_loss         | 1.24     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 30.2     |
|    mean_reward        | -7.38    |
| time/                 |          |
|    total_timesteps    | 97280    |
| train/                |          |
|    entropy_loss       | -0.489   |
|    explained_variance | 0.312    |
|    learning_rate      | 0.0007   |
|    n_updates          | 759      |
|    policy_loss        | 0.111    |
|    value_loss         | 8.31     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4.4      |
|    mean_reward        | -5.68    |
| time/                 |          |
|    total_timesteps    | 97792    |
| train/                |          |
|    entropy_loss       | -0.706   |
|    explained_variance | 0.43     |
|    learning_rate      | 0.0007   |
|    n_updates          | 763      |
|    policy_loss        | -0.884   |
|    value_loss         | 3.27     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 4        |
|    mean_reward        | -5.82    |
| time/                 |          |
|    total_timesteps    | 98304    |
| train/                |          |
|    entropy_loss       | -0.722   |
|    explained_variance | 0.743    |
|    learning_rate      | 0.0007   |
|    n_updates          | 767      |
|    policy_loss        | 0.457    |
|    value_loss         | 2.22     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 56       |
|    mean_reward        | -9.9     |
| time/                 |          |
|    total_timesteps    | 98816    |
| train/                |          |
|    entropy_loss       | -0.71    |
|    explained_variance | 0.692    |
|    learning_rate      | 0.0007   |
|    n_updates          | 771      |
|    policy_loss        | -0.16    |
|    value_loss         | 1.74     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 29.8     |
|    mean_reward        | -8.91    |
| time/                 |          |
|    total_timesteps    | 99328    |
| train/                |          |
|    entropy_loss       | -0.493   |
|    explained_variance | 0.594    |
|    learning_rate      | 0.0007   |
|    n_updates          | 775      |
|    policy_loss        | -0.557   |
|    value_loss         | 4.58     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 3.4      |
|    mean_reward        | -4.81    |
| time/                 |          |
|    total_timesteps    | 99840    |
| train/                |          |
|    entropy_loss       | -0.614   |
|    explained_variance | 0.765    |
|    learning_rate      | 0.0007   |
|    n_updates          | 779      |
|    policy_loss        | -0.421   |
|    value_loss         | 2.09     |
------------------------------------
