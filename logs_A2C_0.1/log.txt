Logging to ./logs_A2C_0.1
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 44.4     |
|    mean_reward        | -372     |
| time/                 |          |
|    total_timesteps    | 512      |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 0.00012  |
|    learning_rate      | 0.0007   |
|    n_updates          | 3        |
|    policy_loss        | -749     |
|    value_loss         | 7.9e+03  |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 43.2      |
|    mean_reward        | -203      |
| time/                 |           |
|    total_timesteps    | 1024      |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -0.000147 |
|    learning_rate      | 0.0007    |
|    n_updates          | 7         |
|    policy_loss        | -1.05e+03 |
|    value_loss         | 1.45e+04  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -215     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 3.93e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 11       |
|    policy_loss        | -753     |
|    value_loss         | 8.72e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 2048      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -0.000121 |
|    learning_rate      | 0.0007    |
|    n_updates          | 15        |
|    policy_loss        | -706      |
|    value_loss         | 6.47e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -241      |
| time/                 |           |
|    total_timesteps    | 2560      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -9.47e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 19        |
|    policy_loss        | -779      |
|    value_loss         | 8.33e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 398       |
|    mean_reward        | -241      |
| time/                 |           |
|    total_timesteps    | 3072      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -7.95e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 23        |
|    policy_loss        | -729      |
|    value_loss         | 8.39e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 14.4      |
|    mean_reward        | -46.7     |
| time/                 |           |
|    total_timesteps    | 3584      |
| train/                |           |
|    entropy_loss       | -10.2     |
|    explained_variance | -3.67e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 27        |
|    policy_loss        | -683      |
|    value_loss         | 8.74e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 4096      |
| train/                |           |
|    entropy_loss       | -10.1     |
|    explained_variance | -7.37e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 31        |
|    policy_loss        | -688      |
|    value_loss         | 7.03e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 4608      |
| train/                |           |
|    entropy_loss       | -10       |
|    explained_variance | -5.78e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 35        |
|    policy_loss        | -601      |
|    value_loss         | 5.85e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 5120      |
| train/                |           |
|    entropy_loss       | -9.85     |
|    explained_variance | -2.44e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 39        |
|    policy_loss        | -638      |
|    value_loss         | 7.12e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 5632     |
| train/                |          |
|    entropy_loss       | -9.59    |
|    explained_variance | -9.3e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 43       |
|    policy_loss        | -348     |
|    value_loss         | 2.86e+03 |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 6144     |
| train/                |          |
|    entropy_loss       | -9.54    |
|    explained_variance | 2.06e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 47       |
|    policy_loss        | -520     |
|    value_loss         | 3.92e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 6656      |
| train/                |           |
|    entropy_loss       | -9.2      |
|    explained_variance | -4.05e-05 |
|    learning_rate      | 0.0007    |
|    n_updates          | 51        |
|    policy_loss        | -366      |
|    value_loss         | 2.86e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7168      |
| train/                |           |
|    entropy_loss       | -9.03     |
|    explained_variance | -0.000117 |
|    learning_rate      | 0.0007    |
|    n_updates          | 55        |
|    policy_loss        | -187      |
|    value_loss         | 1.13e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7680      |
| train/                |           |
|    entropy_loss       | -8.24     |
|    explained_variance | -0.000124 |
|    learning_rate      | 0.0007    |
|    n_updates          | 59        |
|    policy_loss        | -72.1     |
|    value_loss         | 388       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    entropy_loss       | -6.51     |
|    explained_variance | -0.000259 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -20.3     |
|    value_loss         | 153       |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 8704     |
| train/                |          |
|    entropy_loss       | -5.27    |
|    explained_variance | 0.00012  |
|    learning_rate      | 0.0007   |
|    n_updates          | 67       |
|    policy_loss        | 4.89     |
|    value_loss         | 82.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 9216     |
| train/                |          |
|    entropy_loss       | -5.12    |
|    explained_variance | 0.000339 |
|    learning_rate      | 0.0007   |
|    n_updates          | 71       |
|    policy_loss        | -3.3     |
|    value_loss         | 92.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 9728     |
| train/                |          |
|    entropy_loss       | -4.86    |
|    explained_variance | 0.000836 |
|    learning_rate      | 0.0007   |
|    n_updates          | 75       |
|    policy_loss        | 4.42     |
|    value_loss         | 76       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 10240    |
| train/                |          |
|    entropy_loss       | -5.18    |
|    explained_variance | 0.00188  |
|    learning_rate      | 0.0007   |
|    n_updates          | 79       |
|    policy_loss        | -4.02    |
|    value_loss         | 105      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 10752    |
| train/                |          |
|    entropy_loss       | -4.8     |
|    explained_variance | 0.0163   |
|    learning_rate      | 0.0007   |
|    n_updates          | 83       |
|    policy_loss        | 17.8     |
|    value_loss         | 59.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11264    |
| train/                |          |
|    entropy_loss       | -4.58    |
|    explained_variance | 0.0422   |
|    learning_rate      | 0.0007   |
|    n_updates          | 87       |
|    policy_loss        | 3.29     |
|    value_loss         | 65.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11776    |
| train/                |          |
|    entropy_loss       | -4.25    |
|    explained_variance | 0.114    |
|    learning_rate      | 0.0007   |
|    n_updates          | 91       |
|    policy_loss        | -3.78    |
|    value_loss         | 61.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12288    |
| train/                |          |
|    entropy_loss       | -4.14    |
|    explained_variance | 0.351    |
|    learning_rate      | 0.0007   |
|    n_updates          | 95       |
|    policy_loss        | 0.117    |
|    value_loss         | 26.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -3.69    |
|    explained_variance | 0.307    |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | -6.63    |
|    value_loss         | 38       |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.49     |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 43502    |
|    total_timesteps | 12800    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13312    |
| train/                |          |
|    entropy_loss       | -3.6     |
|    explained_variance | 0.352    |
|    learning_rate      | 0.0007   |
|    n_updates          | 103      |
|    policy_loss        | -4.04    |
|    value_loss         | 21.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13824    |
| train/                |          |
|    entropy_loss       | -3.46    |
|    explained_variance | 0.446    |
|    learning_rate      | 0.0007   |
|    n_updates          | 107      |
|    policy_loss        | -4.74    |
|    value_loss         | 12.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14336    |
| train/                |          |
|    entropy_loss       | -3.44    |
|    explained_variance | 0.623    |
|    learning_rate      | 0.0007   |
|    n_updates          | 111      |
|    policy_loss        | 3.24     |
|    value_loss         | 7.06     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14848    |
| train/                |          |
|    entropy_loss       | -3.32    |
|    explained_variance | 0.465    |
|    learning_rate      | 0.0007   |
|    n_updates          | 115      |
|    policy_loss        | -0.298   |
|    value_loss         | 12.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15360    |
| train/                |          |
|    entropy_loss       | -3.27    |
|    explained_variance | 0.585    |
|    learning_rate      | 0.0007   |
|    n_updates          | 119      |
|    policy_loss        | 6.66     |
|    value_loss         | 12.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15872    |
| train/                |          |
|    entropy_loss       | -3.23    |
|    explained_variance | 0.637    |
|    learning_rate      | 0.0007   |
|    n_updates          | 123      |
|    policy_loss        | -2.79    |
|    value_loss         | 7.58     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16384    |
| train/                |          |
|    entropy_loss       | -3.06    |
|    explained_variance | 0.34     |
|    learning_rate      | 0.0007   |
|    n_updates          | 127      |
|    policy_loss        | -2.61    |
|    value_loss         | 14.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16896    |
| train/                |          |
|    entropy_loss       | -2.99    |
|    explained_variance | 0.816    |
|    learning_rate      | 0.0007   |
|    n_updates          | 131      |
|    policy_loss        | 3.46     |
|    value_loss         | 3.63     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 17408    |
| train/                |          |
|    entropy_loss       | -2.94    |
|    explained_variance | 0.709    |
|    learning_rate      | 0.0007   |
|    n_updates          | 135      |
|    policy_loss        | 2.3      |
|    value_loss         | 5.09     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17920    |
| train/                |          |
|    entropy_loss       | -3.17    |
|    explained_variance | 0.623    |
|    learning_rate      | 0.0007   |
|    n_updates          | 139      |
|    policy_loss        | -1.85    |
|    value_loss         | 7.71     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18432    |
| train/                |          |
|    entropy_loss       | -3.21    |
|    explained_variance | 0.759    |
|    learning_rate      | 0.0007   |
|    n_updates          | 143      |
|    policy_loss        | 3.06     |
|    value_loss         | 3.47     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    entropy_loss       | -3.14    |
|    explained_variance | 0.516    |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | -6.75    |
|    value_loss         | 26.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    entropy_loss       | -3.32    |
|    explained_variance | 0.506    |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | 2.85     |
|    value_loss         | 10.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19968    |
| train/                |          |
|    entropy_loss       | -3.23    |
|    explained_variance | 0.532    |
|    learning_rate      | 0.0007   |
|    n_updates          | 155      |
|    policy_loss        | -0.313   |
|    value_loss         | 10       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20480    |
| train/                |          |
|    entropy_loss       | -3.24    |
|    explained_variance | 0.535    |
|    learning_rate      | 0.0007   |
|    n_updates          | 159      |
|    policy_loss        | -0.124   |
|    value_loss         | 11.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20992    |
| train/                |          |
|    entropy_loss       | -3.14    |
|    explained_variance | 0.664    |
|    learning_rate      | 0.0007   |
|    n_updates          | 163      |
|    policy_loss        | -5.37    |
|    value_loss         | 8.92     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 21504    |
| train/                |          |
|    entropy_loss       | -3.14    |
|    explained_variance | 0.549    |
|    learning_rate      | 0.0007   |
|    n_updates          | 167      |
|    policy_loss        | 3.17     |
|    value_loss         | 5.33     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 22016    |
| train/                |          |
|    entropy_loss       | -3.27    |
|    explained_variance | 0.496    |
|    learning_rate      | 0.0007   |
|    n_updates          | 171      |
|    policy_loss        | -1.48    |
|    value_loss         | 11.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    entropy_loss       | -3.87    |
|    explained_variance | 0.433    |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | -6.21    |
|    value_loss         | 15       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23040    |
| train/                |          |
|    entropy_loss       | -3.46    |
|    explained_variance | 0.478    |
|    learning_rate      | 0.0007   |
|    n_updates          | 179      |
|    policy_loss        | 1.82     |
|    value_loss         | 8.31     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23552    |
| train/                |          |
|    entropy_loss       | -3.42    |
|    explained_variance | 0.47     |
|    learning_rate      | 0.0007   |
|    n_updates          | 183      |
|    policy_loss        | -1.47    |
|    value_loss         | 14.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 24064    |
| train/                |          |
|    entropy_loss       | -3.53    |
|    explained_variance | 0.518    |
|    learning_rate      | 0.0007   |
|    n_updates          | 187      |
|    policy_loss        | 2.91     |
|    value_loss         | 5.16     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 24576    |
| train/                |          |
|    entropy_loss       | -3.27    |
|    explained_variance | 0.507    |
|    learning_rate      | 0.0007   |
|    n_updates          | 191      |
|    policy_loss        | -1.49    |
|    value_loss         | 5.58     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    entropy_loss       | -3.26    |
|    explained_variance | 0.501    |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | -0.652   |
|    value_loss         | 10.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -3.12    |
|    explained_variance | 0.693    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | 1.45     |
|    value_loss         | 3.5      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 11.2     |
|    ep_rew_mean     | -9.46    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 75510    |
|    total_timesteps | 25600    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 26112    |
| train/                |          |
|    entropy_loss       | -3.36    |
|    explained_variance | 0.586    |
|    learning_rate      | 0.0007   |
|    n_updates          | 203      |
|    policy_loss        | -2.72    |
|    value_loss         | 5.26     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 26624    |
| train/                |          |
|    entropy_loss       | -3.38    |
|    explained_variance | 0.862    |
|    learning_rate      | 0.0007   |
|    n_updates          | 207      |
|    policy_loss        | 5.55     |
|    value_loss         | 4.58     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 27136    |
| train/                |          |
|    entropy_loss       | -3.26    |
|    explained_variance | 0.794    |
|    learning_rate      | 0.0007   |
|    n_updates          | 211      |
|    policy_loss        | 1.9      |
|    value_loss         | 1.94     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 27648    |
| train/                |          |
|    entropy_loss       | -3.45    |
|    explained_variance | 0.549    |
|    learning_rate      | 0.0007   |
|    n_updates          | 215      |
|    policy_loss        | 0.27     |
|    value_loss         | 5.29     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    entropy_loss       | -3.44    |
|    explained_variance | 0.755    |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | 1.2      |
|    value_loss         | 2.28     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 28672    |
| train/                |          |
|    entropy_loss       | -3.35    |
|    explained_variance | 0.64     |
|    learning_rate      | 0.0007   |
|    n_updates          | 223      |
|    policy_loss        | 0.988    |
|    value_loss         | 6.79     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 29184    |
| train/                |          |
|    entropy_loss       | -3.71    |
|    explained_variance | 0.62     |
|    learning_rate      | 0.0007   |
|    n_updates          | 227      |
|    policy_loss        | 1.44     |
|    value_loss         | 5.44     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    entropy_loss       | -4.23    |
|    explained_variance | 0.635    |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | -3.27    |
|    value_loss         | 4.63     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 30208    |
| train/                |          |
|    entropy_loss       | -4.29    |
|    explained_variance | 0.447    |
|    learning_rate      | 0.0007   |
|    n_updates          | 235      |
|    policy_loss        | -5.52    |
|    value_loss         | 11.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 30720    |
| train/                |          |
|    entropy_loss       | -4.01    |
|    explained_variance | 0.562    |
|    learning_rate      | 0.0007   |
|    n_updates          | 239      |
|    policy_loss        | -3.03    |
|    value_loss         | 8.3      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 31232    |
| train/                |          |
|    entropy_loss       | -4.11    |
|    explained_variance | 0.659    |
|    learning_rate      | 0.0007   |
|    n_updates          | 243      |
|    policy_loss        | 0.769    |
|    value_loss         | 4.69     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 31744    |
| train/                |          |
|    entropy_loss       | -4.56    |
|    explained_variance | 0.547    |
|    learning_rate      | 0.0007   |
|    n_updates          | 247      |
|    policy_loss        | -1.03    |
|    value_loss         | 5.38     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 32256    |
| train/                |          |
|    entropy_loss       | -4.57    |
|    explained_variance | 0.631    |
|    learning_rate      | 0.0007   |
|    n_updates          | 251      |
|    policy_loss        | 0.348    |
|    value_loss         | 4.07     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 32768    |
| train/                |          |
|    entropy_loss       | -4.07    |
|    explained_variance | 0.783    |
|    learning_rate      | 0.0007   |
|    n_updates          | 255      |
|    policy_loss        | 4.77     |
|    value_loss         | 3.06     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 33280    |
| train/                |          |
|    entropy_loss       | -4.18    |
|    explained_variance | 0.794    |
|    learning_rate      | 0.0007   |
|    n_updates          | 259      |
|    policy_loss        | -1.05    |
|    value_loss         | 1.73     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 33792    |
| train/                |          |
|    entropy_loss       | -4.41    |
|    explained_variance | 0.569    |
|    learning_rate      | 0.0007   |
|    n_updates          | 263      |
|    policy_loss        | -3.35    |
|    value_loss         | 3.23     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 34304    |
| train/                |          |
|    entropy_loss       | -4.03    |
|    explained_variance | 0.713    |
|    learning_rate      | 0.0007   |
|    n_updates          | 267      |
|    policy_loss        | -1.06    |
|    value_loss         | 2.63     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 34816    |
| train/                |          |
|    entropy_loss       | -4.64    |
|    explained_variance | 0.341    |
|    learning_rate      | 0.0007   |
|    n_updates          | 271      |
|    policy_loss        | -4.89    |
|    value_loss         | 7.93     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 35328    |
| train/                |          |
|    entropy_loss       | -4.61    |
|    explained_variance | 0.561    |
|    learning_rate      | 0.0007   |
|    n_updates          | 275      |
|    policy_loss        | -2.52    |
|    value_loss         | 3.17     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 35840    |
| train/                |          |
|    entropy_loss       | -4.37    |
|    explained_variance | 0.492    |
|    learning_rate      | 0.0007   |
|    n_updates          | 279      |
|    policy_loss        | -2.37    |
|    value_loss         | 6.78     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 36352    |
| train/                |          |
|    entropy_loss       | -4.42    |
|    explained_variance | 0.594    |
|    learning_rate      | 0.0007   |
|    n_updates          | 283      |
|    policy_loss        | -0.866   |
|    value_loss         | 3.09     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 36864    |
| train/                |          |
|    entropy_loss       | -4.74    |
|    explained_variance | 0.442    |
|    learning_rate      | 0.0007   |
|    n_updates          | 287      |
|    policy_loss        | -4.04    |
|    value_loss         | 4.92     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 37376    |
| train/                |          |
|    entropy_loss       | -4.41    |
|    explained_variance | 0.599    |
|    learning_rate      | 0.0007   |
|    n_updates          | 291      |
|    policy_loss        | -3.97    |
|    value_loss         | 5.23     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 37888    |
| train/                |          |
|    entropy_loss       | -4.39    |
|    explained_variance | 0.758    |
|    learning_rate      | 0.0007   |
|    n_updates          | 295      |
|    policy_loss        | 2.34     |
|    value_loss         | 2.44     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 360      |
|    mean_reward        | -34.5    |
| time/                 |          |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -4.42    |
|    explained_variance | 0.639    |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | 1.2      |
|    value_loss         | 3.57     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.63     |
|    ep_rew_mean     | -7.62    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 300      |
|    time_elapsed    | 107690   |
|    total_timesteps | 38400    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 38912    |
| train/                |          |
|    entropy_loss       | -4.56    |
|    explained_variance | 0.319    |
|    learning_rate      | 0.0007   |
|    n_updates          | 303      |
|    policy_loss        | -7.25    |
|    value_loss         | 16       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 39424    |
| train/                |          |
|    entropy_loss       | -4.21    |
|    explained_variance | 0.444    |
|    learning_rate      | 0.0007   |
|    n_updates          | 307      |
|    policy_loss        | -5.49    |
|    value_loss         | 7.29     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 39936    |
| train/                |          |
|    entropy_loss       | -4.99    |
|    explained_variance | -0.113   |
|    learning_rate      | 0.0007   |
|    n_updates          | 311      |
|    policy_loss        | -18      |
|    value_loss         | 36.8     |
------------------------------------
