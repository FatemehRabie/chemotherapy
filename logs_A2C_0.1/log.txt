Logging to ./logs_A2C_0.1/
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 40        |
|    mean_reward        | -584      |
| time/                 |           |
|    total_timesteps    | 512       |
| train/                |           |
|    entropy_loss       | -10.3     |
|    explained_variance | -9.54e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 3         |
|    policy_loss        | -666      |
|    value_loss         | 6.19e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -406     |
| time/                 |          |
|    total_timesteps    | 1024     |
| train/                |          |
|    entropy_loss       | -10.3    |
|    explained_variance | 5.96e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 7        |
|    policy_loss        | -875     |
|    value_loss         | 1.1e+04  |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 396      |
|    mean_reward        | -213     |
| time/                 |          |
|    total_timesteps    | 1536     |
| train/                |          |
|    entropy_loss       | -10.1    |
|    explained_variance | 2.98e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 11       |
|    policy_loss        | -821     |
|    value_loss         | 1e+04    |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 2048      |
| train/                |           |
|    entropy_loss       | -9.98     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 15        |
|    policy_loss        | -455      |
|    value_loss         | 3.36e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -478     |
| time/                 |          |
|    total_timesteps    | 2560     |
| train/                |          |
|    entropy_loss       | -9.91    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 19       |
|    policy_loss        | -789     |
|    value_loss         | 1.03e+04 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -182      |
| time/                 |           |
|    total_timesteps    | 3072      |
| train/                |           |
|    entropy_loss       | -9.73     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 23        |
|    policy_loss        | -653      |
|    value_loss         | 6.54e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 3584     |
| train/                |          |
|    entropy_loss       | -9.65    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 27       |
|    policy_loss        | -378     |
|    value_loss         | 2.75e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 4096      |
| train/                |           |
|    entropy_loss       | -9.58     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 31        |
|    policy_loss        | -463      |
|    value_loss         | 4.57e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 4608      |
| train/                |           |
|    entropy_loss       | -9.43     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 35        |
|    policy_loss        | -329      |
|    value_loss         | 2.19e+03  |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 5120      |
| train/                |           |
|    entropy_loss       | -9.3      |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 39        |
|    policy_loss        | -231      |
|    value_loss         | 1.25e+03  |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 5632     |
| train/                |          |
|    entropy_loss       | -8.93    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 43       |
|    policy_loss        | -249     |
|    value_loss         | 1.79e+03 |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 6144      |
| train/                |           |
|    entropy_loss       | -8.42     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 47        |
|    policy_loss        | -108      |
|    value_loss         | 459       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 400       |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 6656      |
| train/                |           |
|    entropy_loss       | -7.86     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 51        |
|    policy_loss        | -76.3     |
|    value_loss         | 348       |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7168      |
| train/                |           |
|    entropy_loss       | -6.73     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 55        |
|    policy_loss        | 11.4      |
|    value_loss         | 70.5      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 7680      |
| train/                |           |
|    entropy_loss       | -6.01     |
|    explained_variance | -3.58e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 59        |
|    policy_loss        | 17.6      |
|    value_loss         | 76.6      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8192      |
| train/                |           |
|    entropy_loss       | -4.98     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 63        |
|    policy_loss        | -2.37     |
|    value_loss         | 91.3      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 8704      |
| train/                |           |
|    entropy_loss       | -4.07     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 67        |
|    policy_loss        | 3.04      |
|    value_loss         | 99.3      |
-------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 9216      |
| train/                |           |
|    entropy_loss       | -3.87     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 71        |
|    policy_loss        | 15.1      |
|    value_loss         | 72.3      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 9728     |
| train/                |          |
|    entropy_loss       | -3.75    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 75       |
|    policy_loss        | 4.65     |
|    value_loss         | 112      |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 10240     |
| train/                |           |
|    entropy_loss       | -4.02     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 79        |
|    policy_loss        | 12        |
|    value_loss         | 64.8      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 10752    |
| train/                |          |
|    entropy_loss       | -4.09    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 83       |
|    policy_loss        | 6.69     |
|    value_loss         | 80.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11264    |
| train/                |          |
|    entropy_loss       | -4.09    |
|    explained_variance | 2.38e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 87       |
|    policy_loss        | 8.33     |
|    value_loss         | 69.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 11776    |
| train/                |          |
|    entropy_loss       | -3.94    |
|    explained_variance | 4.77e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 91       |
|    policy_loss        | 1.47     |
|    value_loss         | 107      |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12288    |
| train/                |          |
|    entropy_loss       | -3.99    |
|    explained_variance | 1.01e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 95       |
|    policy_loss        | 5.53     |
|    value_loss         | 65.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -3.78    |
|    explained_variance | 2.74e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 99       |
|    policy_loss        | 5.46     |
|    value_loss         | 45       |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.16     |
|    ep_rew_mean     | -12.5    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 100      |
|    time_elapsed    | 42907    |
|    total_timesteps | 12800    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13312    |
| train/                |          |
|    entropy_loss       | -3.76    |
|    explained_variance | 1.02e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 103      |
|    policy_loss        | 7.31     |
|    value_loss         | 36.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 13824    |
| train/                |          |
|    entropy_loss       | -3.81    |
|    explained_variance | 3.32e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 107      |
|    policy_loss        | -5.44    |
|    value_loss         | 82.1     |
------------------------------------
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 80        |
|    mean_reward        | -181      |
| time/                 |           |
|    total_timesteps    | 14336     |
| train/                |           |
|    entropy_loss       | -3.81     |
|    explained_variance | -2.38e-07 |
|    learning_rate      | 0.0007    |
|    n_updates          | 111       |
|    policy_loss        | 6.99      |
|    value_loss         | 36.3      |
-------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 14848    |
| train/                |          |
|    entropy_loss       | -3.65    |
|    explained_variance | 0        |
|    learning_rate      | 0.0007   |
|    n_updates          | 115      |
|    policy_loss        | 3.07     |
|    value_loss         | 31.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 15360    |
| train/                |          |
|    entropy_loss       | -3.72    |
|    explained_variance | 8.94e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 119      |
|    policy_loss        | -3.87    |
|    value_loss         | 69.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 15872    |
| train/                |          |
|    entropy_loss       | -3.69    |
|    explained_variance | 1.61e-06 |
|    learning_rate      | 0.0007   |
|    n_updates          | 123      |
|    policy_loss        | 1.48     |
|    value_loss         | 37.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 16384    |
| train/                |          |
|    entropy_loss       | -3.51    |
|    explained_variance | 1.13e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 127      |
|    policy_loss        | 0.584    |
|    value_loss         | 45.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 80       |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 16896    |
| train/                |          |
|    entropy_loss       | -3.58    |
|    explained_variance | 7.15e-07 |
|    learning_rate      | 0.0007   |
|    n_updates          | 131      |
|    policy_loss        | 3.97     |
|    value_loss         | 23.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17408    |
| train/                |          |
|    entropy_loss       | -3.54    |
|    explained_variance | 1.06e-05 |
|    learning_rate      | 0.0007   |
|    n_updates          | 135      |
|    policy_loss        | 2.81     |
|    value_loss         | 29.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 100      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 17920    |
| train/                |          |
|    entropy_loss       | -3.66    |
|    explained_variance | 0.000705 |
|    learning_rate      | 0.0007   |
|    n_updates          | 139      |
|    policy_loss        | -2.25    |
|    value_loss         | 33.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18432    |
| train/                |          |
|    entropy_loss       | -3.7     |
|    explained_variance | 0.0344   |
|    learning_rate      | 0.0007   |
|    n_updates          | 143      |
|    policy_loss        | -4.93    |
|    value_loss         | 38.7     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 18944    |
| train/                |          |
|    entropy_loss       | -3.71    |
|    explained_variance | 0.061    |
|    learning_rate      | 0.0007   |
|    n_updates          | 147      |
|    policy_loss        | -5.8     |
|    value_loss         | 37       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 19456    |
| train/                |          |
|    entropy_loss       | -3.73    |
|    explained_variance | 0.122    |
|    learning_rate      | 0.0007   |
|    n_updates          | 151      |
|    policy_loss        | -7.15    |
|    value_loss         | 28.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 19968    |
| train/                |          |
|    entropy_loss       | -3.72    |
|    explained_variance | 0.147    |
|    learning_rate      | 0.0007   |
|    n_updates          | 155      |
|    policy_loss        | -0.547   |
|    value_loss         | 26.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 20480    |
| train/                |          |
|    entropy_loss       | -3.66    |
|    explained_variance | 0.0929   |
|    learning_rate      | 0.0007   |
|    n_updates          | 159      |
|    policy_loss        | -7.54    |
|    value_loss         | 68.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 20992    |
| train/                |          |
|    entropy_loss       | -3.63    |
|    explained_variance | 0.21     |
|    learning_rate      | 0.0007   |
|    n_updates          | 163      |
|    policy_loss        | 2.6      |
|    value_loss         | 16.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 21504    |
| train/                |          |
|    entropy_loss       | -3.54    |
|    explained_variance | 0.24     |
|    learning_rate      | 0.0007   |
|    n_updates          | 167      |
|    policy_loss        | 3.87     |
|    value_loss         | 14.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 22016    |
| train/                |          |
|    entropy_loss       | -3.53    |
|    explained_variance | 0.213    |
|    learning_rate      | 0.0007   |
|    n_updates          | 171      |
|    policy_loss        | -2.51    |
|    value_loss         | 25.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -182     |
| time/                 |          |
|    total_timesteps    | 22528    |
| train/                |          |
|    entropy_loss       | -3.52    |
|    explained_variance | 0.238    |
|    learning_rate      | 0.0007   |
|    n_updates          | 175      |
|    policy_loss        | 3.26     |
|    value_loss         | 14.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 23040    |
| train/                |          |
|    entropy_loss       | -3.65    |
|    explained_variance | 0.119    |
|    learning_rate      | 0.0007   |
|    n_updates          | 179      |
|    policy_loss        | -8.05    |
|    value_loss         | 82.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 23552    |
| train/                |          |
|    entropy_loss       | -3.59    |
|    explained_variance | 0.328    |
|    learning_rate      | 0.0007   |
|    n_updates          | 183      |
|    policy_loss        | 2.17     |
|    value_loss         | 11.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 24064    |
| train/                |          |
|    entropy_loss       | -3.65    |
|    explained_variance | 0.199    |
|    learning_rate      | 0.0007   |
|    n_updates          | 187      |
|    policy_loss        | -1.86    |
|    value_loss         | 18.5     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -180     |
| time/                 |          |
|    total_timesteps    | 24576    |
| train/                |          |
|    entropy_loss       | -3.59    |
|    explained_variance | 0.298    |
|    learning_rate      | 0.0007   |
|    n_updates          | 191      |
|    policy_loss        | -1.09    |
|    value_loss         | 15.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 25088    |
| train/                |          |
|    entropy_loss       | -3.57    |
|    explained_variance | 0.236    |
|    learning_rate      | 0.0007   |
|    n_updates          | 195      |
|    policy_loss        | -5.94    |
|    value_loss         | 19       |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -3.62    |
|    explained_variance | 0.304    |
|    learning_rate      | 0.0007   |
|    n_updates          | 199      |
|    policy_loss        | -6.74    |
|    value_loss         | 16.8     |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.26     |
|    ep_rew_mean     | -9.79    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 200      |
|    time_elapsed    | 79314    |
|    total_timesteps | 25600    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 26112    |
| train/                |          |
|    entropy_loss       | -3.7     |
|    explained_variance | 0.306    |
|    learning_rate      | 0.0007   |
|    n_updates          | 203      |
|    policy_loss        | -5       |
|    value_loss         | 23.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 26624    |
| train/                |          |
|    entropy_loss       | -3.68    |
|    explained_variance | 0.422    |
|    learning_rate      | 0.0007   |
|    n_updates          | 207      |
|    policy_loss        | -0.598   |
|    value_loss         | 8.57     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 27136    |
| train/                |          |
|    entropy_loss       | -3.62    |
|    explained_variance | 0.332    |
|    learning_rate      | 0.0007   |
|    n_updates          | 211      |
|    policy_loss        | 0.96     |
|    value_loss         | 17.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 27648    |
| train/                |          |
|    entropy_loss       | -3.73    |
|    explained_variance | 0.348    |
|    learning_rate      | 0.0007   |
|    n_updates          | 215      |
|    policy_loss        | -4.1     |
|    value_loss         | 13.3     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 28160    |
| train/                |          |
|    entropy_loss       | -4.15    |
|    explained_variance | 0.407    |
|    learning_rate      | 0.0007   |
|    n_updates          | 219      |
|    policy_loss        | 1.29     |
|    value_loss         | 12.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 28672    |
| train/                |          |
|    entropy_loss       | -3.75    |
|    explained_variance | 0.32     |
|    learning_rate      | 0.0007   |
|    n_updates          | 223      |
|    policy_loss        | -6.08    |
|    value_loss         | 19.2     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 29184    |
| train/                |          |
|    entropy_loss       | -3.74    |
|    explained_variance | 0.384    |
|    learning_rate      | 0.0007   |
|    n_updates          | 227      |
|    policy_loss        | -3       |
|    value_loss         | 13.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 29696    |
| train/                |          |
|    entropy_loss       | -3.68    |
|    explained_variance | 0.474    |
|    learning_rate      | 0.0007   |
|    n_updates          | 231      |
|    policy_loss        | -1.18    |
|    value_loss         | 8.07     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 30208    |
| train/                |          |
|    entropy_loss       | -3.64    |
|    explained_variance | 0.289    |
|    learning_rate      | 0.0007   |
|    n_updates          | 235      |
|    policy_loss        | -7.54    |
|    value_loss         | 20.6     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 30720    |
| train/                |          |
|    entropy_loss       | -3.51    |
|    explained_variance | 0.336    |
|    learning_rate      | 0.0007   |
|    n_updates          | 239      |
|    policy_loss        | -6.37    |
|    value_loss         | 15.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 31232    |
| train/                |          |
|    entropy_loss       | -3.47    |
|    explained_variance | 0.33     |
|    learning_rate      | 0.0007   |
|    n_updates          | 243      |
|    policy_loss        | -5.06    |
|    value_loss         | 14.9     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 323      |
|    mean_reward        | -25.3    |
| time/                 |          |
|    total_timesteps    | 31744    |
| train/                |          |
|    entropy_loss       | -3.77    |
|    explained_variance | 0.387    |
|    learning_rate      | 0.0007   |
|    n_updates          | 247      |
|    policy_loss        | -4.1     |
|    value_loss         | 14.1     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -88      |
| time/                 |          |
|    total_timesteps    | 32256    |
| train/                |          |
|    entropy_loss       | -4.09    |
|    explained_variance | 0.522    |
|    learning_rate      | 0.0007   |
|    n_updates          | 251      |
|    policy_loss        | -3.37    |
|    value_loss         | 8.16     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -113     |
| time/                 |          |
|    total_timesteps    | 32768    |
| train/                |          |
|    entropy_loss       | -4.37    |
|    explained_variance | 0.417    |
|    learning_rate      | 0.0007   |
|    n_updates          | 255      |
|    policy_loss        | -10      |
|    value_loss         | 17.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 33280    |
| train/                |          |
|    entropy_loss       | -3.7     |
|    explained_variance | 0.366    |
|    learning_rate      | 0.0007   |
|    n_updates          | 259      |
|    policy_loss        | -3.86    |
|    value_loss         | 8.55     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 33792    |
| train/                |          |
|    entropy_loss       | -3.46    |
|    explained_variance | 0.36     |
|    learning_rate      | 0.0007   |
|    n_updates          | 263      |
|    policy_loss        | -7.96    |
|    value_loss         | 16.4     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 200      |
|    mean_reward        | -62.8    |
| time/                 |          |
|    total_timesteps    | 34304    |
| train/                |          |
|    entropy_loss       | -3.45    |
|    explained_variance | 0.57     |
|    learning_rate      | 0.0007   |
|    n_updates          | 267      |
|    policy_loss        | -1.85    |
|    value_loss         | 4.95     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -100     |
| time/                 |          |
|    total_timesteps    | 34816    |
| train/                |          |
|    entropy_loss       | -3.46    |
|    explained_variance | 0.288    |
|    learning_rate      | 0.0007   |
|    n_updates          | 271      |
|    policy_loss        | -1.54    |
|    value_loss         | 7.56     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -44.2    |
| time/                 |          |
|    total_timesteps    | 35328    |
| train/                |          |
|    entropy_loss       | -3.62    |
|    explained_variance | 0.489    |
|    learning_rate      | 0.0007   |
|    n_updates          | 275      |
|    policy_loss        | -2.46    |
|    value_loss         | 6.01     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -162     |
| time/                 |          |
|    total_timesteps    | 35840    |
| train/                |          |
|    entropy_loss       | -3.39    |
|    explained_variance | 0.42     |
|    learning_rate      | 0.0007   |
|    n_updates          | 279      |
|    policy_loss        | -1.8     |
|    value_loss         | 12.8     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -132     |
| time/                 |          |
|    total_timesteps    | 36352    |
| train/                |          |
|    entropy_loss       | -3.52    |
|    explained_variance | 0.623    |
|    learning_rate      | 0.0007   |
|    n_updates          | 283      |
|    policy_loss        | -0.0803  |
|    value_loss         | 3.51     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -129     |
| time/                 |          |
|    total_timesteps    | 36864    |
| train/                |          |
|    entropy_loss       | -3.53    |
|    explained_variance | 0.425    |
|    learning_rate      | 0.0007   |
|    n_updates          | 287      |
|    policy_loss        | -3.35    |
|    value_loss         | 7.34     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -181     |
| time/                 |          |
|    total_timesteps    | 37376    |
| train/                |          |
|    entropy_loss       | -3.24    |
|    explained_variance | 0.516    |
|    learning_rate      | 0.0007   |
|    n_updates          | 291      |
|    policy_loss        | -1.75    |
|    value_loss         | 5.25     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -67.9    |
| time/                 |          |
|    total_timesteps    | 37888    |
| train/                |          |
|    entropy_loss       | -3.28    |
|    explained_variance | 0.554    |
|    learning_rate      | 0.0007   |
|    n_updates          | 295      |
|    policy_loss        | 1.69     |
|    value_loss         | 4.47     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -61.6    |
| time/                 |          |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -3.54    |
|    explained_variance | 0.572    |
|    learning_rate      | 0.0007   |
|    n_updates          | 299      |
|    policy_loss        | -0.9     |
|    value_loss         | 4.9      |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.3      |
|    ep_rew_mean     | -8.33    |
| time/              |          |
|    fps             | 0        |
|    iterations      | 300      |
|    time_elapsed    | 111939   |
|    total_timesteps | 38400    |
---------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 169      |
|    mean_reward        | -9.48    |
| time/                 |          |
|    total_timesteps    | 38912    |
| train/                |          |
|    entropy_loss       | -3.39    |
|    explained_variance | 0.541    |
|    learning_rate      | 0.0007   |
|    n_updates          | 303      |
|    policy_loss        | -1.88    |
|    value_loss         | 7.13     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 323      |
|    mean_reward        | -21.3    |
| time/                 |          |
|    total_timesteps    | 39424    |
| train/                |          |
|    entropy_loss       | -3.29    |
|    explained_variance | 0.497    |
|    learning_rate      | 0.0007   |
|    n_updates          | 307      |
|    policy_loss        | -5.45    |
|    value_loss         | 6.14     |
------------------------------------
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 400      |
|    mean_reward        | -51.8    |
| time/                 |          |
|    total_timesteps    | 39936    |
| train/                |          |
|    entropy_loss       | -3.21    |
|    explained_variance | 0.449    |
|    learning_rate      | 0.0007   |
|    n_updates          | 311      |
|    policy_loss        | -4.7     |
|    value_loss         | 9.84     |
------------------------------------
